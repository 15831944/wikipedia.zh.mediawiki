'''信息瓶颈'''（{{lang-en|information bottleneck}}）是[[信息论|信息论]]中的一种方法，由{{le|纳夫塔利·泰斯比|Naftali Tishby}}、费尔南多·佩雷拉（Fernando C. Pereira）与{{le|威廉·比亚莱克|William Bialek}}于1999年提出。<ref>{{cite conference |url=http://www.cs.huji.ac.il/labs/learning/Papers/allerton.pdf|title=The Information Bottleneck Method|conference=The 37th annual Allerton Conference on Communication, Control, and Computing|last1=Tishby|first1=Naftali|last2=Pereira|first2=Fernando C.|last3=Bialek|first3=William|date=September 1999|pages=368–377}}</ref>对于一[[随机变量|随机变量]]<math>X</math>，假设已知其与观察变量<math>Y</math>之间的[[联合概率分布|联合概率分布]]<math>p(X,Y)</math>。此时，当需要概括（[[聚类|聚类]]）<math>X</math>时，可以通过信息瓶颈方法来分析如何最优化地平衡[[准确度|准确度]]与复杂度（[[数据压缩|数据压缩]]）。该方法的应用还包括分布聚类（distributional clustering）与[[降维|降维]]等。此外，信息瓶颈也被用于分析[[深度学习|深度学习]]的过程。<ref>{{cite arXiv|author=Naftali Tishby, Noga Zaslavsky|year=2015|title=Deep Learning and the Information Bottleneck Principle|eprint=1503.02406}}</ref>

信息瓶项方法中运用了[[互信息|互信息]]的概念。假设压缩后的随机变量为<math>T</math>，我们试图用<math>T</math>代替<math>X</math>来预测<math>Y</math>。此时，可使用以下算法得到最优的<math>T</math>：
: <math> \min_{p(t|x)} \,\, I(X;T) - \beta I(T;Y),</math>

其中<math>I(X;T)</math>与<math>I(T;Y)</math>分别为<math>X</math>与<math>T</math>之间、以及<math>T</math>与<math>Y</math>之间的互信息，可由<math>p(X,Y)</math>计算得到。<math>\beta</math>则表示[[拉格朗日乘数|拉格朗日乘数]]。

== 参考文献 ==
{{reflist}}

[[Category:聚类分析|Category:聚类分析]]
[[Category:多變量統計|Category:多變量統計]]
[[Category:信息论|Category:信息论]]