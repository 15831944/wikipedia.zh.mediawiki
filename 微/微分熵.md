'''微分熵'''是[[消息理論|消息理論]]中的一個概念，是從以[[随机变量#离散型随机变量|離散隨機變數]]所計算出的夏農熵推廣，以[[连续随机变量|連續型隨機變數]]計算所得之[[熵_(信息论)|熵]]，微分熵與[[随机变量#离散型随机变量|離散隨機變數]]所計算出之夏農熵，皆可代表描述一信息所需碼長的下界，然而，微分熵與夏農熵仍存在著某些相異的性質。

== 定義 ==

令<math>X</math>為一[[连续随机变量|連續型隨機變數]]，其[[機率密度函數|機率密度函數]]為<math>f_X(x)</math>，其中<math>X</math>的[[支撑集|支撐集]]為<math>S=\{x\in X|f_X(x)>0}\</math>。微分熵<math>h_X(x)</math>:

<math>h_X(x)=-\int_{S} f_X(x)log(f_X(x))dx</math>。

與夏農熵為類比，計算夏農熵之算式中的<math>\log</math>通常以2為底，而微分熵為計算方便，常以<math>ln</math>計算後再轉換為<math>log_2</math>的結果。微分熵與夏農熵最大的不同點在於<math>f_X(x)</math>可為大於1的數值，此時可能會造成<math>h_X(x)</math>為負值，而夏農熵<math>H_X(x)</math>恆不為負。

例如，<math>X</math>為[[連續型均勻分布|均勻分布]]<math>U(0,a),a<1</math>：

<math>f_X(x)=</math><math>1\over a</math><math>; h_X(x)=-\int\limits_{0}^{a}</math><math>1\over a</math><math>ln</math><math>1\over a</math><math>dx</math>

<math>h_X(x)=ln(a)</math><math><0</math>

== 相關計算 ==

=== [[条件熵|條件熵]] ===
<math>f(x,y)</math>為<math>X,Y</math>之聯合機率密度函數，其條件熵為:

<math>h(X|Y)=-\int f(x,y)log{f(x|y)}dxdy</math>。

=== [[相对熵|相對熵]] ===
又稱'''KL散度'''（'''Kullback–Leibler divergence)，'''兩機率密度函數f、g的相對熵定義為:

<math>D(f||g)=\int flog{f\over g}</math>。

=== [[互信息|互信息]] ===
兩連續型隨機變數的聯合機率密度函數為<math>f(x,y)</math>，其[[互信息|互信息]]:

<math>I(X;Y)=D(f(x,y)||f(x)f(y))</math>

廣義而言，我們可以將互信息定義在有限多個連續隨機變數值域的劃分。
可參考[[互信息#連續互信息的量化|連續互信息的量化]]。

== 性質 ==

=== [[相对熵#特性|相對熵]]恆正 ===
與夏農相對熵性質相同，恆正。

<math>-{\displaystyle D(f||g)=\int flog{g \over f}} </math>

<math>\leq log\int f{g \over f}  </math>  ('''[[延森不等式|延森不等式]]''')

<math>\leq 0   </math>。

=== 鏈式法則 ===
一次觀測所有隨機變數所測得的的[[联合熵|聯合熵]]，與個別接收隨機變數後計算的[[条件熵|條件熵]]總和相同，即觀測順序與間隔不影響微分熵。

<math>h(X_1,X_2,...,X_n)=\sum_{k=1}^nh(X_i|X_1,X_2,...,X_{i-1})  </math>。

=== 平移 ===
隨機變數的平移不影響微分熵，因為固定的平移不會增加隨機變數的方差。

<math>h(X+c)=h(X)</math>

=== 縮放 ===
將隨機變數縮放會增加其[[方差|方差]]，微分熵亦會隨之增加。

<math>h(AX)=h(X)+log|det(A)|</math>

=== 上界 ===
期望值為0，方差為<math>\sigma ^2</math>且值域為<math>R</math>之隨機變數<math>X</math>的微分熵，其上界為常態分佈<math>N(0,\sigma ^2)</math>的微分熵。

<math>h(X)\leq{1\over2}log(2\pi e\sigma^2)</math>

=== 估計誤差 ===
隨機變數<math>X</math>與其估計子<math>\widehat{X}</math>之均方誤差存在下界，當<math>X</math>為常態分佈且<math>\widehat{X}</math>為[[估计量的偏差|無偏]]估計子時，等號成立。

<math>E[(X-\widehat{X})^2]\geq {1\over{2\pi e}}e^{2h(X)} </math>

== 漸進等分性 ==

=== [[漸進等分性|漸進等分性]]===
離散隨機變數的夏農熵中，[[独立同分布|獨立同分布]]的隨機變數序列，在漸進等分性(Asymptotic equipartition property)之下其[[機率質量函數|機率質量函數]]<math>p(X_1,X_2,...,X_n)
</math>趨近於<math>2^{-nH(X)}</math>。

連續型隨機變數之漸進等分性：

<math>-{1\over n}log(f(X_1,X_2,...,X_n))\rightarrow h(X)</math>

=== 典型集 ===
典型集(Typical set)定義如下

<math>A_\epsilon^{(n)}=\{(x_1,x_2,...,x_n)\in S^n:|-{1\over n}logf(x_1,x_2,...,x_n)-h(X)|\leq\epsilon}\</math>,<math>\epsilon >0
</math>

=== 體積 ===
集合包含於<math>R^n </math>,<math>A\subset R^n </math>，其體積(Volume)<math>Vol(A)</math>定義如下:

<math>Vol(A)=\int\limits_{A} dx_1dx_2...dx_n</math>。

典型集<math>A_\epsilon^{(n)}</math>的體積有以下性質:

1.<math>Vol(A_\epsilon^{(n)})\leq2^{n(h(X)+\epsilon)}</math>

2.<math>Vol(A_\epsilon^{(n)})\geq(1-\epsilon)2^{n(h(X)-\epsilon)}</math>

'''證明'''

1.

由<math>-{1\over n}log(f(X_1,X_2,...,X_n))\rightarrow h(X)</math>，

可得：

<math>1=\int_{S^n} f(x_1,x_2,...,x_n)dx_1dx_2...dx_n
</math>

<math>\geq \int_{A_\epsilon^{(n)}} f(x_1,x_2,...,x_n)dx_1dx_2...dx_n</math>

<math>\geq \int_{A_\epsilon^{(n)}} 2^{-n(h(X)+\epsilon)}dx_1dx_2...dx_n</math>

<math>=2^{-n(h(X)+\epsilon)}\int_{A_\epsilon^{(n)}} dx_1dx_2...dx_n</math>

<math>=2^{-n(h(X)+\epsilon )}Vol(A_\epsilon^{(n)})
</math>

2.

當n足夠大時，<math>Pr(A_\epsilon^{(n)})>1-\epsilon</math>，

因此：

<math>1-\epsilon \leq \int_{A_\epsilon^{(n)}} f(x_1,x_2,...,x_n)dx_1dx_2...dx_n</math>

<math>\leq \int_{A_\epsilon^{(n)}} 2^{-n(h(X)-\epsilon)}dx_1dx_2...dx_n </math>

<math>=2^{-n(h(X)-\epsilon)} \int_{A_\epsilon^{(n)}}dx_1dx_2...dx_n</math>

<math>=2^{-n(h(X)-\epsilon)} Vol(A_\epsilon^{(n)})</math>

== 量化 ==
我們可以將[[機率密度函數|機率密度函數]][[量化_(信号处理)|量化]]後，以夏農熵來計算微分熵。首先將連續隨機變數X以<math>\Delta</math>分為數個區間，根據[[中值定理|均值定理]]，<math>x_i</math>滿足：

<math>f(x_i)\Delta=\int_{i\Delta}^{(i+1)\Delta}f(x)dx=p_i
</math>

量化後的隨機變數<math>X^{\Delta}</math>:

<math>X^{\Delta}=x_i, i \Delta \leq X <(i+1)\Delta </math>

夏農熵為:

<math>H(X^{\Delta})=-\sum_{-\infin}^{\infin}f(x_i)\Delta log(f(x_i))-log\Delta</math>

意即，當<math>\Delta\rightarrow0</math>，<math>h(f)=h(X)</math>。

=== 例子： ===
1.

對X做n位元量化<math>X\sim U(0,{1\over8})</math>。

<math>H(X^{\Delta})=-3+n</math>

上式表示，若我們想得到n位元精確度，則需要n-3個位元來表示。

2.

對X做n位元量化<math>X\sim N(0,{\sigma}^2)</math>。

<math>H(X^{\Delta})={1\over2}log(2\pi e \sigma ^2)+n</math>

上式表示，若我們想得到n位元精確度，需要<math>{1\over2}log(2\pi e \sigma ^2)+n</math>個位元來表示。

== 最大熵 ==

=== 常態分佈 ===
隨機變數<math>X</math>，<math>X_N</math>值域為<math>(-\infin,\infin)</math>，方差為<math>\sigma^2</math>，<math>X</math>為任意分佈，<math>X_N</math>為常態分佈，機率密度函數分別為<math>f(x),g(x)</math>。

則<math>h_X(X)\leq {1\over2}log(2\pi e\sigma^2)</math>

證明:

<math>\begin{align}
 0 & \leq D(f||g)\\
 &=\int f(x)log({f(x)\over{g(x)}})dx\\
 &= -h(X)-\int f(x)log(g(x))dx\\
 
 &= -h(X)+h(x)
\end{align}</math>

其中，

<math>

 \begin{align}
-\int _{-\infin}^{\infin} f(x)log(g(x))dx
 
 &= -\int _{-\infin}^{\infin} f(x)({1\over 2}log(2\pi\sigma^2)+{1\over 2}({{x-\mu}\over \sigma})^2)dx\\
 
 &= {1\over2}log(2\pi e\sigma^2)
 \end{align}
</math>

=== 指數分佈 ===
隨機變數<math>X</math>，<math>Y</math>值域為<math>(0,\infin)</math>，期望值為<math>\lambda</math>，<math>X</math>為任意分佈，<math>Y</math>為指數分佈，機率密度函數分別為<math>f(x),g(x)</math>。

則<math>h_X(X)\leq 1+log\lambda </math>。

證明:

<math>\begin{align}
 0 & \leq D(f||g)\\
 &=\int f(x)log({f(x)\over{g(x)}})dx\\
 &= -h(X)-\int f(x)log(g(x))dx\\
 
 &= -h(X)+h(Y)
\end{align}</math>

其中，

<math>

\begin{align}
-\int \limits_{0}^{\infin} f(x)log(g(x))dy
 
 &= -\int \limits_{0}^{\infin} f(x)(log\lambda +{x\over \lambda})dx\\
 &= 1+log\lambda 
 \end{align}
</math>

== 參考文獻 ==
{{refbegin}}
* Thomas M. Cover, Joy A. Thomas, ''Elements of Information Theory'', 1991 John Wiley & Sons, Inc, 1971.  ISBN 0-471-20061-1
{{refend}}

[[Category:微分代数|Category:微分代数]]