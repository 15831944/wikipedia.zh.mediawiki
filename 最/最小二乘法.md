{{noteTA
|T=zh-hans:最小二乘法;zh-hk:最小二乘法;zh-tw:最小平方法;
|1=zh-hans:最小二乘法;zh-hk:最小二乘法;zh-tw:最小平方法;
}}
{{回归侧栏}}
[[File:Linear_least_squares2.png|right]]
[[File:X33-ellips-1.svg|thumb]]
'''最小二乘法'''（又称'''-{zh-hans:最小平方法; zh-hk:最小平方法; zh-tw:最小平方法}-'''）是一种[[数学|数学]][[优化|优化]]技术。它通过最小化[[误差|误差]]的平方和寻找数据的最佳[[函数|函数]]匹配。

利用'''最小二乘法'''可以简便地求得未知的数据，并使得这些求得的数据与实际数据之间误差的平方和为最小。

“'''最小平方法'''”是對超定方程组，即其中存在比未知數更多的方程組，以[[迴歸分析|迴歸分析]]求得近似解的標準方法。在這整個解決方案中，最小平方法演算為每一方程式的結果中，將殘差平方和的總和最小化。

最重要的應用是在[[曲線擬合|曲線擬合]]上。最小平方所涵義的最佳擬合，即[[误差|殘差]]（殘差為：觀測值與模型提供的擬合值之間的差距）平方總和的最小化。當問題在自變量（''x''變量）有重大不確定性時，那麼使用簡易迴歸和最小平方法會發生問題；在這種情況下，須另外考慮變量-誤差-擬合模型所需的方法，而不是最小平方法。

最小平方問題分為兩種：線性或普通的最小平方法，和非線性的最小平方法，取決於在所有未知數中的殘差是否為線性。線性的最小平方問題發生在統計迴歸分析中；它有一個[[解析解|封閉形式]]的解決方案。非線性的問題通常經由迭代細緻化來解決；在每次迭代中，系統由線性近似，因此在這兩種情況下核心演算是相同的。

最小平方法所得出的多項式，即以擬合曲線的函數來描述自變量與預計應變量的[[方差|變異數]]關係。

當觀測值來自指數族且滿足輕度條件時，最小平方估計和[[最大似然估计|最大似然估计]]是相同的。最小平方法也能從[[矩估计|動差法]]得出。

以下討論大多是以線性函數形式來表示，但對於更廣泛的函數族，最小平方法也是有效和實用的。此外，迭代地將局部的二次近似應用於或然性（藉由[[费希尔信息|費雪信息]]），最小平方法可用於擬合[[廣義線性模型|廣義線性模型]]。

其它依據平方距離的目標加總函數作為逼近函數的主題，請參見最小平方法（函數近似）。

最小平方法通常歸功於[[卡爾·弗里德里希·高斯|高斯]]（Carl Friedrich Gauss，1795），但最小平方法是由[[阿德里安-马里·勒让德|阿德里安-马里·勒让德]]（Adrien-Marie Legendre）首先發表的。

== 示例 ==
[[Image:Linear_least_squares_example2.svg|right]]

某次实验得到了四个数据点 <math>(x, y)</math>：<math>(1, 6)</math>、<math>(2, 5)</math>、<math>(3, 7)</math>、<math>(4, 10)</math>（右图中红色的点）。我们希望找出一条和这四个点最匹配的直线 <math>y=\beta_1+\beta_2 x</math>，即找出在某种「最佳情况」下能够大致符合如下超定线性方程组的 <math>\beta_1</math> 和 <math>\beta_2</math>：
:<math>\begin{alignat}{4}
\beta_1  +  1\beta_2 &&\; = \;&& 6 & \\
\beta_1  +  2\beta_2 &&\; = \;&& 5 & \\
\beta_1  +  3\beta_2 &&\; = \;&& 7 & \\
\beta_1  +  4\beta_2 &&\; = \;&& 10 & \\
\end{alignat}</math>

最小二乘法采用的手段是尽量使得等号两边的方差最小，也就是找出这个函数的最小值：

: <math>\begin{align}
S(\beta_1, \beta_2) =
 &\left[6-(\beta_1+1\beta_2)\right]^2+\left[5-(\beta_1+2\beta_2)   \right]^2 \\
&+\left[7-(\beta_1 +  3\beta_2)\right]^2+\left[10-(\beta_1  +  4\beta_2)\right]^2.\\
\end{align}</math>

最小值可以通过对 <math>S(\beta_1, \beta_2)</math> 分别求 <math>\beta_1</math> 和 <math>\beta_2</math> 的[[偏导数|偏导数]]，然后使它们等于零得到。

:<math>\frac{\partial S}{\partial \beta_1}=0=8\beta_1 + 20\beta_2 -56</math>
:<math>\frac{\partial S}{\partial \beta_2}=0=20\beta_1 + 60\beta_2 -154.</math>

如此就得到了一个只有两个未知数的方程组，很容易就可以解出：

:<math>\beta_1=3.5</math>
:<math>\beta_2=1.4</math>

也就是说直线 <math>y=3.5+1.4x</math> 是最佳的。

== 简介 ==
===歷史背景===
最小平方法發展於[[天文學|天文學]]和[[大地测量学|大地测量学]]領域，科學家和數學家嘗試為[[地理大发现|大航海探索時期]]的海洋航行挑戰提供解決方案。準確描述天體的行為是船艦在大海洋上航行的關鍵，水手不能再依靠陸上目標導航作航行。

這個方法是在十八世紀期間一些進步的集大成：
*不同觀測值的組合是真實值的最佳估計；多次觀測會減少誤差而不是增加，也許在1722年由Roger Cotes首先闡明。

*在相同條件下採取的不同觀察結果，與只嘗試記錄一次最精確的觀察結果是對立的。這個方法被稱為平均值方法。托馬斯·馬耶爾（Tobias Mayer）在1750年研究月球的[[天平動|天平動]]時，特別使用這種方法，而[[皮埃尔-西蒙·拉普拉斯|拉普拉斯]]（Pierre-Simon Laplace）在1788年他的工作成果中以此解釋[[木星|木星]]和[[土星|土星]]的運動差異。

*在不同條件下進行的不同觀測值組合。該方法被稱為最小絕對偏差法，出現在Roger Joseph Boscovich在1757年他對地球形體的著名作品，而拉普拉斯在1799年也表示了同樣的問題。

*評定對誤差達到最小的解決方案標準，拉普拉斯指明了誤差的[[概率|概率]]密度的數學形式，並定義了誤差最小化的估計方法。為此，拉普拉斯使用了一雙邊對稱的指數分佈，現在稱為[[拉普拉斯分布|拉普拉斯分佈]]作為誤差分佈的模型，並將絕對偏差之和作為估計誤差。他認為這是他最簡單的假設，他期待得出算術平均值而成為最佳的估計。可相反地，他的估計是後驗中位數。

=== 最小平方法 ===
[[File:Carl_Friedrich_Gauss.jpg|250px]]

1801年，意大利天文学家[[朱赛普·皮亚齐|朱赛普·皮亚齐]]发现了第一颗小行星[[谷神星|谷神星]]。经过40天的跟踪观测后，由于谷神星运行至太阳背后，使得皮亚齐失去了谷神星的位置。随后全世界的科学家利用皮亚齐的观测数据开始寻找谷神星，但是根据大多数人计算的结果来寻找谷神星都没有结果。时年24岁的[[卡尔·弗里德里希·高斯|高斯]]也计算了谷神星的轨道。奥地利天文学家[[海因里希·奥伯斯|海因里希·奥伯斯]]根据高斯计算出来的轨道重新发现了谷神星。

[[高斯|高斯]]使用的最小二乘法的方法发表于1809年他的著作《天体运动论》中，而法国科学家[[勒让德|勒让德]]于1806年独立发现“最小二乘法”，但因不为世人所知而默默无闻。兩人曾为谁最早创立最小二乘法原理發生争执。

1829年，[[高斯|高斯]]提供了最小二乘法的优化效果强于其他方法的证明，見[[高斯-马尔可夫定理|高斯-马尔可夫定理]]。

== 方法 ==
人们对由某一变量<math>t</math> 或多个变量<math>t_{1}</math>……<math>t_{n}</math> 构成的相关变量<math>y</math>感兴趣。如[[弹簧|弹簧]]的[[形变|形变]]与所用的力相关，一个[[企业|企业]]的盈利与其[[营业额|营业额]]，[[投资收益|投资收益]]和[[原始资本|原始资本]]有关。为了得到这些变量同<math>y</math>之间的关系，便用不相关变量去构建<math>y</math>，使用如下函数模型

:<math>y_m = f(t_1,\dots, t_q;b_1,\dots,b_p)</math>,

<math>q</math>个獨立变量或<math>p</math>个係數去拟合。

通常人们将一个可能的、对不相关变量t的构成都无困难的[[函数|函数]]类型称作函数模型（如[[抛物线|抛物线]]函数或[[指數函數|指数]]函数）。参数b是为了使所选择的函数模型同观测值y相匹配。（如在测量弹簧形变时，必须将所用的力与弹簧的膨胀系数联系起来）。其目标是合适地选择参数，使函数模型最好的拟合观测值。'''一般情况下，观测值远多於所选择的参数。'''

其次的问题是怎样判断不同拟合的质量。[[高斯|高斯]]和[[勒让德|勒让德]]的方法是，'''假设测量误差的平均值为0'''。令每一个测量误差对应一个变量并与其它'''测量误差不相关（随机无关）'''。人们假设，在测量误差中绝对不含[[系统误差|系统误差]]，它们应该是'''纯[[偶然误差|偶然误差]](有固定的變異數)，围绕真值波动'''。除此之外，'''测量误差符合[[正态分布|正态分布]]'''，这保证了偏差值在最后的结果y上忽略不计。

确定拟合的标准应该被重视，并小心选择，较大误差的测量值应被赋予较小的[[權重|權]]。并建立如下规则：被选择的参数，应该使算出的'''函数曲线与观测值之差的平方和最小'''。用[[函数|函数]]表示为：

<math> \min_{\vec{b}} { \sum_{i=1}^{n}(y_m - y_i)^2} . </math>

用[[欧几里得度量|欧几里得度量]]表达为：

<math> \min_{ \vec{b} } \| \vec{y}_{m} ( \vec{b} ) - \vec{y} \|^2_{2} \ .</math>

又因为<math> \| \vec{y}_{m} ( \vec{b} ) - \vec{y} \|_{2} </math>  ≥0,

所以也可以表示为<math> \min_{ \vec{b} } \| \vec{y}_{m} ( \vec{b} ) - \vec{y} \|_{2} \ .</math> 

最小化问题的精度，依赖于所选择的函数模型。

== 线性函数模型 ==
典型的一类函数模型是线性函数模型。最简单的线性式是<math>y = b_0 + b_1 t</math>，写成矩陣式，为

:<math> \min_{b_0,b_1}\left\|\begin{pmatrix}1 & t_1 \\ \vdots & \vdots \\ 1 & t_n  \end{pmatrix} 
\begin{pmatrix} b_0\\ b_1\end{pmatrix} - \begin{pmatrix} y_1 \\ \vdots \\ y_{n}\end{pmatrix}\right\|_{2} = \min_b\|Ab-Y\|_2. </math>
直接给出该式的参数解：

:<math>b_1 = \frac{\sum_{i=1}^n t_iy_i - n \cdot \bar t \bar y}{\sum_{i=1}^n t_i^2- n \cdot (\bar t)^2}</math> 和 <math>b_0 = \bar y - b_1 \bar t </math>

其中<math>\bar t = \frac{1}{n} \sum_{i=1}^n t_i</math>，为t值的[[算术平均值|算术平均值]]。也可解得如下形式：

:<math>b_1 = \frac{\sum_{i=1}^n (t_i - \bar t)(y_i - \bar y)}{\sum_{i=1}^n (t_i - \bar t)^2}</math> 

===简单线性模型 y = b<sub>0</sub> + b<sub>1</sub>t 的例子===
随机选定10艘战舰，并分析它们的长度与宽度，寻找它们长度与宽度之间的关系。由下面的描点图可以直观地看出，一艘战舰的长度（t）与宽度（y）基本呈线性关系。散点图如下：
[[File:散点图.jpg|散点图]]

以下图表列出了各战舰的数据，随后步骤是采用最小二乘法确定两变量间的线性关系。

{| class="wikitable" style="margin: auto"
! align="right" | 编号
! 长度 (m)
! 宽度 (m)
! t<sub>i</sub> - <font style="text-decoration:overline">t</font>
! y<sub>i</sub> - <font style="text-decoration:overline">y</font>
|
|
|
|----
! align="right" | i
! t<sub>i</sub>
! y<sub>i</sub>
! t<sub>i</sub>*
! y<sub>i</sub>*
! t<sub>i</sub>*y<sub>i</sub>*
! t<sub>i</sub>*t<sub>i</sub>*
! y<sub>i</sub>*y<sub>i</sub>*
|---- align="right"
| 1
| 208
| 21.6
| 40.2
| 3.19
| 128.238
| 1616.04
| 10.1761
|---- align="right"
| 2
| 152
| 15.5
| -15.8
| -2.91
| 45.978
| 249.64
| 8.4681
|---- align="right"
| 3
| 113
| 10.4
| -54.8
| -8.01
| 438.948
| 3003.04
| 64.1601
|---- align="right"
| 4
| 227
| 31.0
| 59.2
| 12.59
| 745.328
| 3504.64
| 158.5081
|---- align="right"
| 5
| 137
| 13.0
| -30.8
| -5.41
| 166.628
| 948.64
| 29.2681
|---- align="right"
| 6
| 238
| 32.4
| 70.2
| 13.99
| 982.098
| 4928.04
| 195.7201
|---- align="right"
| 7
| 178
| 19.0
| 10.2
| 0.59
| 6.018
| 104.04
| 0.3481
|---- align="right"
| 8
| 104
| 10.4
| -63.8
| -8.01
| 511.038
| 4070.44
| 64.1601
|---- align="right"
| 9
| 191
| 19.0
| 23.2
| 0.59
| 13.688
| 538.24
| 0.3481
|---- align="right"
| 10
| 130
| 11.8
| -37.8
| -6.61
| 249.858
| 1428.84
| 43.6921
|---- align="right"
! 总和（Σ）
! 1678
! 184.1
! 0.0
! 0.00
! 3287.820
! 20391.60
! 574.8490
|}



仿照上面给出的例子

<math>\bar t = \frac {\sum_{i=1}^n t_i}{n} = \frac {1678}{10} = 167{.}8</math> 并得到相应的<math>\bar y = 18{.}41</math>.

然后确定b<sub>1</sub>

:<math>b_1 = \frac{\sum_{i=1}^n (t_i- \bar {t})(y_i - \bar y)}{\sum_{i=1}^n (t_i- \bar t)^2}</math> 

:<math> = \frac{3287{.}820} {20391{.}60} = 0{.}1612 \;,</math>

可以看出，战舰的长度每变化1m，相对应的宽度便要变化16cm。并由下式得到常数项b<sub>0</sub>：

:<math>b_0 = \bar y - b_1 \bar t = 18{.}41 - 0{.}1612 \cdot 167{.}8 = -8{.}6394
\;,</math>

在这里随机理论不加阐述。可以看出点的拟合非常好，[[长度|长度]]和[[宽度|宽度]]的相关性大约为96.03％。
利用Matlab得到拟合直线：
[[File:最小二乘法拟合.jpg|最小二乘法Matlab拟合]]

=== 一般线性情况 ===
若含有更多不相关模型变量<math>t_1, ..., t_q</math>，可如组成线性[[函数|函数]]的形式

:<math>y(t_1,\dots,t_q;b_0, b_1, \dots, b_q )= b_0 + b_1 t_1 + \cdots + b_q t_q </math>

即[[线性方程组|线性方程组]]

:<math> \begin{matrix}
b_0 + b_1 t_{11} + \cdots + b_j t_{1j}+ \cdots +b_q t_{1q} = y_1\\
b_0 + b_1 t_{21} + \cdots + b_j t_{2j}+ \cdots +b_q t_{2q} = y_2\\
\vdots \\
b_0 + b_1 t_{i1} + \cdots + b_j t_{ij}+ \cdots +b_q t_{iq}= y_i\\
\vdots\\
b_0 + b_1 t_{n1} + \cdots + b_j t_{nj}+ \cdots +b_q t_{nq}= y_n
\end{matrix}
</math>

通常人们将''t<sub>ij</sub>''记作数据矩阵 ''A''，参数''b<sub>j</sub>''记做参数向量''b''，观测值''y<sub>i</sub>''记作''Y''，则线性方程组又可写成：

:<math> \begin{pmatrix}
1 & t_{11} & \cdots & t_{1j} \cdots & t_{1q}\\
1 & t_{21} & \cdots & t_{2j} \cdots & t_{2q}\\
\vdots \\
1 & t_{i1} & \cdots & t_{ij} \cdots & t_{iq}\\
\vdots\\
1 & t_{n1} & \cdots & t_{nj} \cdots & t_{nq}
\end{pmatrix}
\cdot
\begin{pmatrix}
b_0\\
b_1\\
b_2\\
\vdots \\
b_j\\
\vdots\\
b_q
\end{pmatrix}
=
\begin{pmatrix}
y_1\\
y_2\\
\vdots \\
y_i\\
\vdots\\
y_n
\end{pmatrix}
</math>  即  <math>Ab = Y</math> 

上述方程运用最小二乘法导出为线性平方差计算的形式为：

:<math>\min_b\|Ab-Y\|_2</math>。

=== 最小二乘法的解 ===
<math>\min_b \left \|\boldsymbol{Ab}- \boldsymbol{Y} \right \|_2,\boldsymbol{A}\in\mathbf{C}^{n\times m},\boldsymbol{Y}\in\mathbf{C}^{n}</math>

的特解为'''''A'''''的[[广义逆矩阵|广义逆矩阵]]与'''''Y'''''的乘积，这同时也是二[[范数|范数]]极小的解，其通解为特解加上'''''A'''''的[[零空间|零空间]]。证明如下：

先将'''Y'''拆成'''''A'''''的值域及其[[正交补|正交补]]两部分
:<math>\boldsymbol{Y}=\boldsymbol{Y}_{1}+\boldsymbol{Y}_{2}</math>
:<math>\boldsymbol{Y}_{1}=\boldsymbol{A}\boldsymbol{A}^\dagger\boldsymbol{Y}\in R\left(\boldsymbol{A} \right)</math>
:<math>\boldsymbol{Y}_{2}=\left(\boldsymbol{I}- \boldsymbol{A}\boldsymbol{A}^\dagger \right)\boldsymbol{Y}\in R\left(\boldsymbol{A} \right)^{\bot}</math>
所以<math>\boldsymbol{Ab}-\boldsymbol{Y}_{1}\in R\left(\boldsymbol{A} \right)</math>，可得
:<math>\left \| \boldsymbol{Ab}- \boldsymbol{Y} \right \|^{2}=\left \| \boldsymbol{Ab}- \boldsymbol{Y}_{1} +\left(-\boldsymbol{Y}_{2}\right) \right \|^{2}=\left \| \boldsymbol{Ab}- \boldsymbol{Y}_{1} \right \|^{2}+\left \|\boldsymbol{Y}_{2} \right \|^{2}</math>
故当且仅当<math>\boldsymbol{b}</math>是<math>\boldsymbol{Ab}= \boldsymbol{Y}_{1} =\boldsymbol{A}\boldsymbol{A}^\dagger\boldsymbol{Y}</math>解时，<math>\boldsymbol{b}</math>即为最小二乘解，即<math>\boldsymbol{b}=\boldsymbol{A}^\dagger \boldsymbol{Y} = {\left( {{{\mathbf{A}}^H}{\mathbf{A}}} \right)^{ - 1}}{{\mathbf{A}}^H}{\mathbf{Y}}</math>。

又因为
:<math>N\left(\boldsymbol{A}\right)=N\left(\boldsymbol{A}^\dagger \boldsymbol{A}\right)=R\left(\boldsymbol{I}-\boldsymbol{A}^\dagger \boldsymbol{A}\right)=\left\{ \left(\boldsymbol{I}-\boldsymbol{A}^\dagger \boldsymbol{A} \right) \boldsymbol{h}:\boldsymbol{h}\in\mathbf{C}^{n}  \right\}</math>
故<math>\boldsymbol{Ab}=\boldsymbol{A}\boldsymbol{A}^\dagger\boldsymbol{Y}</math>的通解为
:<math>\boldsymbol{b}=\boldsymbol{A}^\dagger\boldsymbol{Y}+\left(\boldsymbol{I}-\boldsymbol{A}^\dagger \boldsymbol{A} \right) \boldsymbol{h}:\boldsymbol{h}\in\mathbf{C}^{n}  </math>
因为
:<math>\begin{align}
\left \| \boldsymbol{A}^\dagger\boldsymbol{Y}\right \|^{2} & < \left \| \boldsymbol{A}^\dagger\boldsymbol{Y} \right \|^{2}+ \left \| \left(\boldsymbol{I}-\boldsymbol{A}^\dagger \boldsymbol{A} \right) \boldsymbol{h} \right \|^{2} \\
& = \left \| \boldsymbol{A}^\dagger\boldsymbol{Y} + \left(\boldsymbol{I}-\boldsymbol{A}^\dagger \boldsymbol{A} \right) \boldsymbol{h} \right \|^{2},\left(\boldsymbol{I}-\boldsymbol{A}^\dagger \boldsymbol{A} \right) \boldsymbol{h}\neq\boldsymbol{0} \\
\end{align}  </math>
所以<math>\boldsymbol{A}^\dagger \boldsymbol{Y}</math>又是二[[范数|范数]]极小的最小二乘解。

== 参考文献 ==<!--
=== 引用 ===
{{Reflist}} -->

=== 书籍 ===
*{{cite book |author= Wang Guorong
|author2= Wei Yimin
|author3= Qiao SanZheng
|title= Generalized Inverses:Theory and Computations
|year= 2004
|publisher= Science Press
|location= Beijing
|language= en
|pages= 第6页
|chapter= Equation Solving Generalized Inverses
|isbn= 7-03-012437-5}}

== 外部链接 ==
* [http://molecular-service-science.com/2012/06/30/statistical-estimation/ 3種統計學點估計的理論推演:動差法,最小平方法,最大概似估計法]
* [http://libai.math.ncu.edu.tw/libai/about.html 中央大學數學系教材-最小平方法]
* http://www.physics.csbsju.edu/stats/least_squares.html
* http://www.zunzun.com
* http://www.orbitals.com/self/least/least.htm
* {{planetmath reference|id=1195|title=最小二乘法}}

{{Authority control}}
[[Category:最优化|Z]]
[[Category:统计学|Z]]
[[Category:误差理论|Z]]
[[Category:回归分析|Category:回归分析]]
[[Category:計量經濟學|Category:計量經濟學]]