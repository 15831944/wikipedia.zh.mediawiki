{{Expand|time=2009-07-26}}
{{noteTA
|1=zh-cn:方差;zh-tw:標準差;
|2=zh-cn:互不相关;zh-tw:彼此獨立;
}}
{{回归侧栏}}
在[[统计学|统计学]]中，'''高斯-马尔可夫定理(Gauss-Markov Theorem)'''陈述的是：在[[线性回归|线性回归]]模型中，如果误差满足零[[均值|均值]]、[[协方差|同方差]]且[[Uncorrelated_random_variables|互不相关]]，则回归系数的最佳线性[[偏差|无偏]][[估计量|估计]]('''BLUE''', Best Linear unbiased estimator)就是[[最小二乘法|普通最小二乘法估计]]。

* 这里'''最佳'''的意思是指相较于其他估计量有更小[[方差|方差]]的估计量，同时把对估计量的寻找限制在'''所有可能的线性无偏估计量'''中。
* '''值得注意'''的是这里不需要假定误差满足{{tsl|en|iid|独立同分布(''iid'')}}或[[正态分布|正态分布]]，而仅需要满足'''零均值'''、'''不相关'''及'''同方差'''这三个稍弱的条件。

== 表述 ==

=== 简单（一元）线性回归模型 ===

对于简单（一元）线性回归模型，

:<math>y_i=\beta_0+\beta_1 x_i+\varepsilon_i; \quad i = 1, \dots n.</math>

其中<math>\beta_0</math>和<math>\beta_1</math>是'''非随机'''但不能观测到的参数，<math>x_i</math>是'''非随机'''且可观测到的一般变量，<math>\varepsilon_i</math>是'''不可观测'''的随机变量，或称为随机误差或噪音，因此<math>y_i</math>是'''可观测'''的随机变量。

'''高斯-马尔可夫定理的假设条件'''是：

* <math>{\rm E}\left(\varepsilon_i\right)=0</math> ，<math>\forall i</math>（零均值），
* <math>{\rm Var}\left(\varepsilon_i\right)=\sigma^2<\infty</math>，<math>\forall i</math>（同方差），
* <math>{\rm Cov}\left(\varepsilon_i,\varepsilon_j\right)=0</math>，<math>\forall i\not=j</math>（不相关）。

则对<math>\beta_0</math>和<math>\beta_1</math>的最佳线性无偏估计为，

:<math>
    \hat\beta_1 = \frac{ \sum{x_iy_i} - \frac{1}{n}\sum{x_i}\sum{y_i} }
                     { \sum{x_i^2} - \frac{1}{n}(\sum{x_i})^2 } = \frac{ \operatorname{Cov}(x,y) }{ \sigma^2_x } , \quad
    \hat\beta_0 = \overline{y} - \hat\beta_1\,\overline{x}\ .
</math>

=== 多元线性回归模型 ===

对于多元线性回归模型，

:<math>y_i=\sum_{j=0}^p \beta_j x_{ij}+\varepsilon_i</math>, <math>x_{i0}=1; \quad i = 1, \dots n.</math>

使用矩阵形式，线性回归模型可简化记为<math>\mathbf{Y}=\mathbf{X}\boldsymbol{\beta}+\boldsymbol{\varepsilon}</math>，其中采用了以下记号：

<math>\mathbf{Y}=(y_1, y_2, \dots, y_n)^T</math> (观测值向量，Vector of Responses), 

<math>\mathbf{X}=(x_{ij})=
    \begin{bmatrix}
    1 & x_{11} & x_{12} & \cdots & x_{1p} \\
    1 & x_{21} & x_{22} & \cdots & x_{2p} \\
    \vdots & \vdots & \vdots & \ddots & \vdots \\
    1 & x_{n1} & x_{n2} & \cdots & x_{np}
    \end{bmatrix}
</math> (设计矩阵，Design Matrix),

<math>\boldsymbol{\beta}=(\beta_0, \beta_1, \dots, \beta_p)^T</math> (参数向量，Vector of Parameters), 

<math>\boldsymbol{\varepsilon}=(\varepsilon_1, \varepsilon_2, \dots, \varepsilon_n)^T</math> (随机误差向量，Vectors of Error)。

'''高斯-马尔可夫定理的假设条件'''是：

* <math>{\rm E}\left(\boldsymbol{\varepsilon}\mid\mathbf{X}\right)=0</math> ，<math>\forall \mathbf{X}</math>（零均值），
* <math>{\rm Var}\left(\boldsymbol{\varepsilon}\mid\mathbf{X}\right)={\rm E}\left(\boldsymbol{\varepsilon}\boldsymbol{\varepsilon}^T\mid\mathbf{X}\right)=\sigma^2_\varepsilon\mathbf{I_n}</math>，（同方差且不相关），其中<math>\mathbf{I_n}</math>为n阶[[单位矩阵|单位矩阵]](Identity Matrix)。

则对<math>\boldsymbol{\beta}</math>的最佳线性无偏估计为

:<math>
    \hat\boldsymbol{\beta} = (\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{Y}
</math>

== 证明 ==

'''首先'''，注意的是这里数据是<math>\mathbf{Y}</math>而非<math>\mathbf{X}</math>，我们希望找到<math>\boldsymbol{\beta}</math>对于<math>\mathbf{Y}</math>的线性估计量，记作

:<math>\hat\boldsymbol{\beta} = \mathbf{M}+\mathbf{N}\mathbf{Y}</math>

其中<math>\hat\boldsymbol{\beta}</math>，<math>\mathbf{M}</math>，<math>\mathbf{N}</math>和<math>\mathbf{Y}</math>分别是<math>(p+1)\times1</math>，<math>(p+1)\times1</math>，<math>(p+1)\times n</math>和<math>n\times1</math>矩阵。

根据零均值假设所得，

:<math>{\rm E}\left(\hat\boldsymbol{\beta}\mid\mathbf{X}\right) = \mathbf{M}+\mathbf{N}{\rm E}\left(\mathbf{Y}\mid\mathbf{X}\right)=\mathbf{M}+\mathbf{N}\mathbf{X}\boldsymbol{\beta}</math>

'''其次'''，我们同时限制寻找的估计量为无偏的估计量，即要求<math>{\rm E}\left(\hat\boldsymbol{\beta}\right) = \boldsymbol{\beta}</math>，因此有

:<math>\mathbf{M}=\mathbf{0}</math>（零矩阵），<math>\mathbf{N}\mathbf{X}=\mathbf{I_{p+1}}</math>

== 外部連結 ==
*[https://web.archive.org/web/19990508225359/http://members.aol.com/jeff570/g.html Earliest Known Uses of Some of the Words of Mathematics: G] (brief history and explanation of its name)
*[http://www.xycoon.com/ols1.htm Proof of the Gauss Markov theorem for multiple linear regression] (makes use of matrix algebra)
*[https://web.archive.org/web/20040213071852/http://emlab.berkeley.edu/GMTheorem/index.html A Proof of the Gauss Markov theorem using geometry]
[[Category:数学定理|G]]
[[Category:统计学|Category:统计学]]