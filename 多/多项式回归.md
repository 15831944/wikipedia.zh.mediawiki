
在[[统计学|统计学]]中， '''多项式回归'''是[[迴歸分析|回归分析]]的一种形式，其中[[自变量和因变量|自变量]] x 和[[自变量和因变量|因变量]] y 之间的关系被建模为关于 x 的 n 次[[多项式|多项式]]。多项式回归拟合x的值与 y 的相应条件均值之间的非线性关系，表示为 E(y|x)，并且已被用于描述非线性现象，例如组织的生长速率<ref>{{Cite journal|title=Intellectual ability and cortical development in children and adolescents|last=Shaw|first=P|journal=Nature|issue=7084|doi=10.1038/nature04513|year=2006|volume=440|pages=676–679|pmid=16572172|display-authors=etal}}</ref>、湖中碳同位素的分布<ref>{{Cite journal|title=A 14,000-Year Oxygen Isotope Record from Diatom Silica in Two Alpine Lakes on Mt. Kenya|last=Barker|first=PA|last2=Street-Perrott|first2=FA|journal=Science|issue=5525|doi=10.1126/science.1059612|year=2001|volume=292|pages=2307–2310|pmid=11423656|last3=Leng|first3=MJ|last4=Greenwood|first4=PB|last5=Swain|first5=DL|last6=Perrott|first6=RA|last7=Telford|first7=RJ|last8=Ficken|first8=KJ}}</ref>以及沉积物和流行病的发展<ref>{{Cite journal|title=Dose-Response and Trend Analysis in Epidemiology: Alternatives to Categorical Analysis|last=Greenland|first=Sander|journal=Epidemiology|issue=4|doi=10.1097/00001648-199507000-00005|year=1995|volume=6|pages=356–365|jstor=3702080|pmid=7548341}}</ref>。虽然多项式回归是拟合数据的非线性模型，但作为[[估计理论|统计估计]]问题，它是线性的。在某种意义上，回归函数 E(y|x) 在从数据估计到的未知参数中是线性的。因此，多项式回归被认为是多元[[線性回歸|线性回归]]的特例。

由“基线”变量的多项式展开得到的解释性（独立）变量称为高次项，这些变量也用于分类场景。<ref name="Chang2010">{{Cite journal|title=Training and testing low-degree polynomial data mappings via linear SVM|url=http://jmlr.csail.mit.edu/papers/v11/chang10a.html|last=Yin-Wen Chang|last2=Cho-Jui Hsieh|journal=[[Journal_of_Machine_Learning_Research|Journal of Machine Learning Research]]|year=2010|volume=11|pages=1471–1490|last3=Kai-Wei Chang|last4=Michael Ringgaard|last5=Chih-Jen Lin}}</ref>

== 历史 ==
多项式回归模型通常使用[[最小二乘法|最小二乘法]]来拟合。在[[高斯-马尔可夫定理|高斯-马尔可夫定理]]的条件下，最小二乘法最小化系数的无偏估计的[[方差|方差]]。最小二乘法由[[阿德里安-马里·勒让德|勒让德]]于1805年发表，1809年由[[卡爾·弗里德里希·高斯|高斯]]发表。多项式回归实验的第一个设计出现在1815年的 Gergonne 的论文中。<ref>{{Cite journal|title=The application of the method of least squares to the interpolation of sequences|url=http://www.sciencedirect.com/science/article/B6WG9-4D7JMHH-20/2/df451ec5fbb7c044d0f4d900af80ec86|last=[[Joseph_Diaz_Gergonne|Gergonne, J. D.]]|date=November 1974|journal=Historia Mathematica|issue=4|doi=10.1016/0315-0860(74)90034-2|origyear=1815|edition=Translated by Ralph St. John and [[Stephen_M._Stigler|S. M. Stigler]] from the 1815 French|volume=1|pages=439–447}}</ref><ref>{{Cite journal|title=Gergonne's 1815 paper on the design and analysis of polynomial regression experiments|url=http://www.sciencedirect.com/science/article/B6WG9-4D7JMHH-1Y/2/680c7ada0198761e9866197d53512ab4|last=[[Stephen_M._Stigler|Stigler, Stephen M.]]|date=November 1974|journal=Historia Mathematica|issue=4|doi=10.1016/0315-0860(74)90033-0|volume=1|pages=431–439}}</ref> 在二十世纪，多项式回归在[[回归分析|回归分析]]的发展中起着重要作用，更加强调设计和推理的问题。<ref>{{Cite journal|title=On the Standard Deviations of Adjusted and Interpolated Values of an Observed Polynomial Function and its Constants and the Guidance They Give Towards a Proper Choice of the Distribution of the Observations|last=[http://www.webdoe.cc/publications/kirstine.php Smith, Kirstine]|journal=Biometrika|issue=1/2|doi=10.2307/2331929|year=1918|volume=12|pages=1–85|jstor=2331929}}</ref>

== 定义和实例 ==
[[File:Polyreg_scheffe.svg|缩略图]]是使用Scheffé方法构建的95％置信区间。]]
回归分析的目标是根据自变量（或自变量向量）x 的值来模拟因变量 y 的期望值。在简单的线性回归中，使用模型

: <math>
y = \beta_0 + \beta_1 x + \varepsilon, \,
</math>

其中ε是未观察到的随机[[误差|误差]]，其以[[标量|标量]] x 为条件，均值为零。在该模型中，对于 x 值的每个单位增加，y 的条件期望增加 <math>\beta_{1}</math>个单位。

在许多情况下，这种线性关系可能不成立。例如，如果我们根据合成发生的温度对化学合成的产率进行建模，我们可以发现通过增加每单位温度增加的量来提高产率。在这种情况下，我们可能会提出如下所示的二次模型：

: <math>y = \beta_0 + \beta_1x + \beta_2 x^2 + \varepsilon </math>。

在该模型中，当温度从x增加到x+1单位时，预期产量会变化<math>\beta_1+\beta_2(2x+ 1).</math><math>\beta_{1}+\beta_{2}(2x+1)</math>。对于 x 的[[無窮小量|无穷小]]变化，对 y 的影响由关于 x 的[[导数|导数]]给出：<math>\beta_1+\beta_2(2x+ 1)</math>, 即使模型在待估计的参数中是线性的，但是产量的变化取决于 x 的事实使得 x 和 y 之间的关系为非线性关系。

通常，我们可以将 y 的期望值建模为 n 次多项式，得到一般多项式回归模型：

: <math>y = \beta_0 + \beta_1 x + \beta_2 x^2 + \beta_3 x^3 + \cdots + \beta_n x^n + \varepsilon</math><math>
y = \beta_0 + \beta_1 x + \beta_2 x^2 + \beta_3 x^3 + \cdots + \beta_n x^n + \varepsilon. \, 
</math>。

为了方便，这些模型从[[估计理论|估计]]的角度来看都是线性的，因为回归函数就未知参数β<sub>0</sub>、β<sub>1</sub>等而言是线性的。因此，对于[[最小二乘法|最小二乘]]分析，多项式回归的计算和推理问题可以使用多元回归技术完全解决，这是通过将 x、x<sup>2</sup> 等视为多元回归模型中的独特自变量来完成的。

== 矩阵形式和估计计算 ==
多项式回归模型

: <math>y_i \,=\, \beta_0 + \beta_1 x_i + \beta_2 x_i^2 + \cdots + \beta_m x_i^m + \varepsilon_i\  (i = 1, 2, \dots , n) </math>

可以由设计矩阵 <math>\mathbf{X}</math>、响应矢量 <math>\vec y</math>、矢量参数 <math>\vec \beta</math>和随机误差向量 <math>\vec\varepsilon</math>来表示。在第 i 行的 <math>\mathbf{X}</math>和 <math>\vec y</math>为第 i 个数据样本的 x 和 y 值。然后该模型可以写成线性方程组：

: <math> \begin{bmatrix} y_1\\ y_2\\ y_3 \\ \vdots \\ y_n \end{bmatrix}= \begin{bmatrix} 1 & x_1 & x_1^2 & \dots & x_1^m \\ 1 & x_2 & x_2^2 & \dots & x_2^m \\ 1 & x_3 & x_3^2 & \dots & x_3^m \\ \vdots & \vdots & \vdots & \ddots & \vdots \\ 1 & x_n & x_n^2 & \dots & x_n^m \end{bmatrix} \begin{bmatrix} \beta_0\\ \beta_1\\ \beta_2\\ \vdots \\ \beta_m \end{bmatrix} + \begin{bmatrix} \varepsilon_1\\ \varepsilon_2\\ \varepsilon_3 \\ \vdots \\ \varepsilon_n \end{bmatrix}, </math>

当使用纯矩阵表示法时，将其写为

: <math>\vec y = \mathbf{X} \vec \beta + \vec\varepsilon</math><math>\vec y = \mathbf{X} \vec \beta + \vec\varepsilon. \,</math>。

估计多项式回归系数的向量（使用[[最小二乘|最小二乘]]估计）为

: <math>\widehat{\vec \beta} = (\mathbf{X}^\mathsf{T} \mathbf{X})^{-1}\; \mathbf{X}^\mathsf{T} \vec y, \,</math>

假设 m<n 是矩阵可逆的必要条件，那么由于 <math>\mathbf{X}</math>是 [[范德蒙矩陣|范德蒙矩阵]]，如果所有 <math>x_i</math>值都不同，则保证可逆性条件成立，这是唯一的最小二乘法的解。

== 解释 ==
虽然多项式回归在技术上是多元线性回归的一个特例，但拟合多项式回归模型的解释需要一个不同的视角。通常难以在多项式回归拟合中解释各个系数，因为基础单项式可以高度相关。例如，当 x 具有在区间 (0,1) 上的[[均勻分佈|均匀分布]]时，x 和 x<sup>2</sup> 具有大约0.97的相关性。尽管可以通过使用[[正交多項式|正交多项式]]来减少相关性，但是通常将拟合的回归函数视为整体来提供更多信息，然后可以使用逐点或同时[[置信区间|置信区间]]来提供回归函数估计的不确定性。

== 替代方法 ==
多项式回归是使用[[基函數|基函数]]来模拟两个变量之间的函数关系的回归分析的一个实例。更具体地说，它用多项式基 <math>\varphi (x) \in \mathbb R^{d_\varphi}</math>替换线性回归中的 <math>x \in \mathbb R^{d_x}</math>，如 <math>[1,x] \mathbin{\stackrel{\varphi}{\rightarrow}} [1,x,x^2,\ldots,x^d]</math>。多项式基的一个缺点在于基函数是“非局部的”，这意味在给定值x = x<sub>0</sub> 处 y 的拟合值很大程度依赖于 x 远离 x<sub>0</sub>的数据值<ref>
Such "non-local" behavior is a property of [[Analytic_function#Properties_of_analytic_functions|analytic function]]s that are not constant (everywhere). Such "non-local" behavior has been widely discussed in statistics: 
*{{cite journal | doi=10.2307/2685560 | last=Magee | first=Lonnie | journal=The American Statistician | title=Nonlocal Behavior in Polynomial Regressions | volume=52 | year=1998 | jstor=2685560 | pages=20–22 | issue=1 }}</ref>。在现代统计中，多项式基函数与新的基函数一起使用，如[[样条函数|样条函数]]、[[径向基函数|径向基函数]]和[[小波|小波]]。这些基函数族为许多类型的数据提供了更简约的拟合。

多项式回归的目标是模拟独立变量和因变量之间的非线性关系（在自变量和因变量的条件均值之间）。这与非参数回归的目标类似，非参数回归旨在捕获非线性回归关系。因此，诸如平滑的非参数回归方法可以是多项式回归的有效替代方案。这些方法中的一些利用了经典多项式回归的局部形式。<ref>{{Cite book|last=Fan|first=Jianqing|year=1996|title=Local Polynomial Modelling and Its Applications: From linear regression to nonlinear regression|series=Monographs on Statistics and Applied Probability|publisher=Chapman & Hall/CRC.|isbn=978-0-412-98321-4}}</ref> 传统多项式回归的一个优点是可以使用多元回归的推理框架（当使用其他基函数族，如样条函数时也是如此）。

另外一种方法是使用核方法模型，如[[支持向量机|支持向量回归]]和多项式核。

== 参见 ==

* [[曲線擬合|曲线拟合]]
* [[線性回歸|线性回归]]
* {{tsl|en|Local polynomial regression|局部多项式回归}}
* {{tsl|en|Polynomial and rational function modeling|多项式和有理函数建模}}
* [[多项式插值|多项式差值]]
* [[反應曲面法|反应曲面法]]
* {{tsl|en|Smoothing spline|平滑样条曲线}}

== 参考文献 ==
<references group=""></references>
[[Category:回归分析|Category:回归分析]]
[[Category:有未审阅翻译的页面|Category:有未审阅翻译的页面]]