{{机器学习导航栏}}

'''多层感知器'''（Multilayer Perceptron,缩写MLP）是一种前向结构的[[人工神经网络|人工神经网络]]，映射一组输入向量到一组输出向量。MLP可以被看作是一个有向图，由多个的节点层所组成，每一层都全连接到下一层。除了输入节点，每个节点都是一个带有非线性激活函数的神经元（或称处理单元）。一种被称为[[反向传播算法|反向传播算法]]的[[监督学习|监督学习]]方法常被用来训练MLP。<ref>Rosenblatt, Frank. x. Principles of Neurodynamics: Perceptrons and the Theory of Brain Mechanisms. Spartan Books, Washington DC, 1961</ref><ref>Rumelhart, David E., Geoffrey E. Hinton, and R. J. Williams.“Learning Internal Representations by Error Propagation”. David E. Rumelhart, James L. McClelland, and the PDP research group.（editors）, Parallel distributed processing: Explorations in the microstructure of cognition, Volume 1: Foundations. MIT Press, 1986.</ref>  MLP是[[感知器|感知器]]的推广，克服了感知器不能对[[线性不可分|线性不可分]]数据进行识别的弱点。<ref>Cybenko, G. 1989. Approximation by superpositions of a sigmoidal function ''[[Mathematics_of_Control,_Signals,_and_Systems|Mathematics of Control, Signals, and Systems]]'', 2（4）, 303–314.</ref>

== 理论 ==

=== [[激活函数|激活函数]] ===
若每个神经元的激活函数都是线性函数，那么，任意层数的MLP都可被约简成一个等价的单层[[感知器|感知器]]。<ref>{{cite book |authorlink=Christopher M. Bishop |title=Neural Networks for pattern recognition |edition=第一版 |year=1995 |publisher=Oxford University Press |location= |isbn=0198538642 }}</ref>

实际上，MLP本身可以使用任何形式的激活函数，譬如阶梯函数或[[逻辑乙形函数|逻辑乙形函数]]（logistic sigmoid function），但为了使用反向传播算法进行有效学习，激活函数必须限制为[[可微函数|可微函数]]。由于具有良好可微性，很多[[S函数|S函数]]，尤其是[[双曲正切函数|双曲正切函数]]（Hyperbolic tangent）及[[逻辑函数|逻辑函数]]，被采用为激活函数。

==应用==
常被MLP用来进行学习的反向传播算法，在模式识别的领域中算是标准监督学习算法，并在计算神经学及并行分布式处理领域中，持续成为被研究的课题。MLP已被证明是一种通用的函数近似方法，可以被用来[[拟合|拟合]]复杂的函数，或解决[[分类|分类]]问题。

MLP在80年代的时候曾是相当流行的机器学习方法，拥有广泛的应用场景，譬如语音识别、图像识别、机器翻译等等，但自90年代以来，MLP遇到来自更为简单的支持向量机的强劲竞争。近来，由于[[深度学习|深度学习]]的成功，MLP又重新得到了关注。

== 文献 ==
{{reflist}}

[[Category:机器学习|Category:机器学习]]
[[Category:人工智能|Category:人工智能]]