{{机器学习导航栏}}
{{ request translation }}

{{noteTA
|G1=IT
}}

'''卷积神经网络'''（Convolutional  Neural Network, '''CNN'''）是一种[[前馈神经网络|前馈神经网络]]，它的人工神经元可以响应一部分覆盖范围内的周围单元，<ref name="deeplearning">{{cite web|title=Convolutional Neural Networks (LeNet) - DeepLearning 0.1 documentation|url=http://deeplearning.net/tutorial/lenet.html|work=DeepLearning 0.1|publisher=LISA Lab|accessdate=31 August 2013}}</ref>对于大型图像处理有出色表现。

卷积神经网络由一个或多个卷积层和顶端的全连通层（对应经典的神经网络）组成，同时也包括关联权重和池化层（pooling layer）。这一结构使得卷积神经网络能够利用输入数据的二维结构。与其他深度学习结构相比，卷积神经网络在图像和[[语音识别|语音识别]]方面能够给出更好的结果。这一模型也可以使用[[反向传播算法|反向传播算法]]进行训练。相比较其他深度、前馈神经网络，卷积神经网络需要考量的参数更少，使之成为一种颇具吸引力的深度学习结构<ref name=""STANCNN">{{cite web|url=http://ufldl.stanford.edu/tutorial/supervised/ConvolutionalNeuralNetwork/|title=Convolutional Neural Network|accessdate=2014-09-16}}</ref>。

==概览==

==发展==
==结构==

===卷積層===
卷積層（Convolutional layer），卷積神經網路中每层卷积层由若干卷积单元组成，每个卷积单元的参数都是通过[[反向传播算法|反向传播算法]]最佳化得到的。卷积运算的目的是提取输入的不同特征，第一层卷积层可能只能提取一些低级的特征如边缘、线条和角等层级，更多层的网路能从低级特征中迭代提取更复杂的特征。

===線性整流層===
線性整流層（Rectified Linear Units layer, ReLU layer）使用[[线性整流函数|線性整流]]（Rectified Linear Units, ReLU）<math alt="function of x equals maximum between zero and x">f(x)=\max(0,x)</math>作为這一層神經的激勵函數（Activation function）。它可以增强判定函数和整个神经网络的非线性特性，而本身并不会改变卷积层。

事实上，其他的一些函数也可以用于增强网络的非线性特性，如[[双曲正切函数|双曲正切函数]] <math alt="f x等于x的双曲正切">f(x)=\tanh(x)</math>, <math alt="f x等于x的双曲正切的绝对值">f(x)=|\tanh(x)|</math>，或者[[S函数|Sigmoid函数]]<math alt="f x 等于 1 加 e的负x次方的倒数">f(x)=(1+e^{-x} )^{-1}</math>。相比其它函数来说，ReLU函数更受青睐，这是因为它可以将神经网络的训练速度提升数倍<ref>{{cite journal|last=Krizhevsky|first=A.|author2=Sutskever, I.|author3=Hinton, G. E.|title=Imagenet classification with deep convolutional neural networks|journal=Advances in Neural Information Processing Systems|volume=1|year=2012|pages=1097–1105|url=http://papers.nips.cc/paper/4824-imagenet|deadurl=yes|archiveurl=https://web.archive.org/web/20150216025624/http://papers.nips.cc/paper/4824-imagenet|archivedate=2015-02-16}}</ref>，而并不会对模型的泛化准确度造成显著影响。

===池化層===
[[File:Max_pooling.png|thumb]]
池化（Pooling）是卷积神经网络中另一个重要的概念，它实际上是一种形式的降采样。有多种不同形式的非线性池化函数，而其中“最大池化（Max pooling）”是最为常见的。它是将输入的图像划分为若干个矩形区域，对每个子区域输出最大值。直觉上，这种机制能够有效地原因在于，在发现一个特征之后，它的精确位置远不及它和其他特征的相对位置的关系重要。池化层会不断地减小数据的空间大小，因此参数的数量和计算量也会下降，这在一定程度上也控制了[[过拟合|过拟合]]。通常来说，CNN的卷积层之间都会周期性地插入池化层。

池化层通常会分别作用于每个输入的特征并减小其大小。目前最常用形式的池化层是每隔2个元素从图像划分出<math>2 \times 2</math>的区块，然后对每个区块中的4个数取最大值。这将会减少75%的数据量。

除了最大池化之外，池化层也可以使用其他池化函数，例如“平均池化”甚至“[[Lp空间#.E9.95.BF.E5.BA.A6.E3.80.81.E8.B7.9D.E7.A6.BB.E4.B8.8E.E8.8C.83.E6.95.B0|L2-范数]]池化”等。过去，平均池化的使用曾经较为广泛，但是最近由于最大池化在实践中的表现更好，平均池化已经不太常用。

由于池化层过快地减少了数据的大小，目前文献中的趋势是使用较小的池化滤镜，<ref>{{cite arXiv|title = Fractional Max-Pooling|eprint= 1412.6071|date = 2014-12-18|first = Benjamin|last = Graham|class= cs.CV}}</ref>甚至不再使用池化层。<ref>{{cite arXiv|title = Striving for Simplicity: The All Convolutional Net|eprint= 1412.6806|date = 2014-12-21|first = Jost Tobias|last = Springenberg|first2 = Alexey|last2 = Dosovitskiy|first3 = Thomas|last3 = Brox|first4 = Martin|last4 = Riedmiller|class= cs.LG}}</ref>

===损失函数层===
损失函数层（loss layer）用于决定训练过程如何来“惩罚”网络的预测结果和真实结果之间的差异，它通常是网络的最后一层。各种不同的损失函数适用于不同类型的任务。例如，[[Softmax函数|Softmax]]交叉熵损失函数常常被用于在K个类别中选出一个，而[[邏輯函數|Sigmoid]]交叉熵损失函数常常用于多个独立的二分类问题。欧几里德损失函数常常用于结果取值范围为任意实数的问题。

==应用==
===影像辨識===
卷积神经网络通常在影像辨識系统中使用。

===視訊分析===
相比影像辨識问题，視訊分析要难许多。CNN也常被用于这类问题。

===自然语言处理===
卷积神经网络也常被用于[[自然语言处理|自然语言处理]]。 CNN的模型被证明可以有效的处理各种自然语言处理的问题，如语义分析<ref>{{cite arXiv|title = A Deep Architecture for Semantic Parsing|eprint= 1404.7296|date = 2014-04-29|first = Edward|last = Grefenstette|first2 = Phil|last2 = Blunsom|first3 = Nando|last3 = de Freitas|first4 = Karl Moritz|last4 = Hermann|class= cs.CL}}</ref>、搜索结果提取<ref>{{Cite web|title = Learning Semantic Representations Using Convolutional Neural Networks for Web Search – Microsoft Research|url = http://research.microsoft.com/apps/pubs/default.aspx?id=214617|website = research.microsoft.com|accessdate = 2015-12-17}}</ref>、句子建模<ref>{{cite arXiv|title = A Convolutional Neural Network for Modelling Sentences|eprint= 1404.2188|date = 2014-04-08|first = Nal|last = Kalchbrenner|first2 = Edward|last2 = Grefenstette|first3 = Phil|last3 = Blunsom|class= cs.CL}}</ref> 、分类<ref>{{cite arXiv|title = Convolutional Neural Networks for Sentence Classification|eprint= 1408.5882|date = 2014-08-25|first = Yoon|last = Kim|class= cs.CL}}</ref>、预测<ref>Collobert, Ronan, and Jason Weston. "A unified architecture for natural language processing: Deep neural networks with multitask learning."Proceedings of the 25th international conference on Machine learning. ACM, 2008.</ref>、和其他传统的NLP任务<ref>{{cite arXiv|title = Natural Language Processing (almost) from Scratch|eprint= 1103.0398|date = 2011-03-02|first = Ronan|last = Collobert|first2 = Jason|last2 = Weston|first3 = Leon|last3 = Bottou|first4 = Michael|last4 = Karlen|first5 = Koray|last5 = Kavukcuoglu|first6 = Pavel|last6 = Kuksa|class= cs.LG}}</ref>
等。

===药物发现===
卷积神经网路已在药物发现中使用。卷积神经网络被用来预测的分子与蛋白质之间的相互作用，以此来寻找靶向位点，寻找出更可能安全和有效的潜在治疗方法。

===围棋===
{{Seealso|AlphaGo李世乭五番棋}}
卷积神经网络在计算机围棋领域也被使用。2016年3月，[[AlphaGo|AlphaGo]]对战[[李世乭|李世乭]]的比赛，展示了深度学习在围棋领域的重大突破。

==微调（fine-tuning）==
卷积神经网络（例如Alexnet、VGG网络）在网络的最后通常为softmax分类器。微调一般用来调整softmax分类器的分类数。例如原网络可以分类出2种图像，需要增加1个新的分类从而使网络可以分类出3种图像。微调（fine-tuning）可以留用之前训练的大多数参数，从而达到快速训练收敛的效果。例如保留各个卷积层，只重构卷积层后的全连接层与softmax层即可。

==經典模型==
*[[LeNet|LeNet]]

*[[AlexNet|AlexNet]]

*[[VGG|VGG]]

*[[GoogLeNet|GoogLeNet]]

*[[ResNet|ResNet]]

==可用包==
* [https://www.ronnie.ai roNNie]: 是一個簡易入門級框架,使用Tensorflow 計算層.可於python下載 pip3 ronnie
* Caffe: Caffe包含了CNN使用最广泛的库。它由伯克利视觉和学习中心（BVLC）研发，拥有比一般实现更好的结构和更快的速度。同时支持[[CPU|CPU]]和[[GPU|GPU]]计算，底层由[[C++|C++]]实现，并封装了Python和[[MATLAB|MATLAB]]的接口。
* Torch7（www.torch.ch）
* OverFeat
* Cuda-convnet
* MatConvnet
* [[Theano|Theano]]：用[[Python|Python]]实现的神经网络包<ref>[http://deeplearning.net/software/theano/ 深度网络：Theano项目主页。]</ref>
* [[TensorFlow|TensorFlow]]
* Paddlepaddle([http://www.paddlepaddle.org/ www.paddlepaddle.org])
* Keras

==参考==
{{reflist|2}}

[[Category:人工智能|Category:人工智能]]
[[Category:人工神经网络|Category:人工神经网络]]