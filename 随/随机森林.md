{{NoteTA|G1=IT}}
{{机器学习导航栏}}
在[[機器學習|機器學習]]中，'''隨機森林'''是一個包含多個[[決策樹|決策樹]]的[[分类问题|分類器]]，並且其輸出的類別是由個別樹輸出的類別的[[眾數|眾數]]而定。
[[Leo_Breiman|Leo Breiman]]和[[Adele_Cutler|Adele Cutler]]發展出推論出隨機森林的演算法。而"Random Forests"是他們的[[商標|商標]]。這個術語是1995年由貝爾實驗室的[[Tin_Kam_Ho|Tin Kam Ho]]所提出的'''隨機決策森林'''（'''random decision forests'''）而來的。這個方法則是結合Breimans的"[[Bootstrap_aggregating|Bootstrap aggregating]]"想法和Ho的"random subspace method"
以建造決策樹的集合。

==历史==
随机森林的引入最初是由Leo Breiman在一篇论文中提出的。<ref name="breiman2001"/>这篇文章描述了一种结合随机节点优化和bagging，利用类CART过程构建不相关树的森林的方法。此外，本文还结合了一些已知的、新颖的、构成了现代随机森林实践的基础成分，特别是

# 使用out-of-bag误差来代替泛化误差
# 通过排列度量变量的重要性

==算法==
===预备：决策树学习===
{{main article|决策树学习}}
决策树是各种机器学习任务的常用方法。 Hastie et al.说：“树学习是如今最能满足于数据挖掘的方法，因为它在特征值的缩放和其他各种转换下保持不变，对无关特征是鲁棒的，而且能生成可被检查的模型。然而，它通常并不准确。”<ref name="elemstatlearn">{{ElemStatLearn}}</ref>

特别的，生长很深的树容易学习到高度不规则的模式，即过学习，在训练集上具有低偏差和高方差的特点。随机森林是平均多个深决策树以降低方差的一种方法，其中，决策树是在一个数据集上的不同部分进行训练的。<ref name="elemstatlearn"/>这是以偏差的小幅增加和一些可解释性的丧失为代价的，但是在最终的模型中通常会大大提高性能。

===Bagging===
{{main article|Bootstrap aggregating}}
随机森林训练算法把[[bagging|bagging]]的一般技术应用到树学习中。给定训练集{{mvar|X}} = {{mvar|x<sub>1</sub>}}, ..., {{mvar|x<sub>n</sub>}}和目标{{mvar|Y}} = {{mvar|y<sub>1</sub>}}, ..., {{mvar|y<sub>n</sub>}}，bagging方法重复（''B''次）从训练集中有放回地采样，然后在这些样本上训练树模型：

: For {{mvar|b}} = 1, ..., {{mvar|B}}:
:# Sample, with replacement, {{mvar|n}} training examples from {{mvar|X}}, {{mvar|Y}}; call these {{mvar|X<sub>b</sub>}}, {{mvar|Y<sub>b</sub>}}.
:# Train a classification or regression tree {{mvar|f<sub>b</sub>}} on {{mvar|X<sub>b</sub>}}, {{mvar|Y<sub>b</sub>}}.

在训练结束之后，对未知样本x的预测可以通过对x上所有单个回归树的预测求平均来实现：

:<math>\hat{f} = \frac{1}{B} \sum_{b=1}^Bf_b (x')</math>

或者在分类任务中选择多数投票的类别。

这种bagging方法在不增加偏置的情况下降低了方差，从而带来了更好的性能。这意味着，即使单个树模型的预测对训练基的噪声非常敏感，但对于多个树模型，只要这些树并不相关，这种情况就不会出现。简单地在同一个数据集上训练多个树模型会产生强相关的树模型（甚至是完全相同的树模型）。[[Bootstrap抽样|Bootstrap抽样]]是一种通过产生不同训练集从而降低树模型之间关联性的方法。

此外，{{mvar|x'}}上所有单个回归树的预测的标准差可以作为预测的不确定性的估计：

:<math>\sigma = \sqrt{\frac{\sum_{b=1}^B (f_b(x')  - \hat{f})^2}{B-1} }.</math>

样本或者树的数量'''B'''是一个自由参数。通常使用几百到几千棵树，这取决于训练集的大小和性质。使用交叉验证，或者通过观察out-of-bag误差（那些不包含{{mvar|xᵢ}}的抽样集合在样本{{mvar|xᵢ}}的平均预测误差），可以找到最优的'''B'''值。当一些树训练到一定程度之后，训练集和测试集的误差开始趋于平稳。

===从bagging到随机森林===

上面的过程描述了树的原始的bagging算法。随机森林与这个通用的方案只有一点不同:它使用一种改进的学习算法，在学习过程中的每次候选分裂中选择[[随机子空间|特征的随机子集]]。这个过程有时又被称为“特征bagging”。这样做的原因是bootstrap抽样导致的树的相关性：如果有一些特征预测目标值的能力很强，那么这些特征就会被许多树所选择，这样就会导致树的强相关性。Ho分析了不同条件下bagging和随机子空间投影对精度提高的影响。<ref name="ho2002">
{{cite journal | first = Tin Kam | last = Ho | title = A Data Complexity Analysis of Comparative Advantages of Decision Forest Constructors | journal = Pattern Analysis and Applications | year = 2002 | pages = 102–112 | url = http://ect.bell-labs.com/who/tkh/publications/papers/compare.pdf }}</ref>

典型地，对于一个包含{{mvar|p}}个特征的分类问题，可以在每次划分时使用<math>\sqrt p</math>个特征。<ref name="elemstatlearn"/>{{rp|592}}对于回归问题，作者推荐{{mvar|p/3}} 但不少于5个特征。<ref name="elemstatlearn"/>{{rp|592}}

===极限树===
再加上一个随机化步骤，就会得到'''极限随机树'''（''extremely randomized trees''），即极限树。与普通的随机森林相同，他们都是单个树的集成，但也有不同：首先，每棵树都使用整个学习样本进行了训练，其次，自上而下的划分是随机的。它并不计算每个特征的最优划分点（例如，基于信息熵或者基尼不纯度），而是随机选择划分点。该值是从特征经验范围内均匀随机选取的。在所有随机的划分点中，选择其中分数最高的作为结点的划分点。与普通的随机森林相似，可以指定每个节点要选择的特征的个数。该参数的默认值，对于分类问题，是<math>\sqrt{n}</math>，对于回归问题，是<math>n</math>，其中 <math>n</math>是模型的特征个数。<ref>{{Cite journal | doi = 10.1007/s10994-006-6226-1| title = Extremely randomized trees| journal = Machine Learning| volume = 63| pages = 3–42| year = 2006| vauthors = Geurts P, Ernst D, Wehenkel L | url = http://orbi.ulg.ac.be/bitstream/2268/9357/1/geurts-mlj-advance.pdf}}</ref>

==性质==
===特征的重要性===
随机森林天然可用来对回归或分类问题中变量的重要性进行排序。下面的技术来自Breiman的论文，R语言包randomForest包含它的实现。<ref name="rpackage">{{cite web |url=https://cran.r-project.org/web/packages/randomForest/randomForest.pdf |title=Documentation for R package randomForest |first1=Andy |last1=Liaw | name-list-format = vanc | date=16 October 2012 |access-date=15 March 2013}}
</ref>

度量数据集 <math>\mathcal{D}_n =\{(X_i, Y_i)\}_{i=1}^n</math>的特征重要性的第一步是，使用训练集训练一个随机森林模型。在训练过程中记录下每个数据点的out-of-bag误差，然后在整个森林上进行平均。

为了度量第<math>j</math>个特征的重要性，第<math>j</math>个特征的值在训练数据中被打乱，并重新计算打乱后的数据的out-of-bag误差。则第<math>j</math>个特征的重要性分数可以通过计算打乱前后的out-of-bag误差的差值的平均来得到，这个分数通过计算这些差值的标准差进行标准化。

产生更大分数的特征比小分数的特征更重要。这种特征重要性的度量方法的统计定义由Zhu ''et al.''<ref>{{cite journal | vauthors = Zhu R, Zeng D, Kosorok MR | title = Reinforcement Learning Trees | journal = Journal of the American Statistical Association | volume = 110 | issue = 512 | pages = 1770–1784 | date = 2015 | pmid = 26903687 | pmc = 4760114 | doi = 10.1080/01621459.2015.1036994 }}</ref>给出。

这种度量方法也有一些缺陷。对于包含不同取值个数的类别特征，随机森林更偏向于那些取值个数较多的特征，[[partial_permutation|partial permutation]]s<ref>{{cite conference
|author=Deng,H.|author2=Runger, G. |author3=Tuv, E.
 |title=Bias of importance measures for multi-valued attributes and solutions
|conference=Proceedings of the 21st International Conference on Artificial Neural Networks (ICANN)
|year=2011|pages=293–300
|url=https://www.researchgate.net/profile/Houtao_Deng/publication/221079908_Bias_of_Importance_Measures_for_Multi-valued_Attributes_and_Solutions/links/0046351909faa8f0eb000000/Bias-of-Importance-Measures-for-Multi-valued-Attributes-and-Solutions.pdf
}}</ref><ref>{{cite journal | vauthors = Altmann A, Toloşi L, Sander O, Lengauer T | title = Permutation importance: a corrected feature importance measure | journal = Bioinformatics | volume = 26 | issue = 10 | pages = 1340–7 | date = May 2010 | pmid = 20385727 | doi = 10.1093/bioinformatics/btq134 | url = http://bioinformatics.oxfordjournals.org/content/early/2010/04/12/bioinformatics.btq134.abstract }}</ref>
、growing unbiased trees<ref>{{cite journal | last = Strobl | first = Carolin | last2 = Boulesteix | first2 = Anne-Laure | last3 = Augustin | first3 = Thomas | name-list-format = vanc | title = Unbiased split selection for classification trees based on the Gini index | journal = Computational Statistics & Data Analysis | year = 2007 | pages = 483–501 | url = https://epub.ub.uni-muenchen.de/1833/1/paper_464.pdf }}</ref><ref>{{cite journal|last1=Painsky|first1=Amichai|last2=Rosset|first2=Saharon| name-list-format = vanc |title=Cross-Validated Variable Selection in Tree-Based Methods Improves Predictive Performance|journal=IEEE Transactions on Pattern Analysis and Machine Intelligence|date=2017|volume=39|issue=11|pages=2142–2153|doi=10.1109/tpami.2016.2636831|pmid=28114007|arxiv=1512.03444}}</ref>可以用来解决这个问题。如果数据包含一些相互关联的特征组，那么更小的组更容易被选择。<ref>{{cite journal | vauthors = Tolosi L, Lengauer T | title = Classification with correlated features: unreliability of feature ranking and solutions | journal = Bioinformatics | volume = 27 | issue = 14 | pages = 1986–94 | date = July 2011 | pmid = 21576180 | doi = 10.1093/bioinformatics/btr300 | url = http://bioinformatics.oxfordjournals.org/content/27/14/1986.abstract }}</ref>

===与最近邻算法的关系===

Lin和Jeon在2002年指出了随机森林算法和[[K-nearest_neighbor_algorithm|{{mvar]] ({{mvar|k}}-NN)的关系。<ref name="linjeon02">{{Cite techreport  |first1=Yi |last1=Lin |first2=Yongho |last2=Jeon |title=Random forests and adaptive nearest neighbors |series=Technical Report No. 1055 |year=2002 |institution=University of Wisconsin |url=http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.153.9168}}</ref> 事实证明，这两种算法都可以被看作是所谓的“加权邻居的方案”。这些在数据集<math>\{(x_i, y_i)\}_{i=1}^n</math>上训练的模型通过查看一个点的邻居来计算一个新点{{mvar|x'}}的预测值<math>\hat{y}</math>，并且使用权重函数{{mvar|W}}对这些邻居进行加权:

:<math>\hat{y} = \sum_{i=1}^n W(x_i, x') \, y_i.</math>

其中， <math>W(x_i, x')</math>是第{{mvar|i}}个点在同一棵树中相对于新的数据点{{mvar|x'}}的非负权重。对于任一特定的点{{mvar|x'}}，<math>x_i</math>的权重的和必须为1。权重函数设定如下：

* 对于{{mvar|k}}-NN算法，如果{{mvar|x<sub>i</sub>}}是距离{{mvar|x'}}最近的{{mvar|k}}个点之一，则<math>W(x_i, x') = \frac{1}{k}</math>，否则为0。
* 对于树，如果{{mvar|x<sub>i</sub>}}与{{mvar|x'}}属于同一个包含{{mvar|k'}}个点的叶结点，则<math>W(x_i, x') = \frac{1}{k'}</math>，否则为0。

因为森林平均了{{mvar|m}}棵树的预测，且这些树具有独立的权重函数<math>W_j</math>，故森林的预测值是：

:<math>\hat{y} = \frac{1}{m}\sum_{j=1}^m\sum_{i=1}^n W_{j}(x_i, x') \, y_i = \sum_{i=1}^n\left(\frac{1}{m}\sum_{j=1}^m W_{j}(x_i, x')\right) \, y_i.</math>

上式表明了整个森林也采用了加权的邻居方案，其中的权重是各个树的平均。在这里，{{mvar|x'}}的邻居是那些在任一树中属于同一个叶节点的点<math>x_i</math>。只要<math>x_i</math>与{{mvar|x'}}在某棵树中属于同一个叶节点，<math>x_i</math>就是{{mvar|x'}}的邻居。

==基于随机森林的非监督学习==
作为构建的一部分，随机森林预测器自然会导致观测值之间的不相似性度量。还可以定义未标记数据之间的随机森林差异度量：其思想是构造一个随机森林预测器，将“观测”数据与适当生成的合成数据区分开来。<ref name=breiman2001/><ref>{{cite journal |authors=Shi, T., Horvath, S. |year=2006 |title=Unsupervised Learning with Random Forest Predictors |journal=Journal of Computational and Graphical Statistics |volume=15 |issue=1 |pages=118–138  |doi=10.1198/106186006X94072 |jstor=27594168|citeseerx=10.1.1.698.2365 }}</ref> 观察到的数据是原始的未标记数据，合成数据是从参考分布中提取的。随机森林的不相似性度量之所以吸引人，是因为它能很好地处理混合变量类型，对输入变量的单调变换是不敏感的，而且对异常数据是鲁棒的。由于其固有变量的选择，随机森林不相似性很容易处理大量的半连续变量。

== 學習演算法 ==
根據下列[[演算法|演算法]]而建造每棵樹：
# 用''N''來表示訓練用例（样本）的個數，''M''表示特征数目。
# 输入特征数目''m''，用于确定决策树上一个节点的决策结果；其中''m''應远小於''M''。
# 從''N''個訓練用例（样本）中以有放回抽样的方式，取樣''N''次，形成一个训练集（即bootstrap取樣），並用未抽到的用例（样本）作預測，評估其誤差。
# 對於每一個節點，隨機選擇''m''個特征，决策树上每个节点的决定都是基于这些特征确定的。根據這m個特征，計算其最佳的分裂方式。
# 每棵樹都會完整成長而不會[[剪枝|剪枝]]（Pruning，這有可能在建完一棵正常樹狀分類器後會被採用）。

== 優點 ==
隨機森林的優點有：
* 對於很多種資料，它可以產生高準確度的分類器。
* 它可以處理大量的輸入變數。
* 它可以在決定類別時，評估變數的重要性。
* 在建造森林時，它可以在內部對於一般化後的誤差產生不偏差的估計。
* 它包含一個好方法可以估計遺失的資料，並且，如果有很大一部分的資料遺失，仍可以維持準確度。
* 它提供一個實驗方法，可以去偵測variable interactions。
* 對於不平衡的分類資料集來說，它可以平衡誤差。
* 它計算各例中的親近度，對於[[数据挖掘|数据挖掘]]、偵測[[離群點|離群點]]（outlier）和將資料視覺化非常有用。
* 使用上述。它可被延伸應用在未標記的資料上，這類資料通常是使用[[非監督式學習|非監督式]][[數據聚類|聚類]]。也可偵測偏離者和觀看資料。
* 學習過程是很快速的。

==开源实现==
* [http://www.stat.berkeley.edu/~breiman/RandomForests/cc_software.htm The Original RF] by Breiman and Cutler written in Fortran 77.
* [http://www.alglib.net/dataanalysis/decisionforest.php ALGLIB] contains a modification of the random forest in C#, C++, Pascal, VBA.
* [http://cran.r-project.org/web/packages/party/index.html party] Implementation based on the conditional inference trees in [[R_(programming_language)|R]].
* [http://cran.r-project.org/web/packages/randomForest/index.html randomForest] for classification and regression in [[R_(programming_language)|R]].
* [http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html Python implementation] with examples in [[scikit-learn|scikit-learn]].
* [[Orange_(software)|Orange data mining]] suite includes random forest learner and can visualize the trained forest.
* [http://code.google.com/p/randomforest-matlab Matlab] implementation.
* [http://sqp.upf.edu SQP] software uses random forest algorithm to predict the quality of survey questions, depending on formal and linguistic characteristics of the question.
* [http://weka.sourceforge.net/doc.dev/weka/classifiers/trees/RandomForest.html Weka RandomForest] in Java library and GUI.
* [https://github.com/imbs-hl/ranger ranger] A C++ implementation of random forest for classification, regression, probability and survival. Includes interface for [[R_(programming_language)|R]].


== 外部連結 ==
* [https://web.archive.org/web/20080704141852/http://cm.bell-labs.com/cm/cs/who/tkh/papers/odt.pdf Ho, Tin Kam (1995). "Random Decision Forest". Proc. of the 3rd Int'l Conf. on Document Analysis and Recognition,  Montreal, Canada, August 14-18, 1995, 278-282]（Preceding Work）
* [https://web.archive.org/web/20070930204101/http://cm.bell-labs.com/cm/cs/who/tkh/papers/df.pdf Ho, Tin Kam (1998). "The Random Subspace Method for Constructing Decision Forests".  IEEE Trans. on Pattern Analysis and Machine Intelligence 20 (8), 832-844]（Preceding Work）
* [http://enpub.fulton.asu.edu/hdeng3/MultiICANN2011.pdf Deng, H; Runger, G; Tuv, Eugene (2011). Bias of importance measures for multi-valued attributes and solutions, Proceedings of the 21st International Conference on Artificial Neural Networks (ICANN2011)]{{dead link|date=十一月 2017 |bot=InternetArchiveBot }}
* [http://www.cis.jhu.edu/publications/papers_in_database/GEMAN/shape.pdf Amit, Yali and Geman, Donald (1997) "Shape quantization and recognition with randomized trees". Neural Computation 9, 1545-1588.]（Preceding work）
* [https://web.archive.org/web/20081204092820/http://www.ics.uci.edu/~liang/seminars/win05/papers/wald2002-2.pdf Breiman, Leo "Looking Inside The Black Box". Wald Lecture II]（Lecture）
* [http://www.springerlink.com/content/u0p06167n6173512/fulltext.pdf Breiman, Leo (2001).  "Random Forests".  Machine Learning 45 (1), 5-32]（Original Article）
* [https://web.archive.org/web/20080622230434/http://stat-www.berkeley.edu/users/breiman/RandomForests/cc_home.htm Random Forest classifier description]（Site of Leo Breiman）
* [http://cran.r-project.org/doc/Rnews/Rnews_2002-3.pdf Liaw, Andy & Wiener, Matthew "Classification and Regression by randomForest" R News (2002) Vol. 2/3 p. 18]（Discussion of the use of the random forest package for [[R_programming_language|R]]）
* [http://cm.bell-labs.com/cm/cs/who/tkh/papers/compare.pdf Ho, Tin Kam (2002). "A Data Complexity Analysis of Comparative Advantages of Decision Forest Constructors". Pattern Analysis and Applications 5, p. 102-112]{{dead link|date=2018年2月 |bot=InternetArchiveBot |fix-attempted=yes }}（Comparison of bagging and random subspace method） 

[[Category:分类算法|Category:分类算法]]
[[Category:机器学习|Category:机器学习]]
[[Category:统计学|Category:统计学]]
[[Category:隨機性|Category:隨機性]]
[[Category:决策树|Category:决策树]]