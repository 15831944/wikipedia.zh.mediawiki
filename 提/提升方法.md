'''提升方法'''（Boosting），是一种可以用来减小[[監督式學習|監督式學習]]中[[偏差|偏差]]的[[机器学习|机器学习]]算法。面對的问题是邁可·肯斯（Michael Kearns）提出的：<ref name="Kearns88">Michael Kearns (1988); [http://www.cis.upenn.edu/~mkearns/papers/boostnote.pdf ''Thoughts on Hypothesis Boosting''], Unpublished manuscript (Machine Learning class project, December 1988)</ref>一組“弱学习者”的集合能否生成一个“强学习者”？弱学习者一般是指一个分类器，它的结果只比随机分类好一点点；强学习者指分类器的结果非常接近真值。

==提升算法==
大多数提升算法包括由迭代使用弱学习分類器組成，並將其結果加入一個最終的成强学习分類器。加入的过程中，通常根据它们的分类准确率给予不同的权重。加和弱学习者之后，数据通常会被重新加权，来强化对之前分类错误数据点的分类。

一个经典的提升算法例子是[[AdaBoost|AdaBoost]]。一些最近的例子包括[[LPBoost|LPBoost]]、[[TotalBoost|TotalBoost]]、[[BrownBoost|BrownBoost]]、[[MadaBoost|MadaBoost]]及[[LogitBoost|LogitBoost]]。许多提升方法可以在[[AnyBoost|AnyBoost]]框架下解释为在[[函数空间|函数空间]]利用一个凸的误差函数作[[梯度下降|梯度下降]]。

==批评==
2008年，[[谷歌|谷歌]]的菲利普·隆（Phillip Long）與[[哥倫比亞大學|哥倫比亞大學]]的羅可·A·瑟維迪歐（Rocco A. Servedio）发表论文指出这些方法是有缺陷的：在训练集有错误的标记的情况下，一些提升算法雖會尝试提升这种样本点的正确率，但卻無法产生一个正确率大于1/2的模型。<ref>[http://www.phillong.info/publications/LS10_potential.pdf Philip M. Long, Rocco A. Servedio, "Random Classiﬁcation Noise Defeats All Convex Potential Boosters"] </ref>

==相關條目==
* [[AdaBoost|AdaBoost]]
* [[随机森林|随机森林]]
* [[Logit模型|Logit模型]]
* [[人工神经网络|人工神经网络]]
* [[支持向量機|支持向量機]]
* [[机器学习|机器学习]]

==实现==
* [[Orange_(software)|Orange]], a free data mining software suite, module [http://orange.biolab.si/docs/latest/reference/rst/Orange.ensemble Orange.ensemble]{{dead link|date=2018年3月 |bot=InternetArchiveBot |fix-attempted=yes }}
* [[Weka_(machine_learning)|Weka]] is a machine learning set of tools that offers variate implementations of boosting algorithms like AdaBoost and LogitBoost
* R package [http://cran.r-project.org/web/packages/gbm/index.html GBM] (Generalized Boosted Regression Models) implements extensions to Freund and Schapire's AdaBoost algorithm and Friedman's gradient boosting machine.
* jboost; AdaBoost, LogitBoost, RobustBoost, Boostexter and alternating decision trees

== 参考文献 ==
===腳註===
{{Reflist}}

===其他參考資料===
* Yoav Freund and Robert E. Schapire (1997); [http://www.cse.ucsd.edu/~yfreund/papers/adaboost.pdf ''A Decision-Theoretic Generalization of On-line Learning and an Application to Boosting''], Journal of Computer and System Sciences, 55(1):119-139
* Robert E. Schapire and Yoram Singer (1999); [http://citeseer.ist.psu.edu/schapire99improved.html ''Improved Boosting Algorithms Using Confidence-Rated Predictors''], Machine Learning, 37(3):297-336

== 外部链接 ==
* Robert E. Schapire (2003); [http://www.cs.princeton.edu/courses/archive/spr08/cos424/readings/Schapire2003.pdf ''The Boosting Approach to Machine Learning: An Overview''], MSRI (Mathematical Sciences Research Institute) Workshop on Nonlinear Estimation and Classification
* [https://web.archive.org/web/20140419015925/http://www.cs.princeton.edu/~schapire/boost.html An up-to-date collection of papers on boosting]

[[Category:分类算法|Category:分类算法]]