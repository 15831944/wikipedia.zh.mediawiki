{{about|机器学习中的决策树|决策分析中的术语|决策树}}
{{机器学习导航栏}}

[[统计学|统计学]]，[[数据挖掘|数据挖掘]]和[[机器学习|机器学习]]中的'''决策树训练'''，使用[[决策树|决策树]]作为[[Predictive_modelling|预测模型]]来预测样本的类标。这种决策树也称作'''分类树'''或'''回归树'''。在这些树的结构里， [[leaf_node|叶子]]节点给出类标而内部节点代表某个属性。 

在决策分析中，一棵决策树可以明确地表达决策的过程。在[[数据挖掘|数据挖掘]]中，一棵决策树表达的是数据而不是决策。本页的决策树是[[数据挖掘|数据挖掘]]中的决策树。

==推广==
[[File:CART_tree_titanic_survivors.png|frame]]上乘客生存的决策树 ("sibsp"指甲板上的兄妹和配偶)。每个决策叶下标识该类乘客的生存几率和观察到的比率]]
在数据挖掘中决策树训练是一个常用的方法。目标是创建一个模型来预测样本的目标值。例如右图。每个 [[内部节点|内部节点]] 对应于一个输入属性，子节点代表父节点的属性的可能取值。每个叶子节点代表输入属性得到的可能输出值。 

一棵树的训练过程为：根据一个指标，分裂训练集为几个子集。这个过程不断的在产生的子集里重复递归进行，即[[递归分割|递归分割]]。当一个训练子集的类标都相同时 [[递归|递归]]停止。这种''决策树的自顶向下归纳'' (TDITD) <ref name="Quinlan86">Quinlan, J. R., (1986). Induction of Decision Trees. Machine Learning 1: 81-106, Kluwer Academic Publishers</ref> 是 [[贪心算法|贪心算法]]的一种, 也是目前为止最为常用的一种训练方法，但不是唯一的方法。


数据以如下方式表示:    
    
:<math>(\textbf{x},Y) = (x_1, x_2, x_3, ..., x_k, Y)</math>

其中Y是目标值，向量'''x'''由这些属性构成, x<sub>1</sub>, x<sub>2</sub>, x<sub>3</sub> 等等，用来得到目标值。

==决策树的类型==
在数据挖掘中，决策树主要有两种类型:

*'''分类树''' 的输出是样本的类标。
*'''回归树''' 的输出是一个实数 (例如房子的价格，病人待在医院的时间等)。

术语'''分类和回归树 (CART)''' 包含了上述两种决策树, 最先由[[Leo_Breiman|Breiman]] 等提出.<ref name="bfos">{{Cite book
|last=Breiman
|first=Leo
|coauthors=Friedman, J. H., Olshen, R. A., & Stone, C. J.
|title=Classification and regression trees
|year=1984
|publisher=Wadsworth & Brooks/Cole Advanced Books & Software
|location=Monterey, CA
|isbn=978-0-412-04841-8
|postscript=.
}}</ref> 分类树和回归树有些共同点和不同点—例如处理在何处分裂的问题。<ref name="bfos"/>

有些''集成''的方法产生多棵树：

*'''装袋算法（Bagging）''', 是一个早期的集成方法，用有放回抽样法来训练多棵决策树，最终结果用投票法产生。<ref>Breiman, L. (1996). Bagging Predictors. "Machine Learning, 24": pp. 123-140.</ref>
*'''[[随机森林|随机森林（Random Forest）]]''' 使用多棵决策树来改进分类性能。
*'''[[梯度提升树|提升树（Boosting Tree）]]''' 可以用来做回归分析和分类决策<ref>Friedman, J. H. (1999). ''Stochastic gradient boosting.'' Stanford University.</ref><ref>Hastie, T., Tibshirani, R., Friedman, J. H. (2001). ''The elements of statistical learning : Data mining, inference, and prediction.'' New York: Springer Verlag.</ref>
*旋转森林（Rotation forest） – 每棵树的训练首先使用主元分析法 (PCA)。<ref>Rodriguez, J.J. and Kuncheva, L.I. and Alonso, C.J. (2006), Rotation forest: A new classifier ensemble method, IEEE Transactions on Pattern Analysis and Machine Intelligence, 28(10):1619-1630.</ref>

还有其他很多决策树算法，常见的有:

* [[ID3算法|ID3算法]]
* [[C4.5算法|C4.5算法]]
* '''CHi-squared Automatic Interaction Detector ([[CHAID|CHAID]])'''. 在生成树的过程中用多层分裂.<ref>{{Cite journal | doi = 10.2307/2986296 | last1 = Kass | first1 = G. V. | year = 1980 | title = An exploratory technique for investigating large quantities of categorical data | jstor = 2986296| journal = Applied Statistics | volume = 29 | issue = 2| pages = 119–127 }}</ref>
* [[Multivariate_adaptive_regression_splines|MARS]]:可以更好的处理数值型数据。

==模型表达式==
构建决策树时通常采用自上而下的方法，在每一步选择一个最好的属性来分裂。<ref>{{cite journal |last1=Rokach |first1=L. |last2=Maimon |first2=O. |title=Top-down induction of decision trees classifiers-a survey |journal=IEEE Transactions on Systems, Man, and Cybernetics, Part C |volume=35 |pages=476–487 |year=2005 |doi=10.1109/TSMCC.2004.843247 |issue=4}}</ref>  "最好" 的定义是使得子节点中的训练集尽量的纯。不同的算法使用不同的指标来定义"最好"。本部分介绍一些最常见的指标。

===基尼不纯度指标===
{{Distinguish|基尼系数}}
在CART算法中, 基尼不纯度表示一个随机选中的样本在子集中被分错的可能性。基尼不纯度为这个样本被选中的概率乘以它被分错的概率。当一个节点中所有样本都是一个类时，基尼不纯度为零。                    

假设y的可能取值为<math>J</math>个类别，另<math>i \in \{1, 2, ...,J\}</math>，<math>p_i</math>表示被标定为第<math>i</math>类的概率，则基尼不纯度的计算为：              

:<math>\operatorname{I}_{G}(p) = \sum_{i=1}^J p_i \sum_{k\neq i} p_k 
 = \sum_{i=1}^{J} p_i (1-p_i) 
 = \sum_{i=1}^{J} (p_i - {p_i}^2) 
 = \sum_{i=1}^J p_i - \sum_{i=1}^{J} {p_i}^2
 = 1 - \sum^{J}_{i=1} {p_i}^{2} </math>

===信息增益===
[[ID3_algorithm|ID3]], [[C4.5_algorithm|C4.5]] 和 C5.0 决策树的生成使用[[信息增益|信息增益]]。信息增益 是基于[[信息论|信息论]]中[[信息熵|信息熵]]与[[自信息|自信息]]理论.

[[信息熵|信息熵]]定义为：

:<math>\Eta(T) = \operatorname{I}_{E}\left(p_1, p_2, ..., p_J\right) 
 = - \sum^{J}_{i=1} {p_i \log_2 p_i}</math>
其中<math>p_1, p_2, ...</math>加和为1，表示当前节点中各个类别的百分比。<ref name="Witten 2011 102–103">{{Cite book|title=Data Mining|last=Witten|first=Ian|last2=Frank|first2=Eibe|last3=Hall|first3=Mark|publisher=Morgan Kaufmann|year=2011|isbn=978-0-12-374856-0|location=Burlington, MA|pages=102–103|quote=|via=}}</ref>

:<math display="block"> \overbrace{IG(T,a)}^\text{Information Gain}
= \overbrace{\Eta(T)}^\text{Entropy (parent)} 
- \overbrace{\Eta(T|a)}^\text{Weighted Sum of Entropy (Children)} </math><math>=-\sum_{i=1}^{J}p_i\log_2{p_i} - \sum_{a}{p(a)\sum_{i=1}^{J}-\Pr(i|a)\log_2{\Pr(i|a)}}</math>

例如，数据集有4个属性：''outlook'' (sunny, overcast, rainy), ''temperature'' (hot, mild, cool), ''humidity'' (high, normal), and ''windy'' (true, false), 目标值''play''（yes, no）, 总共14个数据点。为建造决策树，需要比较4棵决策树的信息增益，每棵决策树用一种属性做划分。信息增益最高的划分作为第一次划分，并在每个子节点继续此过程，直至其信息增益为0。

使用属性''windy''做划分时，产生2个子节点：''windy''值为真与为假。当前数据集，6个数据点的''windy''值为真，其中3个点的''play''值为真，3个点的''play''值为假；其余8个数据点的''windy''为假，其中6个点的''play''值为真，2个点的''play''值为假。 ''windy''=true的子节点的信息熵计算为：

:<math> I_{E}([3,3]) = -\frac 3 6\log^{}_2 \frac 3 6 - \frac 3 6\log^{}_2 \frac 3 6 = -\frac 1 2\log^{}_2 \frac 1 2 - \frac 1 2\log^{}_2 \frac 1 2 = 1 </math>

''windy''=false的子节点的信息熵计算为：

:<math> I_{E}([6,2]) = -\frac 6 8\log^{}_2 \frac 6 8 - \frac 2 8\log^{}_2 \frac 2 8 = -\frac 3 4\log^{}_2 \frac 3 4 - \frac 1 4\log^{}_2 \frac 1 4 = 0.8112781 </math>

这个划分（使用属性''windy''）的信息熵是两个子节点信息熵的加权和：

:<math>I_{E}([3,3],[6,2]) = I_{E}(\text{windy or not}) = \frac 6 {14} \cdot 1 + \frac 8 {14} \cdot 0.8112781 = 0.8921589 </math>

为计算使用属性''windy''的信息增益，必须先计算出最初（未划分）的数据集的信息熵，数据集的''play''有9个yes与5个no：

:<math> I_{E}([9,5]) = -\frac 9 {14}\log^{}_2 \frac 9 {14} - \frac 5 {14}\log_2 \frac 5 {14} = 0.940286 </math>

使用属性''windy''的信息增益是：

:<math> IG(\text{windy}) = I_{E}([9,5]) - I_{E}([3,3],[6,2]) = 0.940286 - 0.8921589 = 0.0481271 </math>

==决策树的优点==
与其他的数据挖掘算法相比，决策树有许多优点:
* '''易于理解和解释''' 人们很容易理解决策树的意义。
* '''只需很少的数据准备''' 其他技术往往需要数据归一化。
* '''即可以处理数值型数据也可以处理[[Categorical_variable|类别型]] 数据'''。其他技术往往只能处理一种数据类型。例如关联规则只能处理类别型的而神经网络只能处理数值型的数据。
* '''使用[[white_box_(software_engineering)|白箱]] 模型.''' 输出结果容易通过模型的结构来解释。而神经网络是黑箱模型，很难解释输出的结果。
* '''可以通过测试集来验证模型的性能''' 。可以考虑模型的稳定性。
* '''[[Robust_statistics|強健控制]].''' 对噪声处理有好的強健性。
* '''可以很好的处理大规模数据''' 。

==缺点==
* 训练一棵最优的决策树是一个完全NP问题。<ref>{{Cite journal | doi = 10.1016/0020-0190(76)90095-8 | last1 = Hyafil | first1 = Laurent | last2 = Rivest | first2 = RL | year = 1976 | title = Constructing Optimal Binary Decision Trees is NP-complete | url = | journal = Information Processing Letters | volume = 5 | issue = 1| pages = 15–17 }}</ref><ref>Murthy S. (1998). Automatic construction of decision trees from data: A multidisciplinary survey. ''Data Mining and Knowledge Discovery''</ref> 因此, 实际应用时决策树的训练采用启发式搜索算法例如 [[贪心算法|贪心算法]] 来达到局部最优。这样的算法没办法得到最优的决策树。
* 决策树创建的过度复杂会导致无法很好的预测训练集之外的数据。这称作[[过拟合|过拟合]].<ref>{{cite book|url=http://link.springer.com/10.1007/978-1-84628-766-4|title=Principles of Data Mining | publisher=SpringerLink|language=en-gb|accessdate=2018-04-02|doi=10.1007/978-1-84628-766-4}}</ref>  [[决策树剪枝|剪枝]]机制可以避免这种问题。
*  有些问题决策树没办法很好的解决,例如 [[异或|异或]]问题。解决这种问题的时候，决策树会变得过大。  要解决这种问题，只能改变问题的领域<ref>{{cite book|url=http://link.springer.com/10.1007/b13700|title=Inductive Logic Programming | publisher=SpringerLink|language=en-gb|accessdate=2018-04-02|doi=10.1007/b13700}}</ref> 或者使用其他更为耗时的学习算法 (例如[[统计关系学习|统计关系学习]] 或者 [[归纳逻辑编程|归纳逻辑编程]]).

* 对那些有类别型属性的数据, [[信息增益|信息增益]] 会有一定的偏置。<ref>{{cite conference|author=Deng,H.|coauthors=Runger, G.; Tuv, E.|url=https://pdfs.semanticscholar.org/1bc3/6ce500a6e195c6434986e4a68c958efd35d6.pdf|title=Bias of importance measures for multi-valued attributes and solutions|conference=Proceedings of the 21st International Conference on Artificial Neural Networks (ICANN)|year=2011|pages=293–300|deadurl=yes|archiveurl=https://web.archive.org/web/20180510185104/https://pdfs.semanticscholar.org/1bc3/6ce500a6e195c6434986e4a68c958efd35d6.pdf|archivedate=2018-05-10}}</ref>

==延伸==
===决策图===
在决策树中, 从根节点到叶节点的路径采用汇合或''与''。
而在决策图中, 可以采用 [[最小消息长度|最小消息长度]] (MML)来汇合两条或多条路径。<ref>http://citeseer.ist.psu.edu/oliver93decision.html</ref>  

===用演化算法来搜索===
演化算法可以用来避免局部最优的问题<ref>Papagelis A., Kalles D.(2001). Breeding Decision Trees Using Evolutionary Techniques, Proceedings of the Eighteenth International Conference on Machine Learning, p.393-400, June 28-July 01, 2001</ref><ref>Barros, Rodrigo C., Basgalupp, M. P., Carvalho, A. C. P. L. F., Freitas, Alex A. (2011). [http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5928432 A Survey of Evolutionary Algorithms for Decision-Tree Induction]. IEEE Transactions on Systems, Man and Cybernetics, Part C: Applications and Reviews, vol. 42, n. 3, p. 291-312, May 2012.</ref>

== 参见 ==
* [[决策树剪枝|决策树剪枝]]
* [[二元决策图|二元决策图]]
* [[Predictive_analytics#Classification_and_regression_trees|CART]]
* [[ID3算法|ID3算法]]
* [[C4.5算法|C4.5算法]]

== 参考资料 ==
{{Reflist}}

== 外部链接 ==
*[https://web.archive.org/web/20110807035706/http://onlamp.com/lpt/a/6464 Building Decision Trees in Python] From O'Reilly.
*[http://www.oreillynet.com/mac/blog/2007/06/an_addendum_to_building_decisi.html An Addendum to "Building Decision Trees in Python"] From O'Reilly.
*[https://web.archive.org/web/20110728045409/http://www.aaai.org/aitopics/html/trees.html Decision Trees page at aaai.org], a page with commented links.
*[http://ai4r.rubyforge.org/index.html Decision tree implementation in Ruby (AI4R)]{{dead link|date=2017年11月 |bot=InternetArchiveBot |fix-attempted=yes }}
*[http://liuzhiqiangruc.iteye.com/blog/1601922 Building Decision Tree In Bash]

{{DEFAULTSORT:Decision Tree Learning}}

[[Category:决策树|Category:决策树]]
[[Category:分類演算法|Category:分類演算法]]