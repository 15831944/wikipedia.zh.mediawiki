{{机器学习导航栏}}
{{split}}

'''递归神经网络'''（'''RNN'''）是神經網絡的一種。单纯的RNN因为无法处理随着递归，权重指数级爆炸或梯度消失的问题（Vanishing gradient problem），难以捕捉长期时间关联；而结合不同的'''[[LSTM|LSTM]]'''可以很好解决这个问题。<ref>[http://arxiv.org/pdf/1402.3511v1.pdf 时钟结构的RNN]，2014年2月。</ref><ref>[http://jmlr.org/proceedings/papers/v37/jozefowicz15.pdf 递归网络结构的经验之谈]，谷歌论文，2015年。</ref>

时间递归神经网络可以描述动态时间行为，因为和[[前馈神经网络|前馈神经网络]]（feedforward neural network）接受较特定结构的输入不同，RNN将状态在自身网络中循环传递，因此可以接受更广泛的[[时间序列|时间序列]]结构输入。[[手写识别|手写识别]]是最早成功利用RNN的研究结果。<ref>A. Graves, M. Liwicki, S. Fernandez, R. Bertolami, H. Bunke, J. Schmidhuber. A Novel Connectionist System for Improved Unconstrained Handwriting Recognition. IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 31, no. 5, 2009.</ref>

==循环神经网络（Recurrent）==

===编码器===

递归神经网络将输入序列<math>\vec{x}</math>编码为一个固定长度的隐藏状态<math>\vec{h}</math>，这里有（用[[自然语言处理|自然语言处理]]作为例子）：
*  <math>\vec{x} = (x_t, ..., x_1)</math> 是输入序列，比如编码为数字的一系列词语，整个序列就是完整的句子。
* <math>\vec{h_t} = f( x_t, \vec{h_{t-1}} )</math>是随时间更新的隐藏状态。当新的词语输入到方程中，之前的状态<math>\vec{h_{t-1}}</math>就转换为和当前输入<math>x_t</math>相关的<math>\vec{h_{t}}</math>，距离当前时间越长，越早输入的序列，在更新后的状态中所占权重越小，从而表现出时间相关性。<ref name=phrase_RNN>[http://arxiv.org/abs/1406.1078 Cho, K., van et al (2014a). 统计机器翻译方法：使用RNN进行短语编码解码。]</ref>

其中，计算隐藏状态的方程<math>f(x, h)</math>是一个[[非线性|非线性方程]]，可以是简单的[[逻辑函数|Logistic方程]]（tanh），也可以是复杂的[[LSTM|LSTM]]单元（Long-Short Term Memory）。<ref name=phrase_RNN></ref> <ref name=LSTM>[http://dl.acm.org/citation.cfm?id=1246450 经典模型Long-Short Term Memory的原论文，发表于1997年。]</ref> 而有了隐藏状态序列，就可以对下一个出现的词语进行预测：
* <math>p(y_{t}) = p( y_{t} \, | \, y_{t-1},..., y_{1} )</math>，其中<math>y_t</math>是第t个位置上的输出，它的概率基于之前输出的所有词语。
* 以上概率可以通过隐藏状态来计算：<math>p(y_t) = g( y_{t-1}, \vec{h_t}, \vec{c} )</math>，<math>\vec{c}</math>是所有隐藏状态的编码，总含了所有隐藏状态，比如可以是简单的最终隐藏状态<math>\vec{h_t}</math>，也可以是[[非线性|非线性方程]]的输出<math>f( h_{t}, ..., h_1 )</math>。因为隐藏状态t就编码了第t个输入前全部的输入信息，<math>y_t</math>也迭代式地隐含了之前的全部输出信息，所以这个概率计算方法是合理的。

这里的非线性方程<math>g( y, h, c )</math>可以是一个复杂的[[前馈神经网络|前馈神经网络]]，也可以是简单的非线性方程（但有可能因此无法适应复杂的条件而得不到任何有用结果）。给出的概率可以用[[监督学习|监督学习]]的方法优化内部参数来给出翻译，也可以训练后用来给可能的备选词语，用计算其第j个备选词<math>y_{t, j}</math>出现在下一位置的概率，给它们排序。排序后用于其它翻译系统，可以提升翻译质量。

===解码器===

更复杂的情况下复发神经网络还可以结合编码器作为[[解码器|解码器]]（Decoder），用于将编码后（Encoded）的信息解码为人类可识别的信息。也就是上述例子中的<math>y_t = f( y_{t-1}, h_t, c )</math>过程，当中非线性模型<math>f</math>就是作为输出的复发神经网络。只是在解码过程中，隐藏状态因为是解码器的参数，所以为了发挥时间序列的特性，需要对<math>h_t'</math>继续进行迭代：
* <math>h_t' = g(h_{t-1}, y_{t-1}, c)</math>，<math>\vec{c}</math>是解码器传递给编码器的参数，是解码器中状态的summary。<math>h_t'</math>是解码器的隐藏状态。<math>y_t</math>是第t个输出。
* 当输入仍为<math>\vec{x} = (x_t, ..., x_1)</math>，输出是<math>\vec{y} = (y_t, ..., y_1)</math>，最大化[[条件概率|条件概率]]<math>P(\vec{y} \, | \, \vec{x})</math>后就是最好的翻译结果。

===双向读取===

用两个复发神经网络双向读取一个序列可以使人工智能获得“注意力”。简单的做法是将一个句子分别从两个方向编码为两个隐藏状态，然后将两个<math>\vec{h}</math>拼接在一起作为隐藏状态。<ref>[http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=650093 Bidirectional RNN, (1997)]</ref> <ref name=rnn_alignment>[http://arxiv.org/abs/1409.0473 Cho. k. et al 使用RNN和对齐模型的短语统计机器翻译方法。(2014.a.)]</ref>这种方法能提高模型表现的原因之一可能是因为不同方向的读取在输入和输出之间创造了更多短期依赖关系，从而被RNN中的LSTM单元（及其变体）捕捉，例如在实验中发现颠倒输入序列的顺序（但不改变输出的顺序）可以意外达到提高表现的效果。<ref>[http://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf 神经网络学习：序列到序列]，作者Sutskever，2014年。</ref>

==结构递归神经网络（Recursive）==

结构递归神经网络是一类用结构递归的方式构建的网络，比如说[[递归自编码机|递归自编码机]]（Recursive Autoencoder），在自然语言处理的神经网络分析方法中用于解析语句。<ref name=rae_sentiment>[http://dl.acm.org/citation.cfm?id=2145450 半监督RAE方法：预测文本情感分布。]</ref> <ref name=rae>[http://jan.stanford.edu/pubs/emnlp2013_ZouSocherCerManning.pdf Bilingual Word Embeddings for Phrase-Based Machine Translation. (2014).]</ref>

== 参考 ==
{{Reflist|2}}

== 外部連結 ==
* [http://arxiv.org/abs/1506.00019 A Critical Review of Recurrent Neural Networks for Sequence Learning]

[[Category:人工智能|Category:人工智能]]
[[Category:人工神经网络|Category:人工神经网络]]