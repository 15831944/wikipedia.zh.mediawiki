'''约翰逊-林登斯特劳斯定理(Johnson–Lindenstrauss theorem)'''，又称'''约翰逊-林登斯特劳斯引理(Johnson–Lindenstrauss lemma)'''，是由{{tsl|en|William_B._Johnson_(mathematician)|William Johnson}}和{{tsl|en|Joram_Lindenstrauss|Joram Lindenstrauss}}于1984年提出的一个关于[[降维|降维]]的著名定理{{r|JL_paper}}，在现代[[机器学习|机器学习]]，尤其是[[压缩感知|压缩感知]]、[[降维|降维]]、{{tsl|en|Nonlinear_dimensionality_reduction#Manifold_learning_algorithms||形状分析}}和{{tsl|en|Density_estimation|分布学习}}等领域中有很重要的应用{{r|slides_JL-SVM-Kernels}}{{r|JL-application_1}}{{r|JL-application_2}}{{r|JL-application_3}}。

这个定理告诉我们，一个高维空间中的点集，可以被'''线性'''地[[嵌入_(数学)|镶嵌]]到低维空间中，同时其空间结构只遭受比较小的形变{{r|slides_JL-intro}}。约翰逊-林登斯特劳斯定理的证明，还说明了如何用{{tsl|en|Random_projection|随机投影法}}明确地求出这个变换，所用的算法只需要[[RP_(复杂度)|随机多项式时间]]{{r|JL_proof}}。当然，降维不是免费的：如果要求形变很少的话，{{tsl|en|trade-off|代价}}是被嵌入的低维空间维数不能很低；反之亦然，如果要求将点集嵌入很低维的空间，那么就不能很好地控制结构形变的程度。

因为能将维数下降到样本量的对数阶，更兼所用的变换是'''线性的'''、'''显式的'''还可以被'''快速计算'''，约翰逊-林登斯特劳斯定理是一个力度非常强的结论。

==定理陈述==
对任何给定的<math>\epsilon\in(0,1)</math>以及<math>N</math>维[[欧几里德空间|欧几里德空间]]中的<math>m</math>个点<math>\{x_1,\ldots,x_m\}</math>，对于任意满足条件<math>n>(\epsilon^2/2-\epsilon^3/3)^{-1}\log m</math>的正整数<math>n</math>，存在一个'''线性'''映射<math>f:\mathbb{R}^N\to\mathbb{R}^n</math>，将这<math>m</math>个点，从<math>\mathbb{R}^N</math>(维数可能很高的空间)中映射到<math>\mathbb{R}^n</math>(低维空间)中，同时“基本上”保持了点集成员两两之间的距离，即：对于任意两个点<math>(x_i,x_j):1\leq i<j\leq m</math>，都有
: <math>(1-\epsilon)\|x_i-x_j\|_2^2\leq \|f(x_i)-f(x_j)\|_2^2\leq(1+\epsilon)\|x_i-x_j\|_2^2</math>
更进一步地，这个线性映射<math>f</math>还可以在[[RP_(复杂度)|随机多项式时间]]内求出{{r|JL_proof}}。

===直观理解===

约翰逊-林登斯特劳斯定理揭示了一些关于降维映射深刻事实，其中一些还是违反简单直觉的{{r|JL-understanding}}。因此，要想直观地理解这个定理，对初学者来说，可能比从数学式子上读懂证明还要难(反而此定理的证明只用到了比较简单的关于投影的{{tsl|en|Concentration of measure|随机误差不等式}}{{r|RV-book}})。举例来说，定理的结论表明，度量形变程度的误差参数<math>\epsilon</math>以及低维空间的维数<math>n</math>这两个度量降维水准的关键量，均与原始数据所在的空间维数<math>N</math>或者原始的<math>m</math>个点具体为何种空间结构无关，这是很不平凡的{{r|RV-book}}。

===最优性===
约翰逊-林登斯特劳斯定理是'''不能'''被本质性地改进的{{r|Alon_2003}}，即：给定任意正整数<math>m</math>和误差参数<math>\epsilon</math>，存在某个<math>N</math>以及<math>\mathbb{R}^N</math>中的<math>m</math>个点，这个点集“难以降维”——也就是说，对任何一个满足“基本保持点距”要求(即：<math>(1-\epsilon)\|x_i-x_j\|_2^2\leq \|f(x_i)-f(x_j)\|_2^2\leq(1+\epsilon)\|x_i-x_j\|_2^2</math>要对任意<math>(i,j):1\leq i<j\leq n</math>成立)的线性映射<math>f:\mathbb{R}^N\to\mathbb{R}^n</math>，它用来镶嵌高维数据的那个低维空间(即<math>\mathbb{R}^n</math>)，至少必须具有
: <math>n=\Omega\left(\frac{\log m}{\epsilon^2}\right)</math>
这么多的维数{{r|Optimality-2017}}。

===证明提要===

定理可以用高年级本科生容易理解的方法证明{{r|JL_proof}}{{r|JL_proof2}}，其思路是证明如下事实：多次独立地重复进行随机投影的试验，每次试验中随机抽取的投影<math>f_0:\mathbb{R}^N\to\mathbb{R}^n</math>都有至少<math>1/n</math>的概率符合定理中对映射<math>f</math>全部的要求(显然，验证任何一个<math>f_0</math>是否符合这些要求只需<math>O(n^2)</math>时间)，因此只要重复该独立实验<math>\omega(n)</math>次就能以逼近100%的概率产生至少一个符合要求的映射<math>f</math>。

==参考文献==
{{reflist|refs=

<ref name=JL_paper>{{cite journal |author1=Johnson, William B |author2=Lindenstrauss, Joram |title=Extensions of Lipschitz mappings into a Hilbert space |journal=Contemporary mathematics |date=1984 |volume=26 |issue=1 |page=189-206}}</ref>

<ref name=JL_proof>{{cite journal |author1=Dasgupta, Sanjoy |author2=Gupta, Anupam |title=An elementary proof of a theorem of Johnson and Lindenstrauss |journal=Random Structures & Algorithms |date=2003 |volume=22 |issue=1 |page=60--65}}</ref>

<ref name="JL_proof2">{{cite web |author1=Michael Mahoney |title=CS369M: Algorithms for Modern Massive Data Set Analysis Lecture 1: The Johnson-Lindenstrauss Lemma |url=https://cs.stanford.edu/people/mmahoney/cs369m/Lectures/lecture1.pdf |website=cs.stanford.edu |accessdate=2018-11-13}}</ref>

<ref name=Alon_2003>{{cite journal |author1=Alon, Noga |title=Problems and results in extremal combinatorics—I |journal=Discrete Mathematics |date=2003 |volume=273 |issue=1-3 |page=31--53}}</ref>

<ref name="slides_JL-SVM-Kernels">{{cite web |author1=Devdatt Dubhashi |title=Johnson-Lindenstrauss, Concentration and applications to Support Vector Machines and Kernels |url=https://simons.berkeley.edu/sites/default/files/docs/721/dubhashislides.pdf |website=simons.berkeley.edu |accessdate=2018-11-13}}</ref>

<ref name="slides_JL-intro">{{cite web |author1=Paul Beame |title=CSE 522: Sublinear (and Streaming) Algorithms Spring 2014 Lecture 10: Johnson-Lindenstrauss Lemmas |url=https://courses.cs.washington.edu/courses/cse522/14sp/lectures/lect10.pdf |website=courses.cs.washington.edu |accessdate=2018-11-13}}</ref>

<ref name="JL-understanding">{{cite web |author1=Sariel Har-Peled |title=The Johnson-Lindenstrauss Lemma |url=https://sarielhp.org/teach/13/b_574_rand_alg/lec/19_jl.pdf |website=sarielhp.org |accessdate=2018-11-13}}</ref>

<ref name="JL-application_1">{{cite web |author1=Suresh Venkat |title=When to use the Johnson-Lindenstrauss lemma over SVD? |url=https://cstheory.stackexchange.com/questions/21487/when-to-use-the-johnson-lindenstrauss-lemma-over-svd |website=cstheory.stackexchange.com |accessdate=2018-11-13}}</ref>

<ref name="JL-application_2">{{cite journal |author1=Krahmer, Felix |author2=Ward, Rachel |title=New and improved Johnson--Lindenstrauss embeddings via the restricted isometry property |journal=SIAM Journal on Mathematical Analysis |date=2011 |volume=43 |issue=3 |page=1269--1281}}</ref>

<ref name="JL-application_3">{{cite journal |author1=Akselrod-Ballin, Ayelet |coauthors=Bock, Davi and Reid, R Clay and Warfield, Simon K |title=Accelerating image registration with the Johnson--Lindenstrauss lemma: application to imaging 3-D neural ultrastructure with electron microscopy |journal=IEEE transactions on medical imaging |date=2011 |volume=30 |issue=7 |page=1427--1438}}</ref>

<ref name="RV-book">{{cite book |author1=Roman Vershynin |title=High-dimensional probability: An introduction with applications in data science |date=2018-08-01 |publisher=Cambridge University Press |isbn=9781108415194 |accessdate=2018-11-13}}</ref>

<ref name="Optimality-2017">{{cite conference |author=Larsen, Kasper Green |coauthors=Nelson, Jelani |title=Optimality of the johnson-lindenstrauss lemma |conference=2017 IEEE 58th Annual Symposium on Foundations of Computer Science (FOCS) |year=2017 |publisher=IEEE |pages=633--638}}</ref>

}}

[[Category:数学定理|J]]
[[Category:机器学习|Category:机器学习]]