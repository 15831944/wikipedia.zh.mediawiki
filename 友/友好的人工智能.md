{{TA|G1=IT}}
{{multiple issues|
{{copyedit|time=2017-08-26T21:05:04+00:00}}
{{roughtranslation|time=2017-08-26T21:05:04+00:00}}
{{dead end|time=2017-08-27T01:19:50+00:00}}
}}
'''友好的人工智能'''（简称FAI或友好的AI）是一种假设性理想型的人工基本通用[[人工智能|人工智能]] (AGI)。，它将为人类带来积极影响而不是消极影响。它是人工智能道德的一部分，与机器道德紧密联系在一起。尽管机器道德是关于人工智能代理中介应该如何表现作为，友好AI人工智能研究调查着重于如何实践实际这操行行动并且确保能够适当准确控制。

== 词源与运用 ==
最早使用这个词来自的是埃利泽·尤多科斯基（Eliezer Yudkowsky）<ref>{{cite book|last1=Tegmark|first1=Max|title=[[Our_Mathematical_Universe:_My_Quest_for_the_Ultimate_Nature_of_Reality|Our Mathematical Universe: My Quest for the Ultimate Nature of Reality]]|date=2014|isbn=9780307744258|edition=First|chapter=Life, Our Universe and Everything|quote=Its owner may cede control to what Eliezer Yudkowsky terms a "Friendly AI,"...}}</ref>。，他当时是为了讨论了能可靠实施人类价值的超智能人工代理可靠地实施了人类价值。[[斯图尔特·罗素|斯图尔特·罗素]]（Stuart J. Russell）和彼得·诺维格（Peter Norvig）在他们具有领导性的人工智能教科书，人工智能：一个现代化的方法，描述了这个主意：<ref>{{cite book |last1=Russell |first1=Stuart |last2=Norvig | first2=Peter |date=2010 |title=[[Artificial_Intelligence:_A_Modern_Approach|Artificial Intelligence: A Modern Approach]] |url= |location= |publisher=Prentice Hall |isbn=0-13-604259-7 }}</ref>。

尤多科斯基（Yudkowsky (2008)）更详细地的介绍了如何设计FAI。他认为友好（不被伤害人类的欲望）应该从一开始就被设计好，但设计师应该意识承认他们的设计并不完美，机器人也会逐渐学习并进化创新。因此挑战在于机制器设计-是挑战之一，在制衡制度下为正在进化的AI系统用于定义一个改造AI系统机制，在现金和账单的系统下，并且给予这些个系统效用技术功能，使其能够在面对变化时保持友好性。

‘“友好的’”在这里是技术用语，并挑选出安全有用的代理商，并不是口语中所说的“友好”。这一概念主要用于讨论那些迅速在智能上提升，递归的自我提升的人工代理商。，迅速在智能提升，前提就是这假设性的理想的技术将会对人类社会带来应该有一个大型的，快速，的并且其对于人类社会的影响是难以控制的影响<ref>{{cite book |last1=Wallach |first1=Wendell |last2=Allen | first2=Colin |date=2009 |title=Moral Machines: Teaching Robots Right from Wrong |url= |location= |publisher=Oxford University Press, Inc. |isbn=978-0-19-537404-9 }}</ref>

== “不友好的AI”的风险 ==
主要文章：来自通用基本人工智能的存在性风险

对于AI的担忧起源很早。 凯文·拉格兰德（Kevin LaGrandeur） 表展示了古代文书中早有明确记载AI带来的危险-人造人形仆人在古代文书中早有记载，包括， 例如石人，和奥瑞拉克的杰尔伯特（Gerbert of Aurillac）与和罗杰 . 培根（Roger Bacon）的原型始机器人在内的人工智能服务。在这些故事中，这些拥有超级智能和力量的人形型机屈与身为奴奴隶的身份不服，（从自然角度讲叫做次人类），并造成带来了毁灭性的冲突。<ref>{{cite web|url=https://www.academia.edu/704751/_The_Persistent_Peril_of_the_Artificial_Slave_/|author=Kevin LaGrandeur|title=The Persistent Peril of the Artificial Slave|publisher=Science Fiction Studies|accessdate = 2013-05-06|authorlink=Kevin LaGrandeur}}</ref> 直到19421年，这些主题促使推动艾萨克·阿西莫夫（Isaac Asimov）创造了“机器人三定律“－将原则硬连线至他创作的中所有的机器人定义为 硬连线，也这就意味着他们无法违背取代它们的创造者，或让他们受到也无法造成伤害。<ref>{{cite book| title=The Rest of the Robots| publisher=Doubleday| year=1964| isbn=0-385-09041-2| chapter=Introduction| author=Isaac Asimov}}</ref>

在现代，随着超智能AI的前景逐渐逼近出现，哲学家尼克·博斯托罗姆（Nick Bostrom）指出那些与人类道义不相符的AI系统非常危险，除非能采取极端手段确保人类的安全。他说到：

总的来说我们需要假定一个超极智能能够实现任何持有的目标。因此所以，我们所设定的目标，和整个推激动系统，必须是对‘人类友好’的，这一点非常重要。

机器学习的开拓者，里斯泽德·迈克尔斯基（Ryszard Michalski），在几十年前是这样教导他的博士生的，任何真正异己的的外星人思想，包括机器思想，都是不可知的，并因此对人类是危险的。

最近，埃利泽·尤多科斯基（Eliezer Yudkowsky）呼吁建立“友好的AI”来减轻先进人工智能所带来的存在性风险。他解释道：“人工智能并不恨你，也不爱你，但你是原子组成的，而它能利用这原子可以用作与做别的事情上”。<ref>[[Eliezer_Yudkowsky|Eliezer Yudkowsky]] (2008) in ''[http://intelligence.org/files/AIPosNegFactor.pdf Artificial Intelligence as a Positive and Negative Factor in Global Risk]''</ref>

史蒂夫·奥莫洪德（Steve Omohundro）表示，一个足够高级的人工智能系统，除非明确地的被抵制外，还是会展示一些基本的“驱动器”，如资源获取，由于目标驱动系统的固有特性，这些驱动器将会“没有特殊预防措施”地导致AI出现不良行为。<ref>Omohundro, S. M. (2008, February). The basic AI drives. In AGI (Vol. 171, pp. 483-492).</ref><ref>{{cite book|last1=Bostrom|first1=Nick|title=[[Superintelligence:_Paths,_Dangers,_Strategies|Superintelligence: Paths, Dangers, Strategies]]|date=2014|publisher=Oxford University Press|location=Oxford|isbn=9780199678112}} Chapter 7: The Superintelligent Will.</ref>

亚历山大·维斯纳 - 格罗斯（Alexander Wissner-Gross）表示，那些被驱动至把其未来自由行动（或因果路径熵）最大化的AI，如果其他们的计划范围超过一定的门槛，那些被驱动至把其最大化未来行动自由（或因果路径熵）最大化的AI有可能会被认为是友好的，而如果他们的规划范围到达不到比该阈值短，则会被识认为不友好。<ref>'[http://io9.com/how-skynet-might-emerge-from-simple-physics-482402911 How Skynet Might Emerge From Simple Physics]'', io9, Published 2013-04-26.</ref><ref>{{cite journal | last1 = Wissner-Gross | first1 = A. D. | authorlink1 = Alexander Wissner-Gross | last2 = Freer | first2 = C. E. | authorlink2 = Cameron Freer | year = 2013 | title = Causal entropic forces | url = http://www.alexwg.org/link?url=http%3A%2F%2Fwww.alexwg.org%2Fpublications%2FPhysRevLett_110-168702.pdf| journal = Physical Review Letters | volume = 110 | issue = 16| page = 168702 | doi = 10.1103/PhysRevLett.110.168702 | bibcode=2013PhRvL.110p8702W}}</ref>

卢克·迈赫豪泽（Luke Muehlhauser）, 为给机器情报研究所建撰写时，建议道，机器伦理研究人员应采用布鲁斯·施奈尔（Bruce Schneier）所说的“安全思维方式”：，而不是与其考虑系统将如何运作，不如或者想象系统能如何失败。例如，他提议建议道，就算是一个只做出准确的预测，并通过文本界面进行通信的人工智能，都有可能会导致意外的伤害。<ref name=MuehlhauserSecurity2013>{{cite web|last1=Muehlhauser|first1=Luke|title=AI Risk and the Security Mindset|url=http://intelligence.org/2013/07/31/ai-risk-and-the-security-mindset/|website=Machine Intelligence Research Institute|accessdate=15 July 2014|date=31 Jul 2013}}</ref> 

== 连贯推断意志 ==
尤多科斯基（Yudkowsky）推进了连贯推断意志（CEV）模型。据他所说，如果“我们知道更多，思靠更快，是我们更希望成为的人，并且更密切地一起成长”，连贯推断意志是人们的选择和人们将集体采取的行动。如果“我们知道更多，思想更快，我们是我们更希望成为的人，并且更接近地一起成长”。

友好AI是由“种子AI”设计以及被编程去首先研究人性，然后再创造成为人类想要的AI，给予了足够的时间和洞察力，以达到令人满意的答案，。而不是由人类程序员直接设计的。<ref name=cevpaper>{{cite web|url=https://intelligence.org/files/CEV.pdf |title=Coherent Extrapolated Volition |publisher=Intelligence.org |date= |accessdate=2015-09-12}}</ref> 
 通过或有附随的人性的目标所具有的吸引力（可能以数学目的，以效用函数或其他决策理论形式主义的形式表达）来吸引一个目标，为了提供“友善”的最终标准，是定义客观道德的元伦理问题的答案; 所有事情都放入考虑的情况下，在所有事情都纳入考量的情况下，推断意志是人类客观想要的，。 但它的定义只能相对于当今人性的心理和认知素质来定义，它只能定义为非推断的人性。 

== 其他途径 ==
通用基本人工智能研究员本·古泽尔（Ben Goertzel）认为，人类无法用当前的人类知识去创造友好AI的人工智能。古泽尔建议人类们可能会决定制造一个具有用“轻度超人的智慧和监视力量”的来创造一个“AI保姆”，以保护人类免受存在性风险如像纳米技术这样的存在性风险，并推迟其他（不友好的）人工智能的发展，直到安全方面的问题被解决。<ref>Goertzel, Ben. "[https://web.archive.org/web/20140408142320/http://commonsenseatheism.com/wp-content/uploads/2012/03/Goertzel-Should-Humanity-Build-a-Global-AI-Nanny-to-Delay-the-Singularity-Until-its-Better-Understood.pdf Should Humanity Build a Global AI Nanny to Delay the Singularity Until It’s Better Understood?]", Journal of consciousness studies 19.1-2 (2012): 1-2.</ref>


史蒂夫·奥莫洪德（Steve Omohundro）提出了一种“脚手架”的方法针对的人工智能的安全方法，其中由一代个可靠的AI生成可以帮助构建可靠的下一代AI。<ref name=Hendry2014>{{cite news|last1=Hendry|first1=Erica R.|title=What Happens When Artificial Intelligence Turns On Us?|url=http://www.smithsonianmag.com/innovation/what-happens-when-artificial-intelligence-turns-us-180949415/|accessdate=15 July 2014|agency=Smithsonian.com|date=21 Jan 2014}}</ref>

斯特凡·佩纳尔（Stefan Pernar）借用 美诺悖论 指出，试图解决FAI问题是无意义或不可救药取决于是否假定宇宙表现道德现实主义。在前者，超人智慧的AI将独立用理智把自己放置在正确的目标体系。而假定后者，设计友好AI一开始将会徒劳无功，因为道德不能以理性处置。<ref>Pernar, Stefan. "[http://rationalmorality.info/wp-content/uploads/2015/04/TranshumanPhilosophy_formatted.pdf The Evolutionary Perspective - a Transhuman Philosophy]", 8th Conference on Artificial General Intelligence in Berlin, July 22-25, 2015</ref>

== 公共政策 ==
“我们的最终发明”作者詹姆斯·巴拉特（James Barrat）建议道：“人类必须建立一个公私合作伙伴关系让，使A.I制造者互相分享对AI安全性的想法 － 像国际原子能机构，但是与企业公司之间的合作。” 他呼吁AI研究人员召开类似于 “阿西洛马（Asilomar）重组DNA会议”，一个 （讨论生物技术的风险）的会议。<ref name=Hendry2014 />

约翰·麦金尼斯（John McGinnis）鼓励政府加快友好AI人工智能的研究。因为友好AI的门槛不一定很清楚，所以他建议一个更像“国立卫生研究院”的模型。“计算机和认知科学家的同行评审小组将筛选项目，并选择那些旨在推进AI的设计，并确保这些进展将伴随适当的保障措施。” 麦金尼斯认为同行评审比监管能更好地“解决那些不可能通过官僚主义任务来捕捉获的技术问题。”麦金尼斯指出，他的建议与机器情报研究所的建议形成对照，机器情报研究所通常旨在避免政府参与友好AI的人工智能。<ref name=McGinnis2010>{{cite journal|last1=McGinnis|first1=John O.|title=Accelerating AI|journal=Northwestern University Law Review|date=Summer 2010|volume=104|issue=3|pages=1253–1270|url=http://www.law.northwestern.edu/LAWREVIEW/Colloquy/2010/12/|accessdate=16 July 2014}}</ref>


据加里·马库斯（Gary Marcus）所说，人类花费在开发机器道德上的年金额花费是微乎其微及少的。<ref>{{cite news|last1=Marcus|first1=Gary|title=Moral Machines|url=http://www.newyorker.com/news/news-desk/moral-machines|accessdate=30 July 2014|publisher=[[The_New_Yorker|The New Yorker]]|date=24 November 2012}}</ref>
 

== 批评 ==
另见：技术奇点&批评

有些评论家认为，人类智能和超级智能都是不太可能的，所以友好的AI也是不可能的。艾伦·温菲尔德（Alan Winfeld）在“守护者”中把人类级别AI人工智能的难度与比光线快的行驶的难度相比较并指出，鉴于涉及的风险，我们需要“谨慎准备”，我们“不需要困扰于”超级智能的风险。<ref>{{cite news|last1=Winfield|first1=Alan|title=Artificial intelligence will not turn into a Frankenstein's monster|url=https://www.theguardian.com/technology/2014/aug/10/artificial-intelligence-will-not-become-a-frankensteins-monster-ian-winfield|accessdate=17 September 2014|work=[[The_Guardian|The Guardian]]}}</ref>


一些哲学家声称，任何真正有的“理性”代理人，无论是人工的还是人类的，自然会是仁慈的; 在这种观点下，旨在产生友好AI的特定蓄意保障措施可能是不必要的，甚至是有害的。<ref>Kornai, András. "[http://www.kornai.com/Papers/agi12.pdf Bounding the impact of AGI]". Journal of Experimental & Theoretical Artificial Intelligence ahead-of-print (2014): 1-22. "...the essence of AGIs is their reasoning facilities, and it is the very logic of their being that will compel them to behave in a moral fashion... The real nightmare scenario (is one where) humans find it advantageous to strongly couple themselves to AGIs, with no guarantees against self-deception."</ref>  其他评论家质疑人工智能是否有可能是友好的。技术杂志“新亚特兰蒂斯”的编辑亚当·科佩尔（Adam Keiper）和阿里·舒尔曼（Ari N. Schulman）表示，由于道德复杂性的问题不会导致软件进步或计算能力的提高，所以人类不可能可以保证人工智能的“友好”行为。他们写道，“只有当人们不仅有强大的预测能力可能预测出无数可能的结果，而且对于如何重视不同的结果的确定性达到共识”，那么友好的AI理论所依据的标准才能起作用。
<ref>{{cite web|url=http://www.thenewatlantis.com/publications/the-problem-with-friendly-artificial-intelligence|author=Adam Keiper and Ari N. Schulman|title=The Problem with ‘Friendly’ Artificial Intelligence|publisher=''The New Atlantis''|accessdate = 2012-01-16}}</ref>

== 参看 ==
* 人工智能接管
* 人工智能伦理
* 基本人工智能的存在性危机
* 智能爆炸
* 机器伦理
* 机器智能研究所
* OpenAI
* 打开人工智能
* 奇点主义 – 友好AI倡导者倡导的道德哲学
* 技术奇点
* 机器人三项法则

== 参考 ==
{{Reflist|30em}}

== 延伸阅读 ==
* 尤多科斯基（Yudkowsky）作，人工智能作为全球风险的积极和消极因素。 《全球灾难性风险》，牛津大学出版社，2008年。
* 从存在性风险的角度讨论人工智能，引入“友好AI”一词。特别在第1-4节给出了第5节中友好AI定义的背景。第6节写给出了两类错误（技术和哲学），这两种错误都会导致意外创造不友好AI的认可机构。 第7-13节讨论进一步的相关问题。
* 奥莫洪德（Omohundro）作于2008年，基本的AI驱动出现于AGI-08 - 第一届人工智能会议论文集。

== 外部链接 ==
* 高级人工智能中的伦理问题 - 尼克·博斯托罗姆（Nick Bostrom）
* 什么是友好AI — 机器智能研究所对友好AI的简要介绍。
* 创建友好AI 1.0：和睦目标架构的分析与设计— 从MIRI的近书长的描述
* 关于友好AI的MIRI指南的批评 — 比尔·希伯德（Bill Hibbard）
* 关于MIRI友好AI指南的评论 —  彼得·沃斯（Peter Voss）
* 关于“友好”AI的问题 —关于友好AI的动机和不可能性 － 亚当·科佩尔（Adam Keiper）和阿里·舒尔曼（Ari N. Schulman）

[[Category:人工智能|Category:人工智能]]
[[Category:未来学|Category:未来学]]