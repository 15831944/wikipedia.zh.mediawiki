{{expert|time=2018-02-02T15:40:11+00:00}}
在[[信息论|信息论]]中，基于相同事件测度的两个[[概率分布|概率分布]]<math>p</math>和<math>q</math>的'''交叉熵'''是指，当基于一个“非自然”（相对于“真实”分布<math>p</math>而言）的概率分布<math>q</math>进行编码时，在事件集合中唯一标识一个事件所需要的平均比特数（[[bit|bit]]）。

基于[[概率分布|概率分布]]<math>p</math>和<math>q</math>的交叉熵定义为：

:<math>H(p, q) = \operatorname{E}_p[-\log q] = H(p) + D_{\mathrm{KL}}(p \| q),\!</math>

其中<math>H(p)</math>是<math>p</math>的[[信息熵|熵]]，<math>D_{\mathrm{KL}}(p \| q)</math>是从<math>p</math>到<math>q</math>的[[KL散度|KL散度]](也被称为''p''相对于''q''的''相对熵'')。

对于[[离散随机变量|离散]]分布<math>p</math>和<math>q</math>，这意味着：

:<math>H(p, q) = -\sum_x p(x)\, \log q(x). \!</math>

对于[[连续随机变量|连续]]分布也是类似的。我们假设<math>p</math>和<math>q</math>在[[测度|测度]] <math>r</math>上是[[绝对连续|绝对连续]]的(通常 <math>r</math>是[[Lebesgue_measure|Lebesgue measure]] on a [[Borel_set|Borel]] [[Sigma-algebra|σ-algebra]])。设<math>P</math>和<math>Q</math>分别为<math>p</math>的<math>q</math>在[[测度|测度]] <math>r</math>上概率密度函数。则

:<math>-\int_X P(x)\, \log Q(x)\, dr(x) = \operatorname{E}_p[-\log Q]. \!</math>

== 源起 ==
在[[信息论|信息论]]中, 以'''直接可解编码'''模式通过值<math>x_i</math>编码一个信息片段，使其能在所有可能的<math>X</math>集合中唯一标识该信息片段，[[Kraft's_inequality|Kraft–McMillan theorem]]确保这一过程可以被看作一种<math>X</math>上的隐式概率分布<math>q(x_i) = 2^{-l_i}</math>，从而使得<math>l_i</math>是<math>x_i</math>的编码位长度。 因此, 交叉熵可以看作每个信息片段在错误分布<math>Q</math>下的期望编码位长度，而信息实际分布为<math>P</math>。这就是期望<math>{E}_p</math>是基于<math>P</math>而不是<math>Q</math>的原因。

:<math>H(p, q) = \operatorname{E}_p[l_i] = \operatorname{E}_p\left[\log \frac{1}{q(x_i)}\right]</math>
:<math>H(p, q) = \sum_{x_i} p(x_i)\, \log \frac{1}{q(x_i)} \!</math>
:<math>H(p, q) = -\sum_x p(x)\, \log q(x). \!</math>

== 估计 ==
在大多数情况下，我们需要在不知道分布<math>p</math>的情况下计算其交叉熵。例如在[[语言模型|语言模型]]中, 我们基于训练集<math>T</math>创建了一个语言模型, 而在测试集合上通过其交叉熵来评估该模型的准确率。<math>p</math>是语料中词汇的真实分布，而<math>q</math>是我们获得的语言模型预测的词汇分布。由于真实分布是未知的，我们不能直接计算交叉熵。在这种情况下，我们可以通过下式来估计交叉熵:

:<math>H(T,q) = -\sum_{i=1}^N \frac{1}{N} \log_2 q(x_i)</math>

<math>N</math>是测试集大小，<math>q(x)</math>是在训练集上估计的事件<math>x</math>发生的概率。我们假设训练集是从<math>p(x)</math>的真实采样，则此方法获得的是真实交叉熵的蒙特卡洛估计。

* {{cite news | first1=Pieter-Tjerk |last1=de Boer |first2=Dirk P. |last2=Kroese |first3=Shie |last3=Mannor |first4=Reuven Y. |last4=Rubinstein |title=A Tutorial on the Cross-Entropy Method |journal=Annals of Operations Research |date=February 2005 |volume=134 |issue=1 |pages=19–67 |issn=1572-9338 |doi=10.1007/s10479-005-5724-z |url=http://eprints.eemcs.utwente.nl/7716/01/fulltext.pdf |type=pdf}}

[[Category:信息學熵|Category:信息學熵]]