{{multiple issues|
{{Request_translation|time=2013-09-08T02:30:21+00:00}}
{{expand|time=2013-09-08T02:30:21+00:00}}
{{orphan|time=2013-09-08T02:30:21+00:00}}
{{unreferenced|time=2013-09-08T02:30:21+00:00}}
}}
「'''貝爾曼方程（Bellman Equation）'''」也被稱作「'''動態規劃方程（Dynamic Programming Equation）'''」，由[[理查德·貝爾曼|理查·貝爾曼]]（Richard Bellman）發現。貝爾曼方程是[[動態規劃|動態規劃]]（Dynamic Programming）這種數學最佳化方法能夠達到[[最佳化|最佳化]]的[[必要條件|必要條件]]。此方程將「決策問題在特定時間點的值」以「來自初始選擇的報酬 及 由初始選擇衍生的決策問題的值」的形式表示。藉這個方式將動態最佳化問題變成較簡單的子問題，而這些子問題遵守由貝爾曼所提出的「最佳化原理」。

貝爾曼方程最早應用在工程領域的[[控制理論|控制理論]]及其他應用數學領域，而後成為[[經濟學|經濟學]]上的重要工具。

幾乎所有可以用[[最優控制|最佳控制理論]]（Optimal Control Theory）解決的問題也可以透過分析合適的貝爾曼方程得到解決。然而，「貝爾曼方程」通常指離散時間（discrete-time）最佳化問題的動態規劃方程。處理連續時間（continuous-time）最佳化問題上，也有類似的偏微分方程，稱作[[漢彌爾頓-雅各比-貝爾曼方程|漢彌爾頓-雅各比-貝爾曼方程]]（Hamilton–Jacobi–Bellman Equation, HJB Equation）。

== 動態規劃中的解析概念 ==
想了解貝爾曼方程，要先了解許多相關概念。首先，任何最佳化問題都有目標：旅行時間最小化、成本最小化、利潤最大化、效用最大化等。用來描述目標的數學函數就稱為'''目標函數'''。

動態規劃將多期規劃問題轉為不同時間點上較簡單的步驟，因此，它需要追蹤決策背景情況隨時間的變化。作正確決策所需要當前情況的資訊被稱作是「'''狀態（State）'''」（貝爾曼，1957，Ch. III.2）。<ref name=BellmanDP>Bellman, R.E. 1957. ''Dynamic Programming''. Princeton University Press, Princeton, NJ. Republished 2003: Dover, ISBN 0-486-42809-5.</ref><ref name=dreyfus>S. Dreyfus (2002), [http://www.wu-wien.ac.at/usr/h99c/h9951826/bellman_dynprog.pdf 'Richard Bellman on the birth of dynamic programming'] {{webarchive|url=https://web.archive.org/web/20050110161049/http://www.wu-wien.ac.at/usr/h99c/h9951826/bellman_dynprog.pdf |date=2005-01-10 }} ''Operations Research'' 50 (1), pp. 48–51.</ref>例如，為了決定每個時間要花多少錢，人們必須要知道他們初始財富的量，此例中財富就是一種「'''狀態變數（State Variables）'''」，或簡稱「'''狀態（State）'''」，當然也可能還有其他的種類。

從任意時點上所挑選以操作的變數通常稱為「'''[[控制變數|控制變數（Control Variables）]]'''」，或簡稱「'''控制（Control）'''」（控制理論中描述輸入的變數）。例如給定現在所具有的財富（狀態），人們便可以用以決定當下的消費（控制變數）。挑選當下的控制變數可被視為挑選下個狀態，廣義而言，下個狀態受到當下控制變數及其他因子的影響。舉個簡單的例子：今天的財富（狀態）及消費（控制變數）會決定明天的財富（新的狀態），雖然通常也還有其他的因素可以影響明天的財富（例如獲得意外之財）。

動態規劃方法中利用「找尋某種規則告訴我們各可能狀態下的（最佳）控制為何」來達成目標函數最佳化。例如：假設消費（c）只與財富（W）相關，我們想要找到一套規則<math>c(W)</math>來以財富描述消費。這些「將控制（Controls）表示成狀態（States）的函數」的規則被稱為'''策略函數（Policy Function）'''。<ref name=BellmanDP />

從定義可知，最佳化目標函數的策略乃是所有可能的策略函數中，其對應到目標函數值最佳者。沿用上述的例子，若某人利用給定的財富來消費以最大化快樂的感覺（這裡假定「快樂的感覺」可以被數學函數描述，像是'''效用函數'''等），那麼各種初始的財富便會對應到一個可能的最大快樂，表示成<math>H(W)</math>。這個最大的可能目標函數值（快樂的感覺），即是'''價值函數（Value Function）'''。

== 貝爾曼方程的推導 ==
=== 動態決策問題 ===
=== 貝爾曼最佳化原理 ===
=== 貝爾曼方程 ===

=== 貝爾曼方程在隨機問題的應用 ===

== 解法 ==
== 經濟學上的應用 ==
== 例子 ==

== 參考資料 ==
{{Reflist}}

[[Category:數學最佳化|Category:數學最佳化]]
[[Category:方程|Category:方程]]
[[Category:动态规划|Category:动态规划]]
[[Category:控制理论|Category:控制理论]]