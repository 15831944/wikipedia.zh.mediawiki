{{机器学习导航栏}}

'''统计学习理论'''（{{lang-en|Statistical learning theory}}），一種[[機器學習|機器學習]]的架構，根據[[統計學|統計學]]與[[泛函分析|泛函分析]]（Functional Analysis）而建立。統計學習理論基於資料（data），找出預測性函數，之後解決問題。[[支持向量机|支持向量机]]（Support Vector Machine）的理論基礎來自於統計學習理論。

[[Category:机器学习|Category:机器学习]]
==形式定义==

令<math>X</math>为所有可能的输入组成的向量空间， <math>Y</math>为所有可能的输出组成的向量空间。统计学习理论认为，积空间<math>Z = X \times Y</math>上存在某个未知的概率分布<math>p(z) = p(\vec{x},y)</math>。训练集由这个概率分布中的<math>n</math>个样例构成，并用<math>S = \{(\vec{x}_1,y_1), \dots ,(\vec{x}_n,y_n)\} = \{\vec{z}_1, \dots ,\vec{z}_n\}</math>表示。每个<math>\vec{x}_i</math>都是训练数据的一个输入向量， 而<math>y_i</math>则是对应的输出向量。

==损失函数==

损失函数的选择是机器学习算法所选的函数<math>f_S</math>中的决定性因素。 损失函数也影响着算法的收敛速率。损失函数的凸性也十分重要。<ref>Rosasco, L., Vito, E.D., Caponnetto, A., Fiana, M., and Verri A. 2004. ''Neural computation'' Vol 16, pp 1063-1076</ref>

根据问题是回归问题还是分类问题，我们可以使用不同的损失函数。

===回归问题===

回归问题中最常用的损失函数是平方损失函数（也被称为[[范数|L2-范数]])。类似的损失函数也被用在[[普通最小二乘回归|普通最小二乘回归]]。其形式是：
:<math>V(f(\vec{x}),y) = (y - f(\vec{x}))^2</math>

另一个常见的损失函数是绝对值范数（[[范数|L1-范数]]）：
:<math>V(f(\vec{x}),y) = |y - f(\vec{x})|</math>

===分类问题===
{{main|统计分类}}
某种程度上说0-1[[指示函数|指示函数]]是分类问题中最自然的损失函数。它在预测结果与真实结果相同时取0，相异时取1。对于<math>Y = \{-1, 1\}</math>的二分类问题，这可以表示为：
:<math>V(f(\vec{x}),y) = \theta(- y f(\vec{x}))</math>
其中<math>\theta</math>为[[单位阶跃函数|单位阶跃函数]]。

===正则化===

[[File:Overfitting_on_Training_Set_Data.pdf|thumb]]

机器学习的一大常见问题是[[过拟合|过拟合]]。由于机器学习是一个预测问题，其目标并不是找到一个与（之前观测到的）数据最拟合的的函数，而是寻找一个能对未来的输入作出最精确预测的函数。[[经验风险最小化|经验风险最小化]]有过拟合的风险：找到的函数完美地匹配现有数据但并不能很好地预测未来的输出。

过拟合的常见表现是不稳定的解：训练数据的一个小的扰动会导致学到的函数的巨大波动。可以证明，如果解的稳定性可以得到保证，那么其可推广性和一致性也同样能得到保证。<ref>Vapnik, V.N. and Chervonenkis, A.Y. 1971. [http://ai2-s2-pdfs.s3.amazonaws.com/a36b/028d024bf358c4af1a5e1dc3ca0aed23b553.pdf On the uniform convergence of relative frequencies of events to their probabilities]. ''Theory of Probability and its Applications'' Vol 16, pp 264-280.</ref><ref>Mukherjee, S., Niyogi, P. Poggio, T., and Rifkin, R. 2006. Learning theory: stability is sufficient for generalization and necessary and sufficient for consistency of empirical risk minimization. ''Advances in Computational Mathematics''. Vol 25, pp 161-193.</ref> [[正则化_(数学)|正则化]]可以解决过拟合的问题并增加解的稳定性。

正则化可以通过限制假设空间<math>\mathcal{H}</math>来完成。一个常见的例子是把<math>\mathcal{H}</math>限制为线性函数：这可以被看成是把问题简化为标准设计的[[线性回归|线性回归]]。<math>\mathcal{H}</math>也可以被限制为<math>p</math>次多项式，指数函数，或[[Lp空间|L1]]上的有界函数。对假设空间的限制能防止过拟合的原因是，潜在的函数的形式得到了限制，因此防止了那些能给出任意接近于0的经验风险的复杂函数。

一个正则化的样例是[[吉洪诺夫正则化|吉洪诺夫正则化]]，即最小化如下损失函数
:<math>\frac{1}{n} \displaystyle \sum_{i=1}^n V(f(\vec{x}_i),y_i) + \gamma
\|f\|_{\mathcal{H}}^2</math>
其中正则化参数<math>\gamma</math>为一个固定的正参数。吉洪诺夫正则化保证了解的存在性、唯一性和稳定性。<ref>Tomaso Poggio, Lorenzo Rosasco, et al. ''Statistical Learning Theory and Applications'', 2012, [http://www.mit.edu/~9.520/spring12/slides/class02/class02.pdf Class 2]</ref>