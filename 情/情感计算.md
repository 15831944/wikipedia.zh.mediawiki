'''情感计算'''（{{lang-en|Affective computing}}，亦作'''-{zh:人工情感智能;zh-hans:人工情感智能;zh-hant:人工情感智慧;zh-tw:人工情感智慧;zh-hk:人工情感智能}-'''，{{Lang-en|artificial emotional intelligence}}，或'''情感AI'''，{{Lang-en|emotion AI}}）<ref>{{cite article| title=We Need Computers with Empathy |magazine=Technology Review |author=Rana el Kaliouby |volume=120 |number=6 |page=8 |date=Nov–Dec 2017}}</ref>是一个跨学科领域，涉及[[计算机科学|计算机科学]]、 [[心理学|心理学]]和[[认知科学|认知科学]]，旨在研发能够识别、解释、处理、模拟人类[[情感_(心理學)|情感]]的系统。<ref name=TaoTan>{{cite conference |first=Jianhua |last=Tao |author2=Tieniu Tan |title=Affective Computing: A Review |booktitle=Affective Computing and Intelligent Interaction |volume=[[LNCS|LNCS]] 3784 |pages=981–995 |publisher=Springer |year=2005 |doi=10.1007/11573548 }}</ref>虽然该学科最早可追溯至早期的哲学研究，即人们对[[情绪|情绪]]{{NoteTag|心理学上，情绪（{{lang-en|emotion}}）和情感（{{lang-en|affect}}）是有区别的两个概念：前者强调情境性和与人的生理性联系，是短暂的；后者强调社会性，更加持久。在情感计算领域大多不刻意区分两词。本文原则上对译，但为行文流畅，并不严格做区分。}}的剖析，<ref>{{Cite journal|title=What is Emotion|last=James|first=William|journal=Mind|doi=10.1093/mind/os-IX.34.188|year=1884|volume=9|pages=188–205}} Cited by Tao and Tan.</ref>但真正使其成为现代计算机科学分支的，则是1995年[[罗莎琳·皮卡德|罗莎琳·皮卡德]]发表的关于情感计算的论文。<ref>[http://affect.media.mit.edu/pdfs/95.picard.pdf "Affective Computing"] MIT Technical Report #321 ([http://vismod.media.mit.edu/pub/tech-reports/TR-321-ABSTRACT.html Abstract]), 1995</ref><ref>{{Cite web|url=http://ls12-www.cs.tu-dortmund.de//~fink/lectures/SS06/human-robot-interaction/Emotion-RecognitionAndSimulation.pdf|title=Recognition and Simulation of Emotions|accessdate=2008-05-13|date=October 2006|last=Kleine-Cosack|first=Christian|format=PDF|archiveurl=https://web.archive.org/web/20080528135730/http://ls12-www.cs.tu-dortmund.de/~fink/lectures/SS06/human-robot-interaction/Emotion-RecognitionAndSimulation.pdf|archivedate=2008-05-28|quote=The introduction of emotion to computer science was done by Pickard (sic) who created the field of affective computing.}}</ref><ref>{{Cite web|url=https://www.wired.com/wired/archive/11.12/love.html|title=The Love Machine; Building computers that care|accessdate=2008-05-13|date=December 2003|last=Diamond|first=David|publisher=Wired|archiveurl=https://web.archive.org/web/20080518185630/http://www.wired.com/wired/archive/11.12/love.html|archivedate=18 May 2008 <!--DASHBot-->|quote=Rosalind Picard, a genial MIT professor, is the field's godmother; her 1997 book, Affective Computing, triggered an explosion of interest in the emotional side of computers and their users.}}</ref>人们研究情感计算很大程度上是为了能够模拟[[共情|共情]]——机器应该能够解释人类的情绪状态，做出相适应的行为，对情绪给予恰当的回应。

[[文本情感分析|文本情感分析]]（{{Lang-en|sentiment analysis}}）和情感分析的区别在于，前者仅辨识词语的情感极性，后者辨识人类的不同情绪。

== 研究范围 ==

=== 分类与维度 ===
{{main|情绪}}
{{情绪侧栏}}
在[[认知科学|认知科学]]和[[神经科学|神经科学]]中，描述人类感知，并对人类情感进行分类的模型主要有两种： 连续模型（{{lang-en|continuous model}}）和分类模型（{{lang-en|categorical model}}）。若将可能的面部表情视作一个空间，那么连续模型将每个情绪的面部表情定义为该空间的特征向量（比如，该模型能够将不同的情绪解释为不同的表达强度）。相反，分类模型由<!--C个-->数个不同的分类器组成，每个分类器各自侦测不同的情感。这个模型解释了许多现象。例如，如果在快乐与惊奇的表情图片之间插入变形序列的话，在这些过渡图像中，人们看到的要么是快乐要么是惊奇的神情，而不会认为是两者兼有。

==== 分类模型 ====
{{main|情绪分类}}
中国自古代就有“喜、怒、哀、惧、爱、恶、欲”的“[[七情|七情]]”说法，这便是情绪分类模型的一个例子。情绪的正确分类是心理学上历来的争议话题，现代心理学中对此大致有两种观点，一种认为情绪可以被割裂地划分为几类，另一种认为大部分情绪本质相同，仅仅是程度上存在差异。<ref name=":0">{{Cite book|title=人体生理信号的情感计算方法|last=刘|first=光远|last2=温|first2=万惠|last3=陈|first3=通|last4=赖|first4=祥伟|publisher=科学出版社|year=2014|isbn=9787030419507|location=北京|pages=}}</ref>持分类模型态度的心理学家大多赞成将情感分为基础情感和复合情感。<ref>{{Cite book|title=情绪心理学|last=孟|first=昭兰|publisher=北京大学出版社|year=2005|isbn=9787301086339|location=北京|pages=}}</ref><ref>{{Cite journal|title=What's basic about basic emotions|author=Ortony A, Tumer T J.|url=|journal=Psychological Review|issue=3|doi=|others=|year=1990|volume=97|page=315-331|pmid=}}</ref>

==== 连续模型 ====
维度情绪论是基本情绪论之外的另一种情绪研究观点，认为情绪固有性质存在度量，即维度（{{Lang-en|dimension}}）。维度具有两个方向的极端，即极性（{{Lang-en|polarity}}）。关于情感的连续模型并没有统一的标准评价其好坏，常见的连续情感模型有：<ref name=":0" />

{{columns-list|2|
* Johnston一维模型
* Wessman–Braduburn–Zevon二维模型
* Wundt三维模型
* Schloberg三维模型（倒立圆锥）
* Plutchik三维模型（抛物锥）<ref>{{Cite journal|title=The nature of emotions|author=Plutchik R.|url=|journal=American Scientist|issue=|doi=|others=|year=2001|volume=89|page=344-356|pmid=}}</ref>
* Mehrabian和Russell的PAD（愉悦度—唤醒度—优势度，{{Lang-en|pleasure–arousal–dominance}}）三维模型<ref>{{Cite book|title=Basic Dimensions for a General Psychological Theory: Implications for Personality, Social, Environmental and Developmental Studies|last=Mehrabian|first=Albert|publisher=Gunn & Hain|year=1980|isbn=0899460046|location=Cambridge|pages=}}</ref>
* Blumenthal三维模型
* Millenson三维模型
* Taylor三维模型
* Izard四维说
* Krech四维说
* Frijda六维说
}}

=== 检测与识别情感信息 ===
情感信息的检测会从被动式[[传感器|传感器]]开始，它能够捕捉到用户的生理状态、行为表现方面的原始数据，这些数据和人类用以觉察他人情感的线索很相似。例如，摄像机可以捕捉面部表情、身体姿势和手势，麦克风则可以捕捉语音。一些传感器还可以通过测量生理数据（如皮肤的温度和电势）来探测情感线索。<ref>{{Cite journal|title=Assistive Technology and Affective Mediation|url=http://www.humantechnology.jyu.fi/articles/volume2/number1/2006/humantechnology-april-2006.pdf|last=Garay|first=Nestor|last2=Idoia Cearreta|date=April 2006|journal=[[Human_Technology|Human Technology]]|accessdate=2008-05-12|issue=1|volume=2|pages=55–83|format=PDF|archiveurl=https://web.archive.org/web/20080528135729/http://www.humantechnology.jyu.fi/articles/volume2/number1/2006/humantechnology-april-2006.pdf|archivedate=2008-05-28 <!--DASHBot-->|deadurl=no|last3=Juan Miguel López|last4=Inmaculada Fajardo}}</ref>

识别情感信息需要从收集的数据中提取有意义的模式，通常要用到多模态的[[机器学习|机器学习]]技术（如[[语音识别|语音识别]]、[[自然语言处理|自然语言处理]]、面部表情检测）。处理的结果要么会被打上标签，要么会映射到“正负性—唤醒度（{{lang|en|valence–arousal}}）”空间上的点。

=== 机器中的情感 ===
设计一种计算设备，使其能够展现出天然的情感能力（或是至少令人信服地模拟出人类的情感），是情感计算的另一研究范围。基于现有的技术能力，模拟对话机器人的情感是更具可行性的一种做法，以此来丰富并推动人与机器之间的互动。<ref>{{Cite book|last=Heise|first=David|contribution=Enculturating agents with expressive role behavior|year=2004|title=Agent Culture: Human-Agent Interaction in a Mutlicultural World|editor1=Sabine Payr|pages=127–142|publisher=Lawrence Erlbaum Associates|editor2-first=Robert |editor2-last=Trappl}}</ref>正如人类的情感和[[激素|激素]]水平以及神经肽的波动息息相关，机器中的情感可能会与其自动学习过程的进展（或无进展）的抽象状态相关联。以此观之，不论是人还是机器，情绪状态与其学习系统的[[學習曲線|学习曲线]]对时间的导数（即“扰动”）相关。

[[人工智能|人工智能]]的先驱之一——[[马文·闵斯基|马文·闵斯基]]，在《{{tsl|en|The Emotion Machine|情感机器}}》一书中将情感和机器智能这一更为广泛的概念联系了起来。他说，情感“和我们称之为‘思考’的过程并没有显著差别”。<ref>{{Cite news|url=https://www.washingtonpost.com/wp-dyn/content/article/2006/12/14/AR2006121401554.html|title=Mind Over Matter|last=Restak|first=Richard|date=2006-12-17|work=The Washington Post|accessdate=2008-05-13}}</ref>

== 识别技术 ==
这些方法有一个很重要的共同缺陷：它们只能从图像中检出一种情绪，也就是在应用了各种方法中胜出的那个情绪；但在日常生活中，我们总能从单一图像中感知到不止一种情感。分类和连续模型都无法识别多种情感，因此对情感建模有一种新方法，即将一小部分类别的重叠视为一个新类别。有关这一主题的详细研究请参见综述《人类对面部表情情感的感知模型：研究现状与展望（{{Lang|en|A model of the perception of facial expressions of emotion by humans: research overview and perspectives}}）》。<ref>{{Cite journal|title=A model of the perception of facial expressions of emotion by humans: Research overview and perspectives.|last=Aleix, and Shichuan Du|first=Martinez|date=2012|journal=The Journal of Machine Learning Research|issue=1|volume=13|pages=1589–1608}}</ref>

以下各节将介绍可用于情感识别的特征。

=== 语音情感 ===
[[自主神经系统|自主神经系统]]的各种变化可以对人的语音产生间接影响。情感技术可以利用这些信息来识别情感。例如，在恐惧、愤怒或欢乐的状态下产生的言语会变得快速、大声，更确切地说，音域会更宽更高；而在疲劳、无聊或悲伤等情绪下，语音倾向于变得缓慢、低沉与不清楚。<ref>Breazeal, C. and Aryananda, L. Recognition of affective communicative intent in robot-directed speech. Autonomous Robots 12 1, 2002. pp. 83–104.</ref> 有些情感被证实更容易通过计算识别，如愤怒<ref name="Dellaert" />或赞同<ref>{{Cite journal|title=Automatic spoken affect classification and analysis|url=http://ieeexplore.ieee.org/document/557292/?reload=true|last=Roy|first=D.|last2=Pentland|first2=A.|date=1996-10-01|journal=Proceedings of the Second International Conference on Automatic Face and Gesture Recognition|doi=10.1109/AFGR.1996.557292|pages=363–367}}</ref>。

情感语音处理技术可以利用语音特征的计算分析来识别用户情绪状态。[[模式识别|模式识别]]技术可被应用于声学参数和[[韵律_(语言学)|韵律]]特征，例如音调高低和语速等。<ref name="Dellaert">Dellaert, F., Polizin, t., and Waibel, A., Recognizing Emotion in Speech", In Proc. Of ICSLP 1996, Philadelphia, PA, pp.1970-1973, 1996</ref><ref name="Lee">Lee, C.M.; Narayanan, S.; Pieraccini, R., Recognition of Negative Emotion in the Human Speech Signals, Workshop on Auto. Speech Recognition and Understanding, Dec 2001</ref>

语音分析可以高效地分析情感状态，在最近的研究中达到了70%至80%的正确率。<ref>{{Cite journal|title=Emotion recognition in spontaneous speech using GMMs|url=http://www.speech.kth.se/prod/publications/files/1192.pdf|last=Neiberg|first=D|last2=Elenius|first2=K|date=2006|journal=Proceedings of Interspeech|last3=Laskowski|first3=K}}</ref><ref>{{Cite journal|title=Recognition of Emotions in Interactive Voice Response Systems|url=http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.420.8158|last=Yacoub|first=Sherif|last2=Simske|first2=Steve|date=2003|journal=Proceedings of Eurospeech|pages=1–4|last3=Lin|first3=Xiaofan|last4=Burns|first4=John}}</ref>这一正确率已经超过了普通人识别精度的均值（大约60%），但是比生理学或面部识别方法的正确率要低。<ref name="Hudlicka-2003-p24">{{Harvnb|Hudlicka|2003|p=24}}</ref>然而，由于大多语音特征与文化或语义无关，研究者认为这是一个未来的研究前景方向<ref name="Hudlicka-2003-p25">{{Harvnb|Hudlicka|2003|p=25}}</ref>。

==== 算法 ====
{{main|机器学习}}
进行语音/文本的情感检测需要建立可靠的[[数据库|数据库]]、[[知识库|知识库]]或者[[向量空間模型|向量空间模型]]<ref name="Osgood75">{{Cite book|title=Cross-Cultural Universals of Affective Meaning|last=Charles Osgood|last2=William May|last3=Murray Miron|publisher=Univ. of Illinois Press|year=1975|isbn=978-94-007-5069-2}}
</ref>。为了适应各种应用，这些库或是模型涉及面应当足够广泛。另外，还需要选择出一个又快又准确的情感分类器。

目前，常用的分类器有线性分类器、k-近邻（k-NN）、高斯混合模型、支持向量机（SVM）、人工神经网络（ANN）、决策树算法和隐马尔可夫模型（HMM）<ref name="Scherer-2010-p241">{{Harvnb|Scherer|2010|p=241}}</ref>。各种研究表明，选择合适的分类器可以大大提高系统的效率与精度。以下简要说明每个算法：
* [[线性分类器|线性分类器]]：特征以向量的形式表示，通过计算特征的线性组合来分类。
* [[最近鄰居法|k-近邻算法]]：计算并选取特征空间中的点，将其与k个最近的数据点相比较，分类决定于比较点中频数最大的类。
* [[高斯混合模型|高斯混合模型]]：是一种概率模型，用以发现并表示整体中局部的存在。利用特征的多个高斯概率密度函数混合来分类。<ref>[http://cnx.org/content/m13205/latest/ "Gaussian Mixture Model"]. Connexions – Sharing Knowledge and Building Communities. Retrieved 10 March 2011.</ref>
* [[支持向量机|支持向量机]]：一种（大多是二分的）线性分类器，可将空间分成两个（或更多）集合，来实现对点的分类。
* [[人工神经网络|人工神经网络]]：利用多层的、仿生的神经网络进行复杂的非线性分类。
* [[决策树学习|决策树学习]]：在一颗[[树_(数据结构)|树]]中，每个叶子结点都是一个分类点，分支（路径）代表了一系列相邻接的特征，最终引向叶子节点实现分类。
* [[隐马尔可夫模型|隐马尔可夫模型]]：是一种马尔可夫统计模型，其中的状态与状态转移并不直观。而依赖于这些状态的输出序列是可见的。在情感识别领域，输出代表了语音特征向量序列，其使得状态序列可在模型处理时被推导出来。这些状态包括情感表达中的各中间步骤，每个状态在输出向量上都有一个概率分布。状态序列是我们能够预测正在试图分类的情感状态，这也是语音情感识别中最为常用的技术之一。
研究证实，有了足够的声音样本之后，人的情感可以被主流分类器所正确分类。文献建议使用的模型由以下三种组合而成：k-NN、C4.5决策树和[[径向基函数核|径向基函数核]]的SVM。这一组合模型比每单个分类器性能都更好，也超过了使用混合核（{{Lang-en|Hybrid kernels}}）的“一对多”（{{Lang-en|one-against-all, OAA}}）多类SVM，以及C5.0决策树与神经网络的组合<ref>{{cite journal|url=http://ntv.ifmo.ru/en/article/11200/raspoznavanie_i_prognozirovanie_dlitelnyh__emociy_v_rechi_(na_angl._yazyke).htm|title=Extended speech emotion recognition and prediction|author=S.E. Khoruzhnikov|journal=Scientific and Technical Journal of Information Technologies, Mechanics and Optics|volume=14|issue=6|page=137|year=2014|display-authors=etal}}</ref>。

==== 数据库 ====
目前绝大多数的系统都是数据依赖的。选择一个恰当的数据库来训练分类器因而成为语音情感识别的首要问题。目前拥有的大部分数据是从演员获得的，都是一些典型的情绪表现。这些所谓的表演数据库大多基于[[保罗·艾克曼|保罗·艾克曼]]的基础情绪理论，其假设了六种基础情绪的存在，即愤怒、害怕、厌恶、惊奇、愉快和哀伤，而其他情绪仅仅是前六者的组合。<ref name="Ekman, P. 1969">Ekman, P. & Friesen, W. V (1969). The repertoire of nonverbal behavior: Categories, origins, usage, and coding. Semiotica, 1, 49–98.</ref>

另一方面，对现实生活应用来说，自然数据更受青睐。通过观察并分析被试在自然情境下的行为，研究者可以建立自然情感的数据库。最终，自然数据库会帮助系统识别情境下的情绪，也可以用来发现交互的目标和结果。由于这类数据的自然性，可以真实自然地反映[[人机交互|人机交互]]下的情感状态，也就可以应用于现实生活中的系统实现。

尽管自然数据相比表演数据有更多优势，然而自然数据难以获得，情感密度也更低。由于环境噪声的存在、被试与麦克风的距离较远，自然情境下获得的数据信号质量也因此更差。[[埃尔朗根-纽伦堡大学|埃尔朗根-纽伦堡大学]]的[[AIBO|AIBO]]情感资料库（{{Lang|en|FAU Aibo Emotion Corpus for CEICES, CEICES: Combining Efforts for Improving Automatic Classification of Emotional User States}}）是建立自然情感数据库的首次尝试，其采集基于10—13岁儿童与索尼[[AIBO|AIBO]]宠物机器人玩耍的真实情境。<ref name="Steidl-2011">{{Cite web|url=http://www5.cs.fau.de/de/mitarbeiter/steidl-stefan/fau-aibo-emotion-corpus/|title=FAU Aibo Emotion Corpus|date=2011-03-05|last=Steidl|first=Stefan|publisher=Pattern Recognition Lab}}</ref><ref name="Scherer-2010-p243">{{Harvnb|Scherer|2010|p=243}}</ref> 同样，在情感研究领域，建立任何一个标准数据库，都需要提供评估方法，以比较不同情感识别系统的差异。

==== 语音叙词 ====
情感识别的复杂度随情感类别和语音叙词的增加而增加，因此选取最为相关的特征是必要的。这样做不仅可以确保模型识别情感的成功率，也可以提升计算性能，尤其对实时检测系统更要如此。可选项很多，有些研究曾提到超过200种不同的特征。<ref name="Scherer-2010-p241"/>最为常见的语音特征被归纳为以下列表<ref name="Steidl-2011"/><ref name="Scherer-2010-p243"/>：

# 频率特征
#* 音调形状（{{lang-en|accent shape}}）：由基础频率改变的频度所影响
#* 平均音高：讲话者相对于正常说话的高低
#* 音高轮廓（{{lang-en|counter slope}}）：频率随时间变化的趋势，可升高、降低或持平
#* 尾音下降（{{lang-en|final lowering}}）：一段话末尾频率下降的多少
#* 音域：一段话最高频率和最低频率的差值
# 时间相关特征：
#* 语速：单位时间内词数或音节数
#* 重音频率（{{lang-en|stress frequency}}）：重读发生的频率
# 音质参数与能量叙词：
#* 呼吸音（{{lang-en|breathiness}}）：说话中的呼吸噪声
#* 透明度（{{lang-en|brilliance}}）：语音中高频和低频的占比
#* 响度：语音波形的振幅
#* 间断不连续性（{{lang-en|pause discontinuity}}）：描述有声和静默之间的转换
#* 音调不连续性（{{lang-en|pitch discontinuity}}）：描述基础频率的转换

=== 面部情感检测 ===
面部表情的检测和处理可以利用[[隐马尔可夫模型|隐马尔可夫模型]]和[[人工神经网络|人工神经网络]]等方法，也可在多模态的检测中将各种方法组合或交融起来（多模态，例如面部表情和语音韵律结合<ref name="face-prosody">{{cite conference | url = http://www.image.ece.ntua.gr/php/savepaper.php?id=447 | first1 = G. | last1 = Caridakis | first2 = L. | last2 = Malatesta | first3 = L. | last3 = Kessous | first4 = N. | last4 = Amir | first5 = A. | last5 = Raouzaiou | first6 = K. | last6 = Karpouzis | title = Modeling naturalistic affective states via facial and vocal expressions recognition | conference = International Conference on Multimodal Interfaces (ICMI'06) | location = Banff, Alberta, Canada | date = November 2–4, 2006 }}</ref>、面部表情和手势结合<ref name="face-gesture">{{Cite book|url=http://www.image.ece.ntua.gr/php/savepaper.php?id=334|title=Machine Learning for Multimodal Interaction|last=Balomenos|first=T.|last2=Raouzaiou|first2=A.|last3=Ioannou|first3=S.|last4=Drosopoulos|first4=A.|last5=Karpouzis|first5=K.|last6=Kollias|first6=S.|publisher=[[Springer-Verlag|Springer-Verlag]]|year=2004|editor-last=Bengio|editor-first=Samy|editor2-last=Bourlard|editor2-first=Herve|series=[[Lecture_Notes_in_Computer_Science|Lecture Notes in Computer Science]]|volume=3361|pages=318–328|chapter=Emotion Analysis in Man-Machine Interaction Systems}}</ref>、面部表情和语音与文本的多模态数据与元数据分析），以更加稳健地估计对象的情感状态。

==== 面部表情数据库 ====
建立情感数据库极其困难和耗时，然而其又是识别人类情感的必要步骤。大多公开的情感数据库仅包含摆拍的面部表情，在这样的数据库中，参与者会被要求摆出不同基础情感的对应表情；而在自然表情数据库中，面部表情是自发的。自然表情的发生需要选取恰当的刺激，这样才能引起目标表情的丰富展示。其次，这个过程需要受过训练的工作者为数据做标注，以实现数据库的高度可靠。因为表情及其强度的感知本质上是主观的，专家的标注对验证而言是十分重要的。

研究者接触到的数据库可能包括以下三种：峰值表情（{{Lang-en|peak expression image}}）数据库、中性到峰值表情的图像序列数据库和打上了情感标注的视频片段。被广泛使用的开放表情数据库有CK+和JAFFE。

==== 情感分类 ====
{{main|情绪分类}}
在19世纪60年代末，[[保罗·艾克曼|保罗·艾克曼]]在巴布亚新几内亚的[[法雷人|法雷人]]部落中进行了跨文化研究，之后提出，情感所对应的面部表情是普遍的，与文化无关。因此他提议，面部表情是生物本能，可以安全、正确地被归类。此后，他于1972年正式提出六个基本的情感：<ref>{{cite conference | last = Ekman | first =  Paul | authorlink = Paul Ekman | year = 1972 | title = Universals and Cultural Differences in Facial Expression of Emotion | editor-first = J. | editor-last = Cole | conference = Nebraska Symposium on Motivation | location = Lincoln, Nebraska | publisher = University of Nebraska Press | pages = 207–283 }}</ref>
{{columns-list|2|
* [[憤怒|愤怒]]
* [[厭惡|厌恶]]
* [[恐惧|恐惧]]
* [[快樂|快乐]]
* [[悲傷|悲伤]]
* [[惊奇|惊奇]]
}}
之后在1990年代，艾克曼在基本情感列表中加入了一系列的积极和消极的情绪。并不是所有这类情感都对应于面部肌肉<ref>{{Cite book|url=http://www.paulekman.com/wp-content/uploads/2009/02/Basic-Emotions.pdf|title=Handbook of Cognition and Emotion|last=Ekman|first=Paul|authorlink=Paul Ekman|publisher=John Wiley & Sons|year=1999|editor-last=Dalgleish|editor-first=T|editor2-last=Power|editor2-first=M|location=Sussex, UK|chapter=Basic Emotions|archiveurl=https://web.archive.org/web/20101228085345/http://www.paulekman.com/wp-content/uploads/2009/02/Basic-Emotions.pdf|archivedate=2010-12-28|deadurl=yes}}.</ref>新加入的情感如下：
{{columns-list|2|
* [[趣味|趣味]]（{{lang-en|amusement}}）
* [[蔑视|蔑视]]（{{lang-en|contempt}}）
* [[放松（情感）|满足]]（{{lang-en|contentment}}）
* [[尷尬|尴尬]]
* [[兴奋|兴奋]]（{{lang-en|excitement}}）
* [[內疚|内疚]]
* [[骄傲|成就感]]（{{lang-en|pride in achievement}}）
* [[放松（情感）|放松]]（{{lang-en|relief}}）
* [[满意|满意]]（{{lang-en|satisfaction}}）
* [[愉快|愉快]]（{{lang-en|sensory pleasure}}）
* [[羞耻|羞辱]]
}}

==== 面部行为编码系统 ====
依肌肉动作定义表情的方法已经被应用于情感生理表达的形式分类。在由保罗·艾克曼（{{Lang|en|Paul Ekman}}）和华莱士·V·弗里森（{{Lang|en|Wallace V. Friesen}}）提出的面部表情编码系统（{{Lang-en|Facial Action Coding System, FACS}}）中，动作单位（{{Lang-en|action unit, AU}}）是核心概念。<ref>[http://face-and-emotion.com/dataface/facs/description.jsp "Facial Action Coding System (FACS) and the FACS Manual"] {{Webarchive|url=https://web.archive.org/web/20131019130324/http://face-and-emotion.com/dataface/facs/description.jsp|date=2013-10-19}}. A Human Face. Retrieved 21 March 2011.</ref>即，一块多一组肌肉的收缩或舒张。尽管这一概念看起来简单，但已经足以建立和描述复杂的情感识别系统。

通过识别不同的面部特征，研究人员能够将其映射到相应的行为单元代码。据此，他们依这些单位提出了六种基本情绪的分类（“+”意思是“和”）：
{| class="wikitable sortable"
! 情感 
! 行为单位
|-
|快乐
|6+12
|-
| 悲伤 
| 1+4+15
|-
|惊讶
|1+2+5B+26
|-
| 恐惧 
| 1+2+4+5+20+26
|-
| 愤怒 
| 4+5+7+23
|-
| 厌恶 
| 9+15+16
|-
| 蔑视 
| R12A+R14A
|}

==== 面部情感检测的挑战 ====
正如计算领域的大多数问题一样，面部表情处理的情感检测也会遇到种种障碍需要克服，这样才能开发出选用算法的潜能。建模与追踪的准确性长久以来就是个问题，特别是在情感计算领域的早期。随着硬件的发展、新方法的创造与时间，精度缺乏的问题逐渐淡出，而噪声问题依旧。降噪的方法也是存在的，如邻域平均、[[高斯模糊|线性高斯平滑]]、中值滤波等。<ref>{{cite web|url=http://homepages.inf.ed.ac.uk/rbf/CVonline/LOCAL_COPIES/OWENS/LECT5/node3.html|title=Spatial domain methods}}</ref>  比较新的方法还有[[菌群优化算法|菌群优化算法]]（{{Lang-en|Bacterial Foraging Optimization Algorithm}}）。<ref>Clever Algorithms. [http://www.cleveralgorithms.com/nature-inspired/swarm/bfoa.html "Bacterial Foraging Optimization Algorithm – Swarm Algorithms – Clever Algorithms"]. Clever Algorithms. Retrieved 21 March 2011.</ref><ref>[http://www.softcomputing.net/bfoa-chapter.pdf "Soft Computing"]. Soft Computing. Retrieved 18 March 2011.</ref><ref>{{Cite journal|title=Hybrid Technique for Human Face Emotion Detection|url=http://thesai.org/Downloads/Volume1No6/Paper_15_Hybrid_Technique_for_Human_Face_Emotion_Detection.pdf|last=Nagpal, Renu, Pooja Nagpal, and Sumeet Kaur|journal=International Journal of Advanced Computer Science and Applications|accessdate=2011-03-11|issue=6|doi=10.14569/IJACSA.2010.010615]|year=2010|volume=1|pages=91–101}}</ref>

一般认为面部表情识别的准确度等级（并非情感状态识别的准确度等级）还没有达到可以广泛应用的层次。曾经有过将这样的技术应用于执法的实践，例如辨识罪犯，但并不成功。在没有提升扫描人脸的软硬件精度的前提下，准确度的进步已大大放缓。

其他问题包括：
* 大多研究中所使用的构成表达式其实并不自然，因此不是100％准确的。
* 缺乏旋转运动的自由度。 情感检测在正脸使用效果很好，但将头部旋转超过20度时，就会“出现问题”。<ref>Williams, Mark. [http://www.technologyreview.com/Infotech/18796/?a=f "Better Face-Recognition Software – Technology Review"]. Technology Review: The Authority on the Future of Technology. Retrieved 21 March 2011.</ref>

=== 身体姿态 ===
身体姿态是检测用户的特定情绪状态的有效手段，特别是与语音和脸部识别一起使用时。依动作的不同，身体姿态分为反射性的（如被问题问住时下意识的抬肩膀）、复杂与有意义的（如手语交流）等。在不借助外物或环境的情况下，我们可以挥手、拍手或招手；借助外物时，则可以指向、移动、触碰和持握。计算机应该做到识别这些信息，以更有效地应用于人机交互。

身体姿态的检测已经有了很多方法提出。<ref name="JK">J. K. Aggarwal, Q. Cai, Human Motion Analysis: A Review, Computer Vision and Image Understanding, Vol. 73, No. 3, 1999</ref> 一些文献将以下两种识别途径区别开来：基于三维模型的，和基于外观的（{{Lang-en|appearance-based}}）。<ref name="Vladimir">{{Cite journal|title=Visual Interpretation of Hand Gestures for Human-Computer Interaction: A Review|url=http://www.cs.rutgers.edu/~vladimir/pub/pavlovic97pami.pdf|last=Pavlovic|first=Vladimir I.|last2=Sharma|first2=Rajeev|journal=[[IEEE_Transactions_on_Pattern_Analysis_and_Machine_Intelligence|IEEE Transactions on Pattern Analysis and Machine Intelligence]]|year=1997|last3=Huang|first3=Thomas S.}}</ref>前者将肢体关键部位的三维信息利用起来，以获得若干重要参数，例如手掌位置和关节角度；后者则是直接利用图像或视频做解释。手势是身体姿态情感研究的一大集中领域，上文所提到的三维模型和外观方法都有在此使用。

=== 生理检测 ===
生理信号可用以检测与分析情绪状态，这些生理信号通常包括脉搏、心率、面部肌肉每分钟收缩频率等。这一研究领域仍处于相对起步的阶段，但发展迅猛，并已经有实用的产品出现。常被用来分析情感的生理信号种类有血容量脉冲、皮肤电反应、面部肌电图等。

==== 血容量脉冲 ====

===== 概述 =====
[[血容量脉冲|血容量脉冲]]（{{Lang-en|blood volume pulse, BVP}}）通过[[光體積變化描記圖法|光电容积描记法]]记录，该方法可以检测肢体末端的血流变化。<ref name="Picard, Rosalind 1998">Picard, Rosalind (1998). Affective Computing. MIT.</ref> 记录峰值代表着心搏周期中血流被泵到肢体末端。当被试受到惊吓或感到害怕时，他们往往会心跳加速，导致心率加快，从而在光电容积描记图上可以清楚地看到波峰与波谷间的距离变小。被试平静下来后，血液流回末端，心率回归正常。

===== 方法 =====
在皮肤上照射红外光，利用特制传感器检测光的反射量。由于红外光被血液中的[[血红蛋白|血红蛋白]]吸收，反射光与BVP相关。

===== 弊端 =====
确保传感器发射红外光并确保检测点始终在同一肢端上相当麻烦。尤其是被试需要伸展身体，也会因为使用电脑变化姿势，这更为检测增加了难度。影响血容量脉冲还有其他因素，例如被试觉得冷了或热了，都会导致血液向肢体末端流动的状态发生改变，而这与其情绪状态无关。
[[File:Em-face-2.png|左]]

==== 面部肌电图 ====
[[面部肌电图|面部肌电图]]（肌电图，{{Lang-en|electromyography, EMG}}）可以用来检测面部肌肉活动，放大肌纤维收缩的微小电流。<ref name="Larsen JT 2003">Larsen JT, Norris CJ, Cacioppo JT, "Effects of positive and negative affect on electromyographic activity over zygomaticus major and corrugator supercilii", (September 2003)</ref>面部表情和情绪关联性极大，以下两组肌肉是情感检测的主要研究对象：[[皱眉肌|皱眉肌]]（用来检测负向情感效果最佳）和[[颧大肌|颧大肌]]（微笑时扬起嘴角用到的肌肉，用来检测正向情感效果最佳）。
[[File:Gsrplot-zh.svg|500x500px|thumb|如图是皮肤电阻的GSR-时间图像，记录了在打电子游戏的过程中，被试者的皮肤电变化。图中有几个清晰的峰值，表明GSR是区分唤醒状态和非唤醒状态的好方法。例如，游戏开始时通常没有太多令人兴奋的地方，记录到的皮肤电阻很高，即电导较低，唤醒较少。这与后面突然出现的低谷形成了鲜明对比，发生低估时，被试在游戏里面的角色被杀死，由此变得十分紧张。<br />
]]

==== 皮肤电反应 ====
皮肤电反应（{{Lang-en|galvanic skin response, GSR}}）是皮肤电导的度量，与皮肤的湿润程度相关。由于汗腺分泌受神经系统控制，GSR同身体的唤醒度状态有关。被试唤醒度越高，皮肤电导和GSR数值越大。<ref name="Picard, Rosalind 1998"/>

皮肤电的测量使用两个[[氯化银|氯化银]]电极，将其贴置于皮肤表面并施加一个小电压。电导由传感器测定。为了减少不适感、减轻刺激，电极可以贴在脚上，以达到释放双手、允许被试操作鼠标键盘的目的。

=== 视觉审美 ===
艺术和摄影世界中的[[美学|美学]]指的是[[美|美]]的本质和欣赏原则，对美和其他审美特质的判断是高度主观的事情。[[宾夕法尼亚州立大学|宾夕法尼亚州立大学]]的一组计算机科学家，将自动评价图像的审美特质视作[[机器学习|机器学习]]的一大挑战，他们将一个同行评级的在线照片分享网站作为数据源，<ref name="datta">Ritendra Datta, Dhiraj Joshi, Jia Li and James Z. Wang, Studying Aesthetics in Photographic Images Using a Computational Approach, Lecture Notes in Computer Science, vol. 3953, Proceedings of the European Conference on Computer Vision, Part III, pp. 288-301, Graz, Austria, May 2006.</ref>从中抽取了特定的视觉特征，可以作为图像导致审美愉悦或不愉悦之间的差别。

== 潜在应用 ==

=== 人机交互与产品设计 ===
正如人工智能先驱[[马文·闵斯基|马文·闵斯基]]在其著作《{{tsl|en|The Society of Mind|心智社会}}》所指出的：“问题不在智能机器是否拥有情感，而是在机器有了智能之后怎样可以没有情感。”<ref>{{Cite book|title=The Society of Mind|last=Minsky|first=Marvin|publisher=Simon and Schuster|year=1986|isbn=|location=|pages=}}</ref>人与人之间的交流因科技的发展而越来越频繁，但通讯过程本身是与机器打交道，而不是与人。在机器愈发智能的21世纪初期，人也越发不满于机器在情感上的冰冷。与机器沟通过程更加友好的需求，使得情感计算在人机交互等领域存在着大量潜在应用。<ref name=":0" />

情感计算可以提升[[人机交互|人机交互]]中的用户体验，例如情感镜子让用户看到自己如何表现情绪、情感监控机器人会在发送愤怒的电子邮件之前发出警告、音乐播放器可以根据情绪选择曲目。<ref>{{cite journal|title=Tune in to Your Emotions: A Robust Personalized Affective Music Player|url=https://link.springer.com/article/10.1007%2Fs11257-011-9107-7|first1=Joris H.|last2=van den Broek|first2=Egon L.|date=July 2012|journal=User Modeling and User-Adapted Interaction|accessdate=2016-03-29|issue=3|doi=10.1007/s11257-011-9107-7|volume=22|pages=255–279|last1=Janssen}}</ref>

可处理情感信息的[[机器人|机器人系统]]在不确定或复杂的环境中展现出了高度的灵活性。陪伴性设备，比如[[电子宠物|电子宠物]]可利用情感计算能力提升真实感并带来更高的自主性。[[社交機器人|社交机器人]]，以及越来越多的被用于医疗的机器人，因为能够识别情感，可以更好地判断用户或患者的情绪状态，以及时对自身的行为或程序做出调整。在老龄化和缺乏年轻医疗工作者的国家，这些应用可以解决很多社会问题。<ref>{{Cite book|url=https://www.worldcat.org/oclc/956349457|title=Heart of the Machine: Our Future in a World of Artificial Emotional Intelligence|last=Yonck|first=Richard|publisher=Arcade Publishing|year=2017|isbn=9781628727333|location=New York|pages=150-153|oclc=956349457}}</ref>

在[[教育技术学|电子学习]]应用中，情感计算可以用来发现学习者厌倦、感兴趣、沮丧或高兴的情况，以调整计算机中教师的教学风格与节奏。<ref>[http://www.autotutor.org/ AutoTutor]</ref><ref name="affect-learning">{{Cite journal|title=Estimation of behavioral user state based on eye gaze and head pose—application in an e-learning environment|url=http://www.image.ece.ntua.gr/php/savepaper.php?id=571|last=S. Asteriadis, P. Tzouveli, K. Karpouzis, S. Kollias|journal=Multimedia Tools and Applications|publisher=Springer|year=2009|volume=41|pages=469–493}}</ref> 

罗马尼亚研究人员Nicu Sebe博士在采访中提出了一个想法，即分析使用某种产品时（原话以冰淇淋为例）一个人的面部表情。<ref>{{Cite web|url=http://www.sciencedaily.com/videos/2006/0811-mona_lisa_smiling.htm|title=Mona Lisa: Smiling? Computer Scientists Develop Software That Evaluates Facial Expressions|date=2006-08-01|archiveurl=https://web.archive.org/web/20071019235625/http://sciencedaily.com/videos/2006/0811-mona_lisa_smiling.htm|archivedate=2007-10-19}}</ref>企业可以通过这类分析来推断他们的产品是否会被相应的市场所接受。人们可以利用实时视频记录被试者的面部表情，使用情绪状态识别，来判断电视广告的有效性。综合考虑从大量被试者身上获得的结果，就可以判断该广告（或电影）是否具有预期的效果，以及观众最感兴趣的要素是什么。

=== 电子游戏 ===
情感型[[电子游戏|电子游戏]]可以通过[[生物反馈|生物反馈]]设备获取玩家的情绪状态。有一些简单的生物反馈形式，例如通过测量[[游戏手柄|游戏手柄]]按钮按压的压力，可以获知玩家的唤醒度水平，二者已被证明具有很强的相关性。另一方面的应用是[[脑机接口|脑机接口]]。情感游戏已被用于医学研究，以改善自闭症儿童的情感发展。<ref>{{Cite journal|title=Designing affective video games to support the social-emotional development of teenagers with autism spectrum disorders|last=Khandaker|first=M|journal=Studies in health technology and informatics|year=2009|volume=144|pages=37–9|pmid=19592726}}</ref>

=== 安全与医疗 ===
情感计算也可以应用于社会监督，改善社会治安、改善居民幸福感。配有情感计算装置的汽车可以监测驾驶者和乘客的情绪状态，采取相应的安全措施。举例来说，可以在检测到驾驶者生气时做出善意的提醒，以规避事故的发生。<ref>{{Cite journal|title=Emotion on the Road—Necessity, Acceptance, and Feasibility of Affective Computing in the Car|author=Florian Eyben|url=https://www.hindawi.com/journals/ahci/2010/263593/|journal=Advances in Human-Computer Interaction|issue=|doi=10.1155/2010/263593|others=Martin Wöllmer, Tony Poitschke, Björn Schuller, Christoph Blaschke, Berthold Farber, and Nhu Nguyen-Thien|year=2010|volume=|page=|pmid=}}</ref>

情感计算也被应用于开发自闭症患者与外界交流的技术。<ref>[http://affect.media.mit.edu/projects.php Projects in Affective Computing]</ref>心理咨询在确定患者情感状态时也可从情感计算中受益。

== 认知主义与交互方法之争 ==
在[[人机交互|人机交互]]领域，[[罗莎琳·皮卡德|罗莎琳·皮卡德]]所倡导的[[认知主义|认知主义]]或“信息模型（{{lang-en|information model}}）”情感概念受到了[[实用主义|实用主义]]者的批评，后者笃信“后认知主义（{{lang-en|post-cognitivist}}）”或“交互方法（{{lang-en|interactional}}）”，其代表人有柯尔斯顿·伯纳（{{Lang|en|Kirsten Boehner}}）等，他们认为情感本质上是社会性的。

皮卡德专注于人机交互，她对情感计算的目标是“让计算机认知与表达情感，甚至在某些场合下‘拥有’情感”。<ref>{{cite book|title=Affective Computing|first1=Rosalind|date=1997|publisher=MIT Press|location=Cambridge, MA|page=1|last1=Picard}}</ref> 相比之下，交互方法寻求“让人们理解与体验自身情感” ，增进以计算机为中介的人际交往，而并不一定要求得情感向客观数学模型的映射，来便于机器理解；情感计算应当让人类畅通无阻地理解彼此的情感，而这些情感信息往往会是歧义的、主观的或上下文敏感的。<ref name="How emotion is made and measured"/>{{rp|284}}

皮卡德的批评者将她的情感概念描述为“客观的、内部的、个人的和机械的”。 他们认为这将情绪降格成可测量的离散生理信号，而生理信号实际上只是认知的输入。情绪体验的复杂性则被忽视了。<ref name="How emotion is made and measured"/>{{rp|280}}<ref name="How emotion is made and measured"/>{{rp|278}}

交互方法认为，虽然情感具有[[生物物理|生物物理]]性，但它是“以文化为基的、动态体验的、某种程度上是行为和交互中构建的”。<ref name="How emotion is made and measured"/>{{rp|276}}换言之，交互方法认为“情感是通过交互体验到的社会与文化产物”。<ref>{{cite journal|title=Affection: From Information to Interaction|first1=Kirsten|last2=DePaula|first2=Rogerio|date=2005|journal=Proceedings of the Aarhus Decennial Conference on Critical Computing|pages=59–68|last3=Dourish|first3=Paul|last4=Sengers|first4=Phoebe|last1=Boehner}}</ref><ref name="How emotion is made and measured">{{cite journal|title=How emotion is made and measured|first1=Kirsten|last2=DePaula|first2=Rogerio|date=2007|journal=International Journal of Human-Computer Studies|issue=4|doi=10.1016/j.ijhcs.2006.11.016|volume=65|pages=275–291|last3=Dourish|first3=Paul|last4=Sengers|first4=Phoebe|last1=Boehner}}</ref><ref>{{cite journal|title=Interactional empowerment|url=http://research.microsoft.com/en-us/um/cambridge/projects/hci2020/pdf/interactional%20empowerment%20final%20Jan%2008.pdf|first1=Kristina|last2=Staahl|first2=Anna|date=2008|journal=Proc. CHI|pages=647–656|last3=Sundstrom|first3=Petra|last4=Laaksolahti|first4=Jarmo|last1=Hook}}</ref>

== 注释 ==
{{Notefoot}}

== 参见 ==
{{Columns-list|2|
* {{tsl|en|Affect control theory|情感控制论}}
* {{tsl|en|Affective design|情感设计}}
* {{tsl|en|Affective haptics|情感触觉论}}
* [[聊天机器人|聊天机器人]]
* [[研究和技术开发框架计划|赛博情感]]（英语：{{tsl|en|CyberEmotions}}）
* {{tsl|en|Emotion Markup Language|情感标记语言}}（EmotionML）
* [[Kismet|Kismet]]
* [[文本情感分析|文本情感分析]]
* [[可穿戴式电脑|可穿戴计算机]]
}}

== 参考文献 ==
=== 引用 ===
{{Reflist|2}}

=== 来源 ===
* {{cite journal | last = Hudlicka | first =  Eva | title = To feel or not to feel: The role of affect in human-computer interaction | journal = International Journal of Human-Computer Studies |  volume = 59 | issue = 1–2 | year = 2003 | pages = 1–32 | ref = harv | doi=10.1016/s1071-5819(03)00047-8}}
* {{cite book | author = Scherer, Klaus R |author2=Banziger, T |author3=Roesch, Etienne B | title = A blueprint for affective computing: a sourcebook | location = Oxford | publisher = Oxford University Press | year = 2010 }}
* {{cite book |author=刘光远|author2=温万惠|author3=陈通|author4=赖祥伟|title=人体生理信号的情感计算方法|location=北京| publisher=科学出版社|year=2014|isbn=9787030419507}}

== 外部链接 ==
* [http://affect.media.mit.edu/ 麻省理工学院数字媒体实验室情感计算研究组]
* [http://emotions.usc.edu/ 南加州大学情感计算组]
* [http://sites.google.com/site/memphisemotivecomputing/ 孟菲斯大学情感计算组]
* [http://www.acii2011.org/ 2011情感计算与智能交互国际会议] (2011 International Conference on Affective Computing and Intelligent Interaction)
* [https://web.archive.org/web/20091024081211/http://www.eecs.tufts.edu/~agirou01/workshop/ 大脑、身体和字节：身心的用户交互 (Brain, Body and Bytes: Psychophysiological User Interaction)] ''CHI 2010 Workshop ''（2010年4月10至15日）
* [http://www.igi-global.com/bookstore/titledetails.aspx?TitleId=1144 International Journal of Synthetic Emotions]
* [http://www.computer.org/portal/web/tac IEEE情感计算学报 (IEEE Transactions on Affective Computing, TAC)]
* [https://dx.doi.org/10.14569/IJACSA.2010.010615 Renu Nagpal, Pooja Nagpal and Sumeet Kaur, "Hybrid Technique for Human Face Emotion Detection" International Journal of Advanced Computer Science and Applications(IJACSA), 1(6), 2010]
* [http://opensmile.sourceforge.net/ openSMILE]，音频特征提取工具

{{Computer Science|state=expanded}}
{{Horizontal psychology|collapsed}}
{{普適遊戲|collapsed}}

[[Category:情感计算|Category:情感计算]]