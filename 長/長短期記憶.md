{{机器学习导航栏}}
{{ request translation }}
'''长短期记忆'''（{{lang-en|Long Short-Term Memory}}，{{lang|en|LSTM}}）是一种[[递归神经网络|时间递归神经网络]]（RNN）<ref>S. Hochreiter and J. Schmidhuber. Long short-term memory. Neural Computation, 9(8):1735–1780, 1997.</ref>，论文首次发表于1997年。由于独特的设计结构，LSTM适合于处理和预测[[时间序列|时间序列]]中间隔和延迟非常长的重要事件。

LSTM的表现通常比[[递归神经网络|时间递归神经网络]]及[[隐马尔科夫模型|隐马尔科夫模型]]（HMM）更好，比如用在不分段连续[[手写识别|手写识别]]上<ref>A. Graves, M. Liwicki, S. Fernandez, R. Bertolami, H. Bunke, J. Schmidhuber. A Novel Connectionist System for Improved Unconstrained Handwriting Recognition. IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 31, no. 5, 2009.</ref>。2009年，用LSTM构建的人工神经网络模型赢得过[[ICDAR|ICDAR]]手写识别比赛冠军。LSTM还普遍用于自主[[语音识别|语音识别]]，2013年運用[[TIMIT|TIMIT]]自然演講資料庫達成17.7%錯誤率的紀錄。作为[[非线性|非线性]]模型，LSTM可作为复杂的非线性单元用于构造更大型[[深度学习|深度神经网络]]。

== 历史 ==
LSTM最早由[[Sepp_Hochreiter|Sepp Hochreiter]]和[[Jürgen_Schmidhuber|Jürgen Schmidhuber]]于1997年提出。最初的版本包含了cells, input以及output gates。

==结构==
[[Image:Lstm_block.svg|thumb]]
LSTM是一種含有LSTM區塊（blocks）或其他的一種類神經網路，文獻或其他資料中LSTM區塊可能被描述成智慧型網路單元，因為它可以記憶不定時間長度的數值，區塊中有一個gate能夠決定input是否重要到能被記住及能不能被輸出output。

右圖底下是四個S函數單元，最左邊函數依情況可能成為區塊的input，右邊三個會經過gate決定input是否能傳入區塊，左邊第二個為input gate，如果這裡產出近似於零，將把這裡的值擋住，不會進到下一層。左邊第三個是forget gate，當這產生值近似於零，將把區塊裡記住的值忘掉。第四個也就是最右邊的input為output gate，他可以決定在區塊記憶中的input是否能輸出 。

LSTM有很多个版本，其中一个重要的版本是GRU（Gated Recurrent Unit）<ref>[http://www.iclr.cc/lib/exe/fetch.php?media=iclr2015:bahdanau-iclr2015.pdf Neural Machine Translation by Jointly Learning to Align and Translate]{{Dead link|date=2018年6月 |bot=InternetArchiveBot |fix-attempted=no }}，Cho et al. 2014年。</ref>，根据谷歌的测试表明，LSTM中最重要的是Forget gate，其次是Input gate，最次是Output gate<ref>[http://jmlr.org/proceedings/papers/v37/jozefowicz15.pdf 递归神经网络结构经验之谈]，2015年。</ref>。

==训练方法==
為了最小化訓練誤差，梯度下降法（Gradient descent）如：{{tsl|en|Backpropagation through time|應用時序性倒傳遞演算法}}，可用來依據錯誤修改每次的權重。梯度下降法在遞迴神經網路（RNN）中主要的問題初次在1991年發現，就是誤差梯度隨著事件間的時間長度成指數般的消失。當設置了LSTM 區塊時，誤差也隨著倒回計算，從output影響回input階段的每一個gate，直到這個數值被過濾掉。因此正常的倒傳遞類神經是一個有效訓練LSTM區塊記住長時間數值的方法。

==应用==

==参见==
* [[人工神经网络|人工神经网络]]
* {{tsl|en|Prefrontal Cortex Basal Ganglia Working Memory|前额叶皮质基底节工作记忆}}（PBWM）
* [[递归神经网络|递归神经网络]]
* [[时间序列|时间序列]]

==完整阅读==
* [http://colah.github.io/posts/2015-08-Understanding-LSTMs/ 理解LSTM网络]，作者Christopher Olah，更新于2015年八月。

==参考==
{{reflist}}

[[Category:人工智能|Category:人工智能]]
[[Category:人工神经网络|Category:人工神经网络]]